{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6-wIUkJ_T1f"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **`README.md`**\n",
        "\n",
        "# Replication of \"*Prompting for Policy: Forecasting Macroeconomic Scenarios with Synthetic LLM Personas*\"\n",
        "\n",
        "<!-- PROJECT SHIELDS -->\n",
        "[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/MIT)\n",
        "[![Python Version](https://img.shields.io/badge/python-3.9%2B-blue.svg)](https://www.python.org/)\n",
        "[![arXiv](https://img.shields.io/badge/arXiv-2511.02458v1-b31b1b.svg)](https://arxiv.org/abs/2511.02458)\n",
        "[![Conference](https://img.shields.io/badge/Conference-ACM%20ICAIF%202025-9cf)](https://icaif.acm.org/2025/)\n",
        "[![Year](https://img.shields.io/badge/Year-2025-purple)](https://github.com/chirindaopensource/forecasting_macroeconomic_scenarios_synthetic_llm_personas)\n",
        "[![Discipline](https://img.shields.io/badge/Discipline-Computational%20Economics-00529B)](https://github.com/chirindaopensource/forecasting_macroeconomic_scenarios_synthetic_llm_personas)\n",
        "[![Data Source](https://img.shields.io/badge/Data%20Source-ECB%20SPF-003299)](https://www.ecb.europa.eu/stats/ecb_surveys/survey_of_professional_forecasters/html/index.en.html)\n",
        "[![Data Source](https://img.shields.io/badge/Data%20Source-PersonaHub-FFD21E)](https://huggingface.co/datasets/proj-persona/PersonaHub)\n",
        "[![Core Method](https://img.shields.io/badge/Method-LLM%20Forecasting%20%7C%20Ablation%20Study-orange)](https://github.com/chirindaopensource/forecasting_macroeconomic_scenarios_synthetic_llm_personas)\n",
        "[![Analysis](https://img.shields.io/badge/Analysis-Time%20Series%20%7C%20Hypothesis%20Testing-red)](https://github.com/chirindaopensource/forecasting_macroeconomic_scenarios_synthetic_llm_personas)\n",
        "[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n",
        "[![Type Checking: mypy](https://img.shields.io/badge/type%20checking-mypy-blue)](http://mypy-lang.org/)\n",
        "[![OpenAI](https://img.shields.io/badge/OpenAI-GPT--4o-412991?logo=openai&logoColor=white)](https://openai.com/index/hello-gpt-4o/)\n",
        "[![spaCy](https://img.shields.io/badge/spaCy-%2309A3D5.svg?style=flat&logo=spaCy&logoColor=white)](https://spacy.io/)\n",
        "[![SentenceTransformers](https://img.shields.io/badge/SentenceTransformers-2E4053-blue)](https://www.sbert.net/)\n",
        "[![Pandas](https://img.shields.io/badge/pandas-%23150458.svg?style=flat&logo=pandas&logoColor=white)](https://pandas.pydata.org/)\n",
        "[![NumPy](https://img.shields.io/badge/numpy-%23013243.svg?style=flat&logo=numpy&logoColor=white)](https://numpy.org/)\n",
        "[![Jupyter](https://img.shields.io/badge/Jupyter-%23F37626.svg?style=flat&logo=Jupyter&logoColor=white)](https://jupyter.org/)\n",
        "\n",
        "**Repository:** `https://github.com/chirindaopensource/forecasting_macroeconomic_scenarios_synthetic_llm_personas`\n",
        "\n",
        "**Owner:** 2025 Craig Chirinda (Open Source Projects)\n",
        "\n",
        "This repository contains an **independent**, professional-grade Python implementation of the research methodology from the 2025 paper entitled **\"Prompting for Policy: Forecasting Macroeconomic Scenarios with Synthetic LLM Personas\"** by:\n",
        "\n",
        "*   Giulia Iadisernia\n",
        "*   Carolina Camassa\n",
        "\n",
        "The project provides a complete, end-to-end computational framework for replicating the paper's findings. It delivers a modular, auditable, and extensible pipeline that executes the entire research workflow: from rigorous data validation and cleansing to a large-scale, multi-stage persona filtering process, high-volume asynchronous forecast generation, and the final statistical analysis, including the central ablation study.\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "- [Introduction](#introduction)\n",
        "- [Theoretical Background](#theoretical-background)\n",
        "- [Features](#features)\n",
        "- [Methodology Implemented](#methodology-implemented)\n",
        "- [Core Components (Notebook Structure)](#core-components-notebook-structure)\n",
        "- [Key Callable: `run_synthetic_economist_study`](#key-callable-run_synthetic_economist_study)\n",
        "- [Prerequisites](#prerequisites)\n",
        "- [Installation](#installation)\n",
        "- [Input Data Structure](#input-data-structure)\n",
        "- [Usage](#usage)\n",
        "- [Output Structure](#output-structure)\n",
        "- [Project Structure](#project-structure)\n",
        "- [Customization](#customization)\n",
        "- [Contributing](#contributing)\n",
        "- [Recommended Extensions](#recommended-extensions)\n",
        "- [License](#license)\n",
        "- [Citation](#citation)\n",
        "- [Acknowledgments](#acknowledgments)\n",
        "\n",
        "## Introduction\n",
        "\n",
        "This project provides a Python implementation of the analytical framework presented in Iadisernia and Camassa (2025). The core of this repository is the iPython Notebook `forecasting_macroeconomic_scenarios_synthetic_llm_personas_draft.ipynb`, which contains a comprehensive suite of functions to replicate the paper's findings. The pipeline is designed to be a robust and scalable system for evaluating the impact of persona-based prompting on the macroeconomic forecasting performance of Large Language Models (LLMs).\n",
        "\n",
        "The paper's central research question is whether sophisticated persona descriptions improve LLM performance in a real-world forecasting task. This codebase operationalizes the paper's experimental design, allowing users to:\n",
        "-   Rigorously validate and manage the entire experimental configuration via a single `config.yaml` file.\n",
        "-   Execute a multi-stage filtering pipeline on the ~370M-entry PersonaHub corpus to distill a set of 2,368 high-quality, domain-specific expert personas.\n",
        "-   Systematically replicate 50 rounds of the ECB Survey of Professional Forecasters using GPT-4o.\n",
        "-   Generate over 120,000 individual forecasts across a main \"persona\" arm and a \"no-persona\" baseline for a controlled ablation study.\n",
        "-   Perform a comprehensive statistical analysis comparing the accuracy (MAE, win-share) and disagreement (dispersion) of the AI panels against the human expert panel.\n",
        "-   Run hypothesis tests (Monte Carlo and exact binomial) to assess the statistical significance of the findings.\n",
        "-   Execute the final ablation study tests (paired t-test, Kolmogorov-Smirnov test) to formally evaluate the impact of personas.\n",
        "\n",
        "## Theoretical Background\n",
        "\n",
        "The implemented methods are grounded in the principles of experimental design, computational linguistics, and time-series forecast evaluation.\n",
        "\n",
        "**1. Persona-Based Prompting:**\n",
        "The core hypothesis is that providing an LLM with a detailed \"persona\" or \"role\" can improve its performance on domain-specific reasoning tasks. This is tested via an ablation study, where the performance of prompts with personas is compared to identical prompts without them.\n",
        "\n",
        "**2. Forecast Evaluation Metrics:**\n",
        "-   **Mean Absolute Error (MAE):** A standard metric for point forecast accuracy, measuring the average magnitude of forecast errors.\n",
        "    $$\n",
        "    \\mathrm{MAE}_{vh} = \\frac{1}{n_{vh}} \\sum_{r=1}^{n_{vh}} | \\hat{y}_{rvh} - y_{rvh} |\n",
        "    $$\n",
        "-   **Win-Share:** A head-to-head comparison metric that counts the proportion of forecast rounds where one panel's forecast was strictly more accurate than another's, excluding ties.\n",
        "    $$\n",
        "    w_{vh} = \\frac{W_{vh}}{n_{vh}}, \\quad \\text{where } W_{vh} = \\sum_{r} \\mathbf{1}\\{ e_{rvh}^{\\mathrm{AI}} < e_{rvh}^{\\mathrm{H}} \\}\n",
        "    $$\n",
        "\n",
        "**3. Hypothesis Testing:**\n",
        "-   **Null Hypothesis:** The AI panel and human panel have an equal probability of producing a more accurate forecast ($H_0: p=0.5$).\n",
        "-   **Test for Large Samples (In-Sample):** The null distribution is approximated using a **Monte Carlo simulation** with $N=10,000$ draws from a Binomial distribution, $W^* \\sim \\text{Binom}(n_{vh}, 0.5)$.\n",
        "-   **Test for Small Samples (Out-of-Sample):** An **exact Binomial test** is used, calculating probabilities directly from the Binomial PMF.\n",
        "\n",
        "## Features\n",
        "\n",
        "The provided iPython Notebook (`forecasting_macroeconomic_scenarios_synthetic_llm_personas_draft.ipynb`) implements the full research pipeline, including:\n",
        "\n",
        "-   **Modular, Multi-Task Architecture:** The entire pipeline is broken down into 25 distinct, modular tasks, each with its own orchestrator function.\n",
        "-   **Configuration-Driven Design:** All study parameters are managed in an external `config.yaml` file.\n",
        "-   **Scalable Data Processing:** Includes streaming validators and processors for the large-scale PersonaHub dataset.\n",
        "-   **Resilient API Orchestration:** A robust, asynchronous framework for managing over 120,000 API calls with concurrency control, rate limiting, automatic retries, and resumability via checkpointing.\n",
        "-   **Advanced Persona Filtering:** A four-stage pipeline combining keyword filtering, NER, semantic deduplication (via embeddings and HNSW), and a multi-run, majority-vote LLM-as-a-judge triage.\n",
        "-   **Rigorous Statistical Analysis:** Implements all specified forecast evaluation metrics and hypothesis tests with high fidelity.\n",
        "-   **Complete Replication:** A single top-level function call can execute the entire study from raw data to final result tables.\n",
        "\n",
        "## Methodology Implemented\n",
        "\n",
        "The core analytical steps directly implement the methodology from the paper:\n",
        "\n",
        "1.  **Validation & Cleansing (Tasks 1-4):** Ingests and validates all raw inputs, including the ~370M-entry PersonaHub file and all analytical datasets.\n",
        "2.  **Persona Filtering (Tasks 5-9):** Executes the four-stage filtering pipeline to derive the final `persona_final_df` of 2,368 personas. Includes a Cohen's kappa validation of the LLM judge.\n",
        "3.  **Forecast Generation (Tasks 10-12):** Assembles all 123,400 prompts and executes the API calls for both the persona and baseline arms.\n",
        "4.  **Analysis & Scoring (Tasks 13-22):** Consolidates and QCs all forecasts, computes AI panel medians, aligns all data sources, calculates dispersion, errors, MAE, and win-shares, and runs all hypothesis tests.\n",
        "5.  **Ablation Study (Tasks 24-25):** Runs the final paired t-test and Kolmogorov-Smirnov test on the aligned results to formally test the paper's main hypothesis.\n",
        "\n",
        "## Core Components (Notebook Structure)\n",
        "\n",
        "The `forecasting_macroeconomic_scenarios_synthetic_llm_personas_draft.ipynb` notebook is structured as a logical pipeline with modular orchestrator functions for each of the 25 major tasks. All functions are self-contained, fully documented with type hints and docstrings, and designed for professional-grade execution.\n",
        "\n",
        "## Key Callable: `run_synthetic_economist_study`\n",
        "\n",
        "The project is designed around a single, top-level user-facing interface function:\n",
        "\n",
        "-   **`run_synthetic_economist_study`:** This master orchestrator function, located in the final section of the notebook, runs the entire automated research pipeline from end-to-end. A single call to this function reproduces the entire computational portion of the project.\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "-   Python 3.9+\n",
        "-   An OpenAI API key.\n",
        "-   Core dependencies: `pandas`, `numpy`, `pyyaml`, `pyarrow`, `openai`, `spacy`, `sentence-transformers`, `networkx`, `hnswlib`, `scipy`, `scikit-learn`, `tqdm`, `faker`.\n",
        "\n",
        "## Installation\n",
        "\n",
        "1.  **Clone the repository:**\n",
        "    ```sh\n",
        "    git clone https://github.com/chirindaopensource/forecasting_macroeconomic_scenarios_synthetic_llm_personas.git\n",
        "    cd forecasting_macroeconomic_scenarios_synthetic_llm_personas\n",
        "    ```\n",
        "\n",
        "2.  **Create and activate a virtual environment (recommended):**\n",
        "    ```sh\n",
        "    python -m venv venv\n",
        "    source venv/bin/activate  # On Windows, use `venv\\Scripts\\activate`\n",
        "    ```\n",
        "\n",
        "3.  **Install Python dependencies:**\n",
        "    ```sh\n",
        "    pip install -r requirements.txt\n",
        "    ```\n",
        "\n",
        "4.  **Download the spaCy model:**\n",
        "    ```sh\n",
        "    python -m spacy download en_core_web_trf\n",
        "    ```\n",
        "\n",
        "5.  **Set your OpenAI API Key:**\n",
        "    ```sh\n",
        "    export OPENAI_API_KEY='your-key-here'\n",
        "    ```\n",
        "\n",
        "## Input Data Structure\n",
        "\n",
        "The pipeline requires several input files with specific schemas, which are rigorously validated. A synthetic data generator is included in the notebook for a self-contained demonstration.\n",
        "1.  **`persona_hub_raw.parquet`**: The large-scale persona dataset.\n",
        "2.  **`contextual_data.csv`**: Time-series data for the 50 SPF rounds.\n",
        "3.  **`human_benchmark.csv`**: Human expert panel median forecasts.\n",
        "4.  **`human_micro.csv`**: Individual human expert forecasts.\n",
        "5.  **`realized_outcomes.csv`**: Ground-truth macroeconomic data.\n",
        "6.  **`human_annotations.csv`**: Human judgments for the kappa validation.\n",
        "\n",
        "All other parameters are controlled by the `config.yaml` file.\n",
        "\n",
        "## Usage\n",
        "\n",
        "The `forecasting_macroeconomic_scenarios_synthetic_llm_personas_draft.ipynb` notebook provides a complete, step-by-step guide. The primary workflow is to execute the final cell of the notebook, which demonstrates how to use the top-level `run_synthetic_economist_study` orchestrator:\n",
        "\n",
        "```python\n",
        "# Final cell of the notebook\n",
        "\n",
        "# This block serves as the main entry point for the entire project.\n",
        "if __name__ == '__main__':\n",
        "    # 1. Define paths and load configuration.\n",
        "    run_dir = Path(\"./synthetic_economist_run\")\n",
        "    raw_data_dir = run_dir / \"raw_data\"\n",
        "    output_dir = run_dir / \"output\"\n",
        "    \n",
        "    with open('config.yaml', 'r') as f:\n",
        "        config = yaml.safe_load(f)\n",
        "    \n",
        "    # 2. Generate a full set of synthetic data files for the demonstration.\n",
        "    # (The generation functions are defined in the notebook)\n",
        "    data_paths = setup_synthetic_data(raw_data_dir, config)\n",
        "    \n",
        "    # 3. Execute the entire replication study.\n",
        "    final_artifacts = run_synthetic_economist_study(\n",
        "        data_paths=data_paths,\n",
        "        config=config,\n",
        "        output_dir=str(output_dir),\n",
        "        total_persona_rows=10000, # Use the size of our synthetic dataset\n",
        "        run_kappa_validation=True\n",
        "    )\n",
        "    \n",
        "    # 4. Inspect final results.\n",
        "    print(\"--- Ablation Paired T-Test Report ---\")\n",
        "    print(final_artifacts['ablation_ttest_report'])\n",
        "```\n",
        "\n",
        "## Output Structure\n",
        "\n",
        "The pipeline generates a structured output directory:\n",
        "-   **`output/processed/`**: Contains intermediate, processed data files (e.g., cleansed and filtered persona sets).\n",
        "-   **`output/checkpoints/`**: Contains raw JSONL results from all API calls, enabling resumability.\n",
        "-   **`output/results/`**: Contains all final output tables (MAE, win-share, dispersion) as CSV files and a comprehensive `full_pipeline_report.json`.\n",
        "-   **`output/pipeline_run.log`**: A detailed log file for the entire run.\n",
        "\n",
        "## Project Structure\n",
        "\n",
        "```\n",
        "forecasting_macroeconomic_scenarios_synthetic_llm_personas/\n",
        "│\n",
        "├── forecasting_macroeconomic_scenarios_synthetic_llm_personas_draft.ipynb\n",
        "├── config.yaml\n",
        "├── requirements.txt\n",
        "│\n",
        "├── study_run/\n",
        "│   ├── raw_data/\n",
        "│   └── output/\n",
        "│       ├── processed/\n",
        "│       ├── results/\n",
        "│       ├── checkpoints/\n",
        "│       └── pipeline_run.log\n",
        "│\n",
        "├── LICENSE\n",
        "└── README.md\n",
        "```\n",
        "\n",
        "## Customization\n",
        "\n",
        "The pipeline is highly customizable via the `config.yaml` file. Users can modify all study parameters, including model names, API settings, filtering thresholds, and file paths, without altering the core Python code.\n",
        "\n",
        "## Contributing\n",
        "\n",
        "Contributions are welcome. Please fork the repository, create a feature branch, and submit a pull request with a clear description of your changes. Adherence to PEP 8, type hinting, and comprehensive docstrings is required.\n",
        "\n",
        "## Recommended Extensions\n",
        "\n",
        "Future extensions could include:\n",
        "-   **Evaluating Different LLMs:** The modular design allows for easy substitution of the `model_name` in the config to test other models (e.g., from Anthropic, Google).\n",
        "-   **Density Forecasting:** Extending the prompts to ask for probability distributions (as in the real SPF) and evaluating the quality of the LLM's density forecasts.\n",
        "-   **Alternative Prompting Strategies:** Implementing and testing other prompting techniques, such as chain-of-thought or adversarial prompting, to see if they can generate more diverse or accurate forecasts.\n",
        "\n",
        "## License\n",
        "\n",
        "This project is licensed under the MIT License. See the `LICENSE` file for details.\n",
        "\n",
        "## Citation\n",
        "\n",
        "If you use this code or the methodology in your research, please cite the original paper:\n",
        "\n",
        "```bibtex\n",
        "@inproceedings{iadisernia2025prompting,\n",
        "  author = {Iadisernia, Giulia and Camassa, Carolina},\n",
        "  title = {Prompting for Policy: Forecasting Macroeconomic Scenarios with Synthetic LLM Personas},\n",
        "  year = {2025},\n",
        "  booktitle = {Proceedings of the 6th ACM International Conference on AI in Finance},\n",
        "  series = {ICAIF '25}\n",
        "}\n",
        "```\n",
        "\n",
        "For the implementation itself, you may cite this repository:\n",
        "```\n",
        "Chirinda, C. (2025). A Production-Grade Replication of \"Prompting for Policy: Forecasting Macroeconomic Scenarios with Synthetic LLM Personas\".\n",
        "GitHub repository: https://github.com/chirindaopensource/forecasting_macroeconomic_scenarios_synthetic_llm_personas\n",
        "```\n",
        "\n",
        "## Acknowledgments\n",
        "\n",
        "-   Credit to **Giulia Iadisernia and Carolina Camassa** for the foundational research that forms the entire basis for this computational replication.\n",
        "-   This project is built upon the exceptional tools provided by the open-source community. Sincere thanks to the developers of the scientific Python ecosystem, including **Pandas, NumPy, spaCy, Sentence-Transformers, NetworkX, Scipy, Scikit-learn, and OpenAI**.\n",
        "\n",
        "--\n",
        "\n",
        "*This README was generated based on the structure and content of the `forecasting_macroeconomic_scenarios_synthetic_llm_personas_draft.ipynb` notebook and follows best practices for research software documentation.*\n"
      ],
      "metadata": {
        "id": "xgwk0f8pBrUJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Paper\n",
        "\n",
        "Title: \"*Prompting for Policy: Forecasting Macroeconomic Scenarios with Synthetic LLM Personas*\"\n",
        "\n",
        "Authors: Giulia Iadisernia, Carolina Camassa\n",
        "\n",
        "E-Journal Submission Date: 4 November 2025\n",
        "\n",
        "Conference Affiliation: The 6th ACM International Conference on AI in Finance (ICAIF 2025)\n",
        "\n",
        "Link: https://arxiv.org/abs/2511.02458\n",
        "\n",
        "Abstract:\n",
        "\n",
        "We evaluate whether persona-based prompting improves Large Language Model (LLM) performance on macroeconomic forecasting tasks. Using 2,368 economics-related personas from the PersonaHub corpus, we prompt GPT-4o to replicate the ECB Survey of Professional Forecasters across 50 quarterly rounds (2013-2025). We compare the persona-prompted forecasts against the human experts panel, across four target variables (HICP, core HICP, GDP growth, unemployment) and four forecast horizons. We also compare the results against 100 baseline forecasts without persona descriptions to isolate its effect. We report two main findings. Firstly, GPT-4o and human forecasters achieve remarkably similar accuracy levels, with differences that are statistically significant yet practically modest. Our out-of-sample evaluation on 2024-2025 data demonstrates that GPT-4o can maintain competitive forecasting performance on unseen events, though with notable differences compared to the in-sample period. Secondly, our ablation experiment reveals no measurable forecasting advantage from persona descriptions, suggesting these prompt components can be omitted to reduce computational costs without sacrificing accuracy. Our results provide evidence that GPT-4o can achieve competitive forecasting accuracy even on out-of-sample macroeconomic events, if provided with relevant context data, while revealing that diverse prompts produce remarkably homogeneous forecasts compared to human panels.\n"
      ],
      "metadata": {
        "id": "oBv_Y_20_hPf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary\n",
        "\n",
        "### **The Core Research Question and Motivation**\n",
        "\n",
        "The central question the authors investigate is both simple and profound: **Does emulating human expertise through detailed \"persona\" descriptions improve the macroeconomic forecasting performance of a Large Language Model (LLM)?**\n",
        "\n",
        "The motivation is clear. Central banks, like the European Central Bank (ECB), rely on surveys of human experts—in this case, the Survey of Professional Forecasters (SPF)—to gauge expectations about the economy. These surveys are crucial for setting monetary policy. LLMs offer the tantalizing possibility of simulating these expert panels at scale, speed, and low cost. However, the field of \"prompt engineering\" is still nascent. While it's known that LLM outputs are sensitive to prompts, there is little systematic evidence on whether complex, role-playing prompts (i.e., personas) actually add value in a technical domain like forecasting, or if they are merely computational window dressing. This paper aims to fill that empirical gap.\n",
        "\n",
        "### **The Experimental Design and Methodology**\n",
        "\n",
        "The authors' experimental pipeline, elegantly summarized in Figure 1, is the heart of this paper. It's a multi-stage process designed to rigorously test their hypothesis.\n",
        "\n",
        "**A. The Task:** The goal is to replicate the ECB's quarterly Survey of Professional Forecasters. The LLM must produce point forecasts for four key Euro area variables:\n",
        "*   **HICP:** Headline inflation\n",
        "*   **HICPX:** Core inflation (excluding volatile items)\n",
        "*   **rGDP:** Real GDP growth\n",
        "*   **UNR:** Unemployment rate\n",
        "\n",
        "These forecasts are required for multiple time horizons: the current year (CY), next year (CY+1), two years out (CY+2), and the long term. The experiment spans 50 quarterly rounds from 2013 to 2025.\n",
        "\n",
        "**B. The \"Synthetic Experts\" (Persona Creation):** Instead of hand-crafting a few \"expert\" personas, the authors perform a large-scale filtering exercise.\n",
        "1.  They start with the massive **PersonaHub dataset** (370 million expert descriptions).\n",
        "2.  They apply a multi-stage filter (keyword search, named-entity recognition, deduplication) to isolate descriptions relevant to economics and monetary policy.\n",
        "3.  Crucially, they use another LLM (GPT-4o-mini) as a \"judge\" to score the remaining personas on three criteria: **EU-centrality**, **neutrality** (no obvious bias), and **monetary policy depth**.\n",
        "4.  This rigorous process yields a final panel of **2,368 distinct, relevant, and high-quality personas**.\n",
        "\n",
        "**C. The Prompt Architecture:** For each of the 50 survey rounds, every one of the 2,368 personas is prompted. Each prompt contains standardized contextual information critical for any forecaster, human or AI:\n",
        "*   **The Persona Blurb:** The specific expert description being tested.\n",
        "*   **Monetary Policy Context:** The full text of the latest ECB press release.\n",
        "*   **Macro Snapshot:** Key recent data points (e.g., latest inflation reading).\n",
        "*   **Past Forecasts:** The median forecast from the previous SPF survey.\n",
        "*   **Task Instruction:** A clear directive to provide numerical forecasts in a specific format.\n",
        "\n",
        "**D. The Control Group (Ablation Study):** To isolate the effect of the persona, they run a parallel experiment. They create 100 \"baseline\" forecasts for each round using the exact same prompt, but with the persona description block completely removed. This is the control group. If the personas add value, the persona-driven forecasts should be significantly more accurate than these baseline forecasts.\n",
        "\n",
        "**E. The Evaluation Framework:** Performance is measured in two primary ways:\n",
        "1.  **Against Reality (Point Accuracy):** The median forecast of the 2,368 AI personas is compared to the actual realized economic data. The metric used is the Mean Absolute Error (MAE).\n",
        "2.  **Against Humans (Relative Performance):** The AI panel's median forecast is compared to the human SPF panel's median forecast. This is evaluated using **\"win-share,\"** which calculates the percentage of rounds where the AI's forecast was closer to the true value than the human forecast (ties are excluded).\n",
        "\n",
        "The authors also astutely include an **out-of-sample** test period (2024-2025), which falls after the knowledge cutoff of the GPT-4o model they use. This helps mitigate concerns that the model is simply \"memorizing\" economic history rather than performing genuine, context-based reasoning.\n",
        "\n",
        "### **The Principal Findings**\n",
        "\n",
        "The paper delivers two clear, high-impact findings.\n",
        "\n",
        "**Finding 1: AI Achieves Competitive Accuracy with Human Experts.**\n",
        "When comparing the median AI forecast to the median human forecast, the performance is remarkably similar. As shown in Table 4 (MAE) and the heatmap in Figure 1 (Win Rate), neither panel consistently dominates the other. The differences in accuracy are often statistically significant but practically modest. For example, AI tends to have a slight edge in forecasting core inflation, while humans are slightly better at short-term unemployment. The key takeaway is that, when provided with the proper real-time context, an LLM can function as a \"synthetic forecaster\" on par with a panel of credentialed human experts.\n",
        "\n",
        "**Finding 2: Sophisticated Personas Provide No Measurable Forecasting Advantage.**\n",
        "This is the paper's most significant methodological contribution. The ablation experiment yields a **null result**.\n",
        "*   Comparing the error distributions of the persona-driven forecasts versus the no-persona baseline forecasts reveals no statistically significant difference.\n",
        "*   This is visualized perfectly in Figure 4, where the kernel density plots of the absolute errors for both groups are almost perfectly overlapping. A Kolmogorov-Smirnov test confirms the distributions are statistically indistinguishable.\n",
        "*   The implication is powerful: the elaborate and computationally expensive process of selecting and using 2,368 unique personas added no value to the final forecast accuracy. The model's performance is driven by the contextual data (ECB statements, macro data), not by being told to \"act like\" a specific type of economist.\n",
        "\n",
        "### **Key Quantitative and Behavioral Insights**\n",
        "\n",
        "Beyond the two main findings, the paper reveals a crucial behavioral difference between the AI and human panels.\n",
        "\n",
        "*   **Extremely Low Dispersion in AI Forecasts:** As quantified in Table 3 and visualized in Figure 2, the AI forecasts are incredibly homogeneous. The inter-quartile range (IQR) of the 2,368 AI forecasts is often two orders of magnitude smaller than the IQR of the human panel.\n",
        "*   **Interpretation:** Despite being prompted with thousands of diverse personas, the LLM exhibits strong **consensus-seeking behavior**. It converges to a single, stable view of the economy based on the provided data. Human experts, in contrast, display a wide and realistic range of disagreement, reflecting diverse models, biases, and interpretations. This suggests that while LLMs can replicate the *central tendency* of expert opinion, they fail to capture the *heterogeneity* of that opinion.\n",
        "\n",
        "### **Implications and Concluding Remarks**\n",
        "\n",
        "In essence, this paper delivers a clear, actionable, and somewhat humbling message for the field of AI-driven forecasting.\n",
        "\n",
        "1.  **For Practitioners:** The good news is that LLMs like GPT-4o are powerful tools for macroeconomic forecasting that can achieve human-level accuracy. The crucial insight is that resources should be focused on **high-quality data integration and prompt clarity**, not on elaborate persona engineering. One can achieve top-tier results with a simple, direct prompt, saving significant computational cost and complexity.\n",
        "\n",
        "2.  **For Researchers:** The paper demonstrates that LLMs act as powerful information synthesizers but not as genuine simulators of diverse human cognition. The lack of forecast dispersion is a critical finding. Future work should explore methods to induce realistic diversity in LLM outputs, perhaps through adversarial prompting or chain-of-thought techniques that force the model to consider alternative scenarios, rather than just personas.\n",
        "\n",
        "This work is a fine example of how to conduct clean, hypothesis-driven research on LLMs. It replaces speculation with empirical evidence and provides a solid foundation for building more effective and efficient AI-augmented forecasting systems."
      ],
      "metadata": {
        "id": "WAbRweUXFP8Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Essential Modules"
      ],
      "metadata": {
        "id": "IIlKN_qxr4iY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#!/usr/bin/env python3\n",
        "# ==============================================================================#\n",
        "#\n",
        "#  Replication Pipeline for \"Prompting for Policy: Forecasting Macroeconomic\n",
        "#  Scenarios with Synthetic LLM Personas\"\n",
        "#\n",
        "#  This module provides a complete, production-grade implementation of the\n",
        "#  end-to-end research pipeline presented in \"Prompting for Policy: Forecasting\n",
        "#  Macroeconomic Scenarios with Synthetic LLM Personas\" by Iadisernia and\n",
        "#  Camassa (2025). It delivers a robust and reproducible system for replicating\n",
        "#  the paper's core findings on the efficacy of persona-based prompting for\n",
        "#  LLM-driven macroeconomic forecasting.\n",
        "#\n",
        "#  Core Methodological Components:\n",
        "#  • Large-scale persona filtering from the PersonaHub corpus using keyword\n",
        "#    matching, Named Entity Recognition (NER), semantic deduplication, and a\n",
        "#    multi-criteria LLM-as-a-judge triage.\n",
        "#  • Systematic replication of the ECB Survey of Professional Forecasters (SPF)\n",
        "#    across 50 quarterly rounds (2013-2025) using GPT-4o.\n",
        "#  • Generation of 118,400 forecasts from 2,368 synthetic LLM personas.\n",
        "#  • Controlled ablation study with 5,000 baseline forecasts (no persona).\n",
        "#  • Rigorous forecast evaluation against human expert medians and realized\n",
        "#    outcomes using Mean Absolute Error (MAE) and win-share statistics.\n",
        "#  • Statistical hypothesis testing using Monte Carlo simulation for in-sample\n",
        "#    data and exact binomial tests for out-of-sample data.\n",
        "#\n",
        "#  Technical Implementation Features:\n",
        "#  • Streaming data processing for large-scale validation and cleansing.\n",
        "#  • Asynchronous, resilient, and resumable API orchestration for high-volume\n",
        "#    LLM calls with concurrency and rate-limit management.\n",
        "#  • Efficient semantic search using sentence embeddings and an HNSW index.\n",
        "#  • Graph-based clustering for robust deduplication.\n",
        "#  • Modular, task-specific orchestrators composed into a single, auditable\n",
        "#    end-to-end pipeline.\n",
        "#\n",
        "#  Paper Reference:\n",
        "#  Iadisernia, G., & Camassa, C. (2025). Prompting for Policy: Forecasting\n",
        "#  Macroeconomic Scenarios with Synthetic LLM Personas. In Proceedings of the\n",
        "#  6th ACM International Conference on AI in Finance (ICAIF '25).\n",
        "#  arXiv preprint arXiv:2511.02458. https://arxiv.org/abs/2511.02458\n",
        "#\n",
        "#  Author: CS Chirinda\n",
        "#  License: MIT\n",
        "#  Version: 1.0.0\n",
        "#\n",
        "# ==============================================================================#\n",
        "\n",
        "# ==============================================================================#\n",
        "# Consolidated Imports for the End-to-End Pipeline\n",
        "# ==============================================================================#\n",
        "\n",
        "# --- Standard Library ---\n",
        "import asyncio\n",
        "import hashlib\n",
        "import json\n",
        "import logging\n",
        "import os\n",
        "import re\n",
        "import unicodedata\n",
        "from collections import Counter\n",
        "from pathlib import Path\n",
        "from typing import (Any, Coroutine, Dict, Generator, List, Optional, Set,\n",
        "                    Tuple, Union)\n",
        "\n",
        "# --- Third-Party Libraries ---\n",
        "# Core Data Handling and Numerical Operations\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pyarrow as pa\n",
        "import pyarrow.parquet as pq\n",
        "\n",
        "# Natural Language Processing\n",
        "import spacy\n",
        "import tiktoken\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from spacy.language import Language\n",
        "\n",
        "# Asynchronous Operations and API Interaction\n",
        "import openai\n",
        "from openai import APIError, AsyncOpenAI, RateLimitError\n",
        "from tqdm import tqdm\n",
        "from tqdm.asyncio import tqdm_asyncio\n",
        "\n",
        "# Statistical Analysis\n",
        "from scipy.stats import binom, ks_2samp, ttest_rel\n",
        "from sklearn.metrics import cohen_kappa_score, confusion_matrix\n",
        "\n",
        "# Graph Analysis for Deduplication\n",
        "import networkx as nx\n",
        "\n",
        "# Approximate Nearest Neighbor Search for Deduplication\n",
        "try:\n",
        "    import hnswlib\n",
        "except ImportError:\n",
        "    # Provide a clear error message if this optional dependency is missing.\n",
        "    raise ImportError(\n",
        "        \"The 'hnswlib' library is required for the deduplication task. \"\n",
        "        \"Please install it by running: pip install hnswlib\"\n",
        "    )\n",
        "\n",
        "# ==============================================================================\n",
        "# Setup: Logger Configuration\n",
        "# ==============================================================================\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n"
      ],
      "metadata": {
        "id": "ghSQiVtbr_qX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation"
      ],
      "metadata": {
        "id": "mwB5a8oBsCg3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Draft 1\n",
        "\n",
        "### **Documentation of Pipeline Orchestrators**\n",
        "\n",
        "#### **Task 1: `validate_persona_hub_df`**\n",
        "\n",
        "*   **Inputs:**\n",
        "    *   `df_path`: Path to the raw `persona_hub_raw_df` Parquet file (~370M records).\n",
        "    *   `config`: The main configuration dictionary.\n",
        "    *   `total_rows`: The known total number of rows in the file.\n",
        "*   **Processes:**\n",
        "    1.  Reads the large Parquet file in a streaming fashion, chunk by chunk, to maintain a low memory footprint.\n",
        "    2.  **Schema Validation:** For the first chunk, verifies that all expected columns (`id`, `description`, etc.) are present with the correct dtypes.\n",
        "    3.  **ID Uniqueness:** Iteratively builds a set of all unique `id` values encountered across all chunks, checking for duplicates both within and across chunks.\n",
        "    4.  **Content Validation:** Accumulates statistics across all chunks, including the count of null descriptions, the distribution of `token_count`, the set of `language_code` values, and the frequency of `domain_tags`.\n",
        "    5.  **Integrity Validation:** On a random sample of rows drawn probabilistically from the stream, it re-computes the SHA-256 hash of the `description` and compares it to the stored hash. It also aggregates counts for each `shard_id` to ensure full coverage.\n",
        "*   **Outputs:**\n",
        "    *   A comprehensive dictionary (`report`) containing the overall validation status (`SUCCESS`/`FAILURE`), a list of any errors found, and detailed statistics from all checks (e.g., duplicate ID count, hash mismatch count, top domain tags).\n",
        "*   **Role in Research Pipeline:** This function serves as the **initial data integrity gatekeeper** for the largest and most critical input dataset. It corresponds to the implicit pre-processing and validation that must occur before the filtering pipeline described in **Section 3.1 (\"Persona dataset\")** can begin. It ensures the raw data is structurally sound and internally consistent, preventing errors in all subsequent processing steps.\n",
        "\n",
        "\n",
        "\n",
        "#### **Task 2: `validate_analytical_inputs`**\n",
        "\n",
        "*   **Inputs:**\n",
        "    *   `contextual_data_df`, `human_survey_predictions_benchmark_df`, `human_survey_micro_df`, `realized_macro_data_df`: The four core analytical DataFrames, loaded into memory.\n",
        "    *   `config`: The main configuration dictionary.\n",
        "*   **Processes:**\n",
        "    1.  **Contextual Data Validation:** Verifies `contextual_data_df` has exactly 50 rows, correct `survey_round` format, valid temporal logic (`ecb_meeting_date <= survey_date`), and correct schemas for its complex nested dictionary columns. It cross-validates HICPX availability rules against the config.\n",
        "    2.  **Human Survey Validation:** Verifies that the `benchmark` and `micro` DataFrames contain only allowed categorical values. It performs a critical cross-validation by re-computing the `target_year` and ensuring it matches the provided data. It also enforces HICPX and OOS business rules.\n",
        "    3.  **Realized Data Validation:** Checks for uniqueness on `(reference_year, variable)` and performs a critical look-ahead bias check: `first_release_date` must be after the `reference_year`.\n",
        "    4.  **Config Validation:** Performs a deep validation of the `config` dictionary, ensuring key experimental parameters (model names, temperature, simulation counts, etc.) match the paper's specification exactly.\n",
        "*   **Outputs:**\n",
        "    *   A dictionary (`report`) summarizing the validation status of each input. The function raises a `ValueError` if any check fails.\n",
        "*   **Role in Research Pipeline:** This function is the **integrity gatekeeper for the analytical datasets and the experimental parameters**. It corresponds to the data validation steps described throughout **Section 3 (\"Data and experimental setup\")**, ensuring that the human benchmarks, realized outcomes, and contextual data are perfectly aligned and that the experiment will be run with the correct parameters.\n",
        "\n",
        "\n",
        "\n",
        "#### **Task 3: `cleanse_persona_hub_df`**\n",
        "\n",
        "*   **Inputs:**\n",
        "    *   `raw_df_path`: Path to the validated raw persona hub file.\n",
        "    *   `cleansed_df_path`: Path where the output file will be written.\n",
        "    *   `config`: The configuration dictionary.\n",
        "*   **Processes:**\n",
        "    1.  Reads the raw data in a streaming fashion.\n",
        "    2.  For each chunk, it applies a series of \"minimal, lossless\" transformations:\n",
        "        *   **Text Normalization:** Normalizes whitespace, removes control characters, and applies Unicode NFC normalization to the `description` column.\n",
        "        *   **Filtering:** Creates boolean masks to identify and subsequently drop rows with empty descriptions, disallowed language codes, or invalid token counts.\n",
        "        *   **Auxiliary Field Cleansing:** Normalizes the `domain_tags` column, handling mixed types (lists, JSON strings, nulls).\n",
        "        *   **Integrity Enforcement:** Re-computes the `sha256_description_hash` based on the *newly cleansed* description. It also sets the `id` column as the DataFrame index.\n",
        "    3.  Writes the cleansed, filtered, and indexed chunks to a new Parquet file in a streaming fashion.\n",
        "*   **Outputs:**\n",
        "    *   A new Parquet file at `cleansed_df_path`.\n",
        "    *   A dictionary (`report`) summarizing the process (rows read, written, and dropped by reason).\n",
        "*   **Role in Research Pipeline:** This function performs the **canonical data preparation** for the persona dataset. It creates the clean, standardized starting point for the main filtering pipeline described in **Section 3.1**.\n",
        "\n",
        "\n",
        "\n",
        "#### **Task 4: `cleanse_analytical_inputs`**\n",
        "\n",
        "*   **Inputs:** The four raw (but validated) analytical DataFrames.\n",
        "*   **Processes:**\n",
        "    1.  **Contextual Data Cleansing:** Normalizes the `ecb_communication_text`, parses date strings into `datetime` objects, and re-computes `ecb_communication_token_count` using the correct tokenizer for perfect accuracy.\n",
        "    2.  **`target_year` Computation:** This is a critical transformation. It computes the canonical `target_year` for every row in the `human_benchmark_df` and `human_micro_df` by combining the `round` and `horizon` information, using `contextual_data_df` for the 'LT' mapping.\n",
        "    3.  **Realized Data Cleansing:** Deduplicates the `realized_macro_data_df` and strictly validates the `aggregation_method` strings.\n",
        "*   **Outputs:**\n",
        "    *   A tuple of the four cleansed and transformed DataFrames.\n",
        "*   **Role in Research Pipeline:** This function performs the **final preparation of all analytical datasets**. The most important step is the computation of the `target_year`, which creates the essential join key that will be used in **Task 15** to align forecasts with their corresponding realized outcomes for scoring.\n",
        "\n",
        "\n",
        "\n",
        "#### **Task 5: `apply_keyword_domain_filter`**\n",
        "\n",
        "*   **Inputs:** Path to the cleansed persona hub file.\n",
        "*   **Processes:**\n",
        "    1.  Reads the data in a streaming fashion.\n",
        "    2.  For each chunk, it applies two filters in parallel:\n",
        "        *   **Keyword Filter:** Counts the number of unique lexicon phrases in each description and checks if the count is $\\geq 2$. Implements: $I_i^{\\mathrm{kw}} = \\mathbf{1}\\{ c_i \\geq 2 \\}$, where $c_i = \\left| \\{ \\ell \\in L : \\ell \\text{ is in } \\text{description}_i \\} \\right|$.\n",
        "        *   **Domain Filter:** Checks if the persona's `domain_tags` have a non-empty intersection with a set of allowed domains. Implements: $I_i^{\\mathrm{dom}} = \\mathbf{1}\\{ \\mathrm{domain\\_tags}_i \\cap \\mathrm{Dom\\_allow} \\neq \\emptyset \\}$.\n",
        "    3.  Keeps only the rows that pass both filters and writes them to a new Parquet file.\n",
        "*   **Outputs:**\n",
        "    *   A new, smaller Parquet file (`persona_step1.parquet`).\n",
        "    *   A summary report.\n",
        "*   **Role in Research Pipeline:** This is the **first main filtering stage** described in **Section 3.1, step (1) \"Keyword search and domain filtering\"**. It acts as a high-recall pre-screen to drastically reduce the dataset from ~370M to ~200k candidates.\n",
        "\n",
        "\n",
        "\n",
        "#### **Task 6: `apply_ner_person_filter`**\n",
        "\n",
        "*   **Inputs:** The `persona_step1_df` DataFrame (~200k records).\n",
        "*   **Processes:**\n",
        "    1.  Loads and configures a `spaCy` NER model.\n",
        "    2.  Processes all descriptions in batches using the efficient `nlp.pipe()` method.\n",
        "    3.  For each description, it checks if the model identified any entities with the label 'PERSON'.\n",
        "    4.  It filters out any persona where a 'PERSON' entity was found. Implements: $I_i^{\\mathrm{NER}} = \\mathbf{1}\\{ \\text{no entity in } E_i \\text{ has label PERSON} \\}$.\n",
        "*   **Outputs:**\n",
        "    *   A new, smaller DataFrame (`persona_step2_df`, ~43k records).\n",
        "    *   A summary report.\n",
        "*   **Role in Research Pipeline:** This is the **second filtering stage**, corresponding to **Section 3.1, step (2) \"Name filtering\"**. Its purpose is to remove descriptions that mention specific individuals to prevent the LLM from simply role-playing a known figure.\n",
        "\n",
        "\n",
        "\n",
        "#### **Task 7: `apply_embedding_deduplication`**\n",
        "\n",
        "*   **Inputs:** The `persona_step2_df` DataFrame (~43k records).\n",
        "*   **Processes:**\n",
        "    1.  **Embedding:** Generates a high-dimensional vector embedding for each description using a `sentence-transformers` model and L2-normalizes them.\n",
        "    2.  **Similarity Search:** Builds an HNSW (Approximate Nearest Neighbor) index on the embeddings. Queries the index to find all pairs of personas $(i, j)$ whose cosine similarity is above the threshold of 0.90. Implements: Find all $(i, j)$ where $s_{ij} = e_i^\\top e_j \\geq 0.90$.\n",
        "    3.  **Clustering:** Treats the similar pairs as edges in a graph and finds the connected components. Each component represents a cluster of semantically duplicate personas.\n",
        "    4.  **Selection:** From each cluster, it selects a single representative persona based on a deterministic rule (the one with the lexicographically smallest `id`).\n",
        "*   **Outputs:**\n",
        "    *   A new, much smaller DataFrame (`persona_step3_df`, ~4.3k records in the paper, though the implementation yields ~2.4k).\n",
        "    *   A summary report.\n",
        "*   **Role in Research Pipeline:** This is the **third filtering stage**, corresponding to **Section 3.1, step (3) \"Duplicate removal\"**. It ensures the set of candidate personas is diverse and non-redundant at a semantic level.\n",
        "\n",
        "\n",
        "\n",
        "#### **Task 8: `apply_llm_judge_filter`**\n",
        "\n",
        "*   **Inputs:** The `persona_step3_df` DataFrame.\n",
        "*   **Processes:**\n",
        "    1.  For each of the ~4.3k candidate personas, it makes three independent, asynchronous calls to the `gpt-4o-mini` API with `temperature=1.0`.\n",
        "    2.  The prompt for each call is a verbatim copy of the system prompt from Appendix A, which instructs the model to evaluate the persona on three criteria: EU-centrality, monetary policy depth, and neutrality.\n",
        "    3.  It parses the structured JSON response from each call.\n",
        "    4.  **Majority Vote:** For each persona and each criterion, it applies a majority vote rule. Implements: $\\text{pass}_c(i) = \\mathbf{1}\\{ \\sum_{r=1}^{3} \\mathbf{1}\\{ \\text{run } r \\text{ passed for } c \\} \\geq 2 \\}$.\n",
        "    5.  **Final Selection:** It keeps only those personas that passed the majority vote on *all three* criteria. Implements: $\\text{keep}_i = \\text{pass}_{\\mathrm{EU}}(i) \\cdot \\text{pass}_{\\mathrm{MP}}(i) \\cdot \\text{pass}_{\\mathrm{NEU}}(i)$.\n",
        "*   **Outputs:**\n",
        "    *   The final DataFrame of selected personas (`persona_final_df`, with exactly 2,368 rows).\n",
        "    *   A summary report.\n",
        "*   **Role in Research Pipeline:** This is the **fourth and final filtering stage**, corresponding to **Section 3.1, step (4) \"Zero-shot relevance rating\"**. It is the most sophisticated filter, using an LLM's reasoning capabilities to select the highest-quality personas.\n",
        "\n",
        "\n",
        "\n",
        "#### **Task 9: `validate_llm_judge_reliability`**\n",
        "\n",
        "*   **Inputs:** The `persona_step3_df`, the full set of LLM judgments, and a DataFrame of human annotations for a sample of 50 personas.\n",
        "*   **Processes:**\n",
        "    1.  Aligns the judgments from two human annotators and the LLM's majority-vote decision for the 50 sampled personas.\n",
        "    2.  For each of the three criteria, it computes Cohen's kappa for three pairs of raters: (Human1, Human2), (Human1, LLM), and (Human2, LLM).\n",
        "    3.  Implements the kappa formula: $\\kappa = \\frac{p_o - p_e}{1 - p_e}$.\n",
        "    4.  Compares the computed kappa scores to the \"substantial agreement\" range ([0.61, 0.81]) reported in the paper.\n",
        "*   **Outputs:**\n",
        "    *   A detailed report DataFrame containing the 9 kappa scores, their interpretation, and confusion matrices.\n",
        "    *   A high-level summary of the validation outcome.\n",
        "*   **Role in Research Pipeline:** This is a **methodological validation step** that corresponds to the validation process described at the end of **Section 3.1, step (4)**. It provides statistical evidence that the LLM-as-judge filter is reliable and aligns with human judgment.\n",
        "\n",
        "\n",
        "\n",
        "#### **Task 10: `assemble_forecasting_prompts`**\n",
        "\n",
        "*   **Inputs:** The `persona_final_df` (2,368 personas) and the `clean_contextual_data_df` (50 rounds).\n",
        "*   **Processes:**\n",
        "    1.  Iterates through each of the 50 survey rounds.\n",
        "    2.  For each round, it prepares a dictionary of all contextual data placeholders by extracting and formatting data from `contextual_data_df`.\n",
        "    3.  It then generates two sets of prompts for that round:\n",
        "        *   **Persona Arm:** It loops through all 2,368 personas and creates a prompt for each one, injecting the persona's description into the persona-specific system prompt template.\n",
        "        *   **No-Persona Arm:** It loops 100 times and creates a baseline prompt for each run, using the no-persona system prompt template.\n",
        "    4.  It yields each fully formed prompt as a structured object.\n",
        "*   **Outputs:**\n",
        "    *   A generator that yields a total of 123,400 prompt objects (118,400 for the persona arm + 5,000 for the baseline arm).\n",
        "*   **Role in Research Pipeline:** This function implements the **prompt engineering** stage described in **Section 3.3 (\"Prompt architecture\")**. It is responsible for creating the exact text that will be sent to the forecasting LLM.\n",
        "\n",
        "\n",
        "\n",
        "#### **Task 11: `generate_persona_forecasts`**\n",
        "\n",
        "*   **Inputs:** The generator of prompts from Task 10, filtered for the 'persona' arm.\n",
        "*   **Processes:**\n",
        "    1.  Orchestrates the execution of 118,400 asynchronous API calls to the `gpt-4o-2024-11-20` model.\n",
        "    2.  Manages concurrency, rate limiting, and retries.\n",
        "    3.  Saves the raw JSON response for each call to a checkpoint file for resumability.\n",
        "    4.  Parses the JSON responses, validates their structure (16 forecast items), and converts them into a tidy DataFrame.\n",
        "*   **Outputs:**\n",
        "    *   A DataFrame (`persona_forecasts_df`) containing all 1,894,400 individual forecast points (118,400 completions * 16 points).\n",
        "    *   A summary report.\n",
        "*   **Role in Research Pipeline:** This function executes the **main experimental run**, generating the AI forecasts for the persona arm as described in **Section 1** and the overall experimental design.\n",
        "\n",
        "\n",
        "\n",
        "#### **Task 12: `generate_baseline_forecasts`**\n",
        "\n",
        "*   **Inputs:** The generator of prompts from Task 10, filtered for the 'baseline' arm.\n",
        "*   **Processes:**\n",
        "    1.  Orchestrates the execution of 5,000 asynchronous API calls.\n",
        "    2.  Follows the exact same process as Task 11 (concurrency, checkpointing, parsing).\n",
        "    3.  Includes an additional validation step to check for stochastic variation in the outputs, ensuring the `temperature=1.0` setting was effective.\n",
        "*   **Outputs:**\n",
        "    *   A DataFrame (`baseline_forecasts_df`) containing all 80,000 individual baseline forecast points.\n",
        "    *   A summary report.\n",
        "*   **Role in Research Pipeline:** This function executes the **control group run** for the ablation study, as described in the paper's experimental design.\n",
        "\n",
        "\n",
        "\n",
        "#### **Task 13: `parse_and_qc_all_forecasts`**\n",
        "\n",
        "*   **Inputs:** The `persona_forecasts_df` and `baseline_forecasts_df`.\n",
        "*   **Processes:**\n",
        "    1.  **Consolidation:** Combines the two DataFrames into a single, unified DataFrame, adding a `source_type` column ('persona' or 'baseline') and renaming the ID columns to a generic `source_id`.\n",
        "    2.  **Final QC:** Performs a final validation pass, ensuring all `value`s are valid floats and adding a `qc_pass` flag.\n",
        "    3.  **Outlier Flagging:** Applies sanity bounds to the `value`s based on their `variable` and adds a boolean `is_outlier` flag, but does not filter these rows.\n",
        "*   **Outputs:**\n",
        "    *   A single, clean, unified DataFrame (`unified_forecasts_df`) containing all valid forecast points from both arms.\n",
        "    *   A summary report.\n",
        "*   **Role in Research Pipeline:** This is the **final data preparation step** before analysis begins, ensuring a clean, consistent, and unified dataset.\n",
        "\n",
        "\n",
        "\n",
        "#### **Task 14: `compute_ai_panel_medians`**\n",
        "\n",
        "*   **Inputs:** The `unified_forecasts_df`.\n",
        "*   **Processes:**\n",
        "    1.  For the 'persona' arm, it groups by `(round, variable, horizon)` and computes the median forecast. Implements: $\\hat{y}_{rvh}^{\\mathrm{AI}} = \\mathrm{median}_{p \\in P^*} \\left( \\hat{y}^{\\mathrm{AI}}_{p, r, v, h} \\right)$.\n",
        "    2.  For the 'baseline' arm, it does the same. Implements: $\\hat{y}_{rvh}^{\\mathrm{AI,NP}} = \\mathrm{median}_{s \\in \\{1, \\ldots, 100\\}} \\left( \\hat{y}^{\\mathrm{AI,NP}}_{s, r, v, h} \\right)$.\n",
        "    3.  It merges these two sets of results into a single DataFrame.\n",
        "*   **Outputs:**\n",
        "    *   A DataFrame (`ai_panel_medians_df`) with the consensus (median) forecast for both arms for every forecast instance.\n",
        "*   **Role in Research Pipeline:** This function performs the **cross-sectional aggregation** described in **Section 3.4 (\"Scoring metrics\")**. It transforms the panel of individual AI forecasts into the single time series that will be compared against the human panel.\n",
        "\n",
        "\n",
        "\n",
        "#### **Task 15: `align_forecasts_for_scoring`**\n",
        "\n",
        "*   **Inputs:** The `ai_panel_medians_df`, `human_benchmark_df`, `realized_outcomes_df`, and `contextual_data_df`.\n",
        "*   **Processes:**\n",
        "    1.  Computes the `target_year` for every AI forecast.\n",
        "    2.  Merges the AI medians with the human medians on `(round, variable, horizon)`.\n",
        "    3.  Merges the result with the realized outcomes on `(target_year, variable)`.\n",
        "    4.  Applies the OOS filtering rules.\n",
        "*   **Outputs:**\n",
        "    *   The master analysis DataFrame (`aligned_df`) containing all data needed for scoring.\n",
        "*   **Role in Research Pipeline:** This is the **final data alignment step**, creating the master table upon which all subsequent scoring and analysis (Tasks 18-25) will be performed.\n",
        "\n",
        "\n",
        "\n",
        "#### **Task 16: `compute_ai_persona_dispersion`**\n",
        "\n",
        "*   **Inputs:** The `unified_forecasts_df`.\n",
        "*   **Processes:**\n",
        "    1.  Groups the 'persona' forecasts by `(round, variable, horizon)`.\n",
        "    2.  For each group, calculates the IQR and the population standard deviation. Implements: $\\mathrm{IQR}_{rvh}^{\\mathrm{AI}}$ and $\\mathrm{SD}_{rvh}^{\\mathrm{AI}} = \\sqrt{ \\frac{1}{K} \\sum \\dots }$.\n",
        "    3.  Summarizes these per-round metrics by taking the median across all rounds for each `(variable, horizon)`.\n",
        "*   **Outputs:**\n",
        "    *   A summary DataFrame (`ai_dispersion_summary_df`) with the median dispersion metrics.\n",
        "*   **Role in Research Pipeline:** This function implements the **panel disagreement analysis** for the AI panel, as described in **Section 3.4** and presented in **Table 3**.\n",
        "\n",
        "\n",
        "\n",
        "#### **Task 17: `compute_human_dispersion_and_compare`**\n",
        "\n",
        "*   **Inputs:** The `human_survey_micro_df` and the `ai_dispersion_summary_df`.\n",
        "*   **Processes:**\n",
        "    1.  Performs the exact same dispersion analysis as Task 16, but on the human micro-data. Implements: $\\mathrm{IQR}_{rvh}^{\\mathrm{H}}$ and $\\mathrm{SD}_{rvh}^{\\mathrm{H}} = \\sqrt{ \\frac{1}{M} \\sum \\dots }$.\n",
        "    2.  Merges the resulting human dispersion summary with the AI summary.\n",
        "*   **Outputs:**\n",
        "    *   A final comparison DataFrame (`dispersion_comparison_df`) that replicates **Table 3** from the paper.\n",
        "*   **Role in Research Pipeline:** This function completes the **panel disagreement analysis** by providing the human benchmark and creating the final comparison table.\n",
        "\n",
        "\n",
        "\n",
        "#### **Task 18: `compute_absolute_errors`**\n",
        "\n",
        "*   **Inputs:** The `aligned_df`.\n",
        "*   **Processes:**\n",
        "    1.  Filters the data to scoreable rows (where `realized_value` is not null).\n",
        "    2.  Calculates the absolute error for the AI panel. Implements: $e_{rvh}^{\\mathrm{AI}} = \\left| \\hat{y}_{rvh}^{\\mathrm{AI}} - y_{rvh} \\right|$.\n",
        "    3.  Calculates the absolute error for the human panel. Implements: $e_{rvh}^{\\mathrm{H}} = \\left| \\hat{y}_{rvh}^{\\mathrm{SPF}} - y_{rvh} \\right|$.\n",
        "*   **Outputs:**\n",
        "    *   A DataFrame (`scored_forecasts_df`) containing the original data plus the two new absolute error columns.\n",
        "*   **Role in Research Pipeline:** This function implements the **error calculation** step defined in **Section 3.4**.\n",
        "\n",
        "\n",
        "\n",
        "#### **Task 19: `compute_mae_results`**\n",
        "\n",
        "*   **Inputs:** The `scored_forecasts_df`.\n",
        "*   **Processes:**\n",
        "    1.  Groups the data by `(period, variable, horizon)`.\n",
        "    2.  For each group, calculates the mean of the absolute errors for both the AI and human panels. Implements: $\\mathrm{MAE}_{vh} = \\frac{1}{n_{vh}} \\sum_{r=1}^{n_{vh}} e_{rvh}$.\n",
        "*   **Outputs:**\n",
        "    *   A summary DataFrame (`mae_results_df`) that replicates **Table 4** from the paper.\n",
        "*   **Role in Research Pipeline:** This function implements the **point accuracy analysis** using Mean Absolute Error, as defined in **Section 3.4**.\n",
        "\n",
        "\n",
        "\n",
        "#### **Task 20: `construct_win_share_statistics`**\n",
        "\n",
        "*   **Inputs:** The `scored_forecasts_df`.\n",
        "*   **Processes:**\n",
        "    1.  For each match, determines the winner by comparing absolute errors. Implements: $\\mathrm{win}_{rvh} = \\mathbf{1}\\{ e_{rvh}^{\\mathrm{AI}} < e_{rvh}^{\\mathrm{H}} \\}$. It robustly handles ties.\n",
        "    2.  Groups by `(period, variable, horizon)` and aggregates the counts of AI wins, human wins, and ties.\n",
        "    3.  Calculates the win-share, excluding ties from the denominator. Implements: $w_{vh} = W_{vh} / n_{vh}$, where $n_{vh} = W_{vh} + \\text{human\\_wins}$.\n",
        "*   **Outputs:**\n",
        "    *   A summary DataFrame (`win_share_df`) containing the statistics needed for hypothesis testing and replicating **Table 5**.\n",
        "*   **Role in Research Pipeline:** This function implements the **relative performance (win-share) analysis** defined in **Section 3.4**.\n",
        "\n",
        "\n",
        "\n",
        "#### **Task 21: `run_in_sample_mc_tests`**\n",
        "\n",
        "*   **Inputs:** The `win_share_df`.\n",
        "*   **Processes:**\n",
        "    1.  Filters for in-sample data.\n",
        "    2.  For each group, it runs a Monte Carlo simulation by drawing 10,000 samples from the null distribution, $W_{vh} \\sim \\mathrm{Binom}(n_{vh}, 0.5)$.\n",
        "    3.  It calculates the empirical one-tailed and two-tailed p-values based on these simulations. Implements: $p_{vh}^{(1)} = \\frac{1}{N} \\sum \\mathbf{1}\\{ W_j^* \\geq W_{vh} \\}$ and $p_{vh}^{(2)} = 2 \\min(\\dots)$.\n",
        "*   **Outputs:**\n",
        "    *   The `win_share_df` updated with p-values and significance stars for the in-sample rows.\n",
        "*   **Role in Research Pipeline:** This function implements the **hypothesis testing for the in-sample period**, as described in **Section 3.4, equation (1) and (2)**.\n",
        "\n",
        "\n",
        "\n",
        "#### **Task 22: `run_out_of_sample_exact_tests`**\n",
        "\n",
        "*   **Inputs:** The `win_share_df` (already containing in-sample results).\n",
        "*   **Processes:**\n",
        "    1.  Filters for out-of-sample data.\n",
        "    2.  For each group, it calculates the p-values directly from the Binomial probability mass function. Implements: $p_{vh}^{(1)} = \\Pr\\{ W \\geq W_{vh} \\}$ and $p_{vh}^{(2)} = 2 \\min(\\dots)$, where $W \\sim \\mathrm{Binom}(n_{vh}, 0.5)$.\n",
        "*   **Outputs:**\n",
        "    *   The `win_share_df` now fully populated with p-values and significance stars for all rows.\n",
        "*   **Role in Research Pipeline:** This function implements the **hypothesis testing for the out-of-sample period**, as described in **Section 3.4, equation (3) and (4)**.\n",
        "\n",
        "\n",
        "\n",
        "#### **Task 23: `run_full_replication_pipeline`**\n",
        "\n",
        "*   **Inputs:** All raw data paths and the config.\n",
        "*   **Processes:** Orchestrates the execution of all the above tasks (1-22) in the correct sequence, managing the flow of data and artifacts.\n",
        "*   **Outputs:** A comprehensive dictionary of all results and reports, and saves all final tables to disk.\n",
        "*   **Role in Research Pipeline:** This is the **master orchestrator** for the entire replication study.\n",
        "\n",
        "\n",
        "\n",
        "#### **Task 24: `run_ablation_paired_ttest`**\n",
        "\n",
        "*   **Inputs:** The `aligned_df`.\n",
        "*   **Processes:**\n",
        "    1.  Prepares a paired dataset of absolute errors for the persona and no-persona arms.\n",
        "    2.  Performs a paired t-test on these errors to test if the mean difference is zero. Implements the standard paired t-test formula: $t = \\frac{\\bar{d}}{s_d / \\sqrt{J}}$.\n",
        "*   **Outputs:**\n",
        "    *   A report DataFrame with the t-statistic, p-value, and interpretation.\n",
        "*   **Role in Research Pipeline:** This is the **first statistical test of the ablation study**, as described in **Section 4.1 (\"Persona ablation effect\")**.\n",
        "\n",
        "\n",
        "\n",
        "#### **Task 25: `run_ablation_ks_test`**\n",
        "\n",
        "*   **Inputs:** The `aligned_df`.\n",
        "*   **Processes:**\n",
        "    1.  Prepares two samples: the distribution of absolute errors for the persona arm and for the no-persona arm.\n",
        "    2.  Performs a two-sample Kolmogorov-Smirnov test to check if the two samples are drawn from the same distribution. Implements: $D = \\sup_{x} \\left| \\hat{F}_{\\mathrm{P}}(x) - \\hat{F}_{\\mathrm{NP}}(x) \\right|$.\n",
        "*   **Outputs:**\n",
        "    *   A report DataFrame with the D-statistic, p-value, and interpretation.\n",
        "*   **Role in Research Pipeline:** This is the **second statistical test of the ablation study**, providing converging evidence for the findings in **Section 4.1**.\n",
        "\n",
        "\n",
        "\n",
        "#### **Top-Level Orchestrator: `run_synthetic_economist_study`**\n",
        "\n",
        "*   **Inputs:** All raw data paths and the config.\n",
        "*   **Processes:** A top-level wrapper that first calls `run_full_replication_pipeline` (Tasks 1-22) and then, using the artifacts from that run, calls `run_ablation_paired_ttest` (Task 24) and `run_ablation_ks_test` (Task 25).\n",
        "*   **Outputs:** A final, comprehensive dictionary containing all results from the entire study.\n",
        "*   **Role in Research Pipeline:** This is the **single entry point** to execute the entire study, including the main replication and the final ablation tests.\n",
        "\n",
        "<br><br>\n",
        "\n",
        "### **Usage Example**\n",
        "\n",
        "Below is a fully functional, high-fidelity example of how to run the main orchestrator function; i.e. the `run_synthetic_economist_study` function. The example consists of three main parts: *setting up the environment, generating realistic synthetic input data, and then executing the main function call*:\n",
        "\n",
        "\n",
        "```python\n",
        "# ==============================================================================\n",
        "# Task: High-Fidelity Example of End-to-End Pipeline Execution\n",
        "# ==============================================================================\n",
        "# This script provides a complete, runnable example of how to use the\n",
        "# `run_synthetic_economist_study` function. It includes three main parts:\n",
        "# 1. Environment Setup: Prepares directories, loads configuration, and sets up logging.\n",
        "# 2. Synthetic Data Generation: Creates high-fidelity, realistic mock data files\n",
        "#    that conform to the required schemas and business rules.\n",
        "# 3. Pipeline Execution: Calls the top-level orchestrator with the prepared\n",
        "#    data and configuration to run the full study.\n",
        "#\n",
        "# NOTE: This example makes REAL API calls to OpenAI and will incur costs.\n",
        "# Ensure your API key is correctly configured.\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Step 1: Environment Setup\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "# --- Import all necessary libraries and functions ---\n",
        "# (This assumes all previously defined orchestrator functions are in scope)\n",
        "import asyncio\n",
        "import hashlib\n",
        "import json\n",
        "import logging\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "import unicodedata\n",
        "from collections import Counter\n",
        "from pathlib import Path\n",
        "from typing import Any, Coroutine, Dict, Generator, List, Optional, Set, Tuple, Union\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pyarrow as pa\n",
        "import pyarrow.parquet as pq\n",
        "import yaml\n",
        "from faker import Faker\n",
        "from scipy.stats import binom, ks_2samp, ttest_rel\n",
        "\n",
        "# --- Configure Logging ---\n",
        "# Set up a basic logger for clear output.\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    force=True # Override any existing handlers\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# --- Create Directory Structure ---\n",
        "# Define a root directory for this example run.\n",
        "run_dir = Path(\"./synthetic_economist_run\")\n",
        "# Define subdirectories for raw data and all pipeline outputs.\n",
        "raw_data_dir = run_dir / \"raw_data\"\n",
        "output_dir = run_dir / \"output\"\n",
        "# Create the directories. `exist_ok=True` prevents errors if they already exist.\n",
        "raw_data_dir.mkdir(parents=True, exist_ok=True)\n",
        "output_dir.mkdir(parents=True, exist_ok=True)\n",
        "logger.info(f\"Created directory structure in '{run_dir.resolve()}'.\")\n",
        "\n",
        "# --- Load Configuration ---\n",
        "# The config.yaml file should be in the same directory as this script.\n",
        "config_path = Path(\"config.yaml\")\n",
        "if not config_path.exists():\n",
        "    raise FileNotFoundError(\"config.yaml not found. Please create it from the provided template.\")\n",
        "\n",
        "with open(config_path, 'r') as f:\n",
        "    config = yaml.safe_load(f)\n",
        "logger.info(\"Successfully loaded configuration from config.yaml.\")\n",
        "\n",
        "# --- API Key Configuration ---\n",
        "# IMPORTANT: Set your OpenAI API key as an environment variable.\n",
        "# For example, in your terminal: export OPENAI_API_KEY='your-key-here'\n",
        "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "if not api_key:\n",
        "    raise ValueError(\"OPENAI_API_KEY environment variable not set. The pipeline cannot run.\")\n",
        "# Add the key to the config for easy access by the API functions.\n",
        "config['openai_api_key'] = api_key\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Step 2: High-Fidelity Synthetic Data Generation\n",
        "# ------------------------------------------------------------------------------\n",
        "# We will now generate realistic mock data files.\n",
        "\n",
        "# Initialize Faker for generating random text.\n",
        "fake = Faker()\n",
        "Faker.seed(42)\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "def generate_synthetic_persona_hub(path: Path, num_rows: int, config: Dict[str, Any]):\n",
        "    \"\"\"Generates a small, representative mock of the PersonaHub dataset.\"\"\"\n",
        "    logger.info(f\"Generating {num_rows} synthetic persona records...\")\n",
        "    lexicon = config['phase_1_parameters']['filtering']['keyword_filtering']['lexicon']\n",
        "    data = []\n",
        "    for i in range(num_rows):\n",
        "        # Create a mix of relevant and irrelevant descriptions.\n",
        "        if i % 5 == 0: # 20% of descriptions will be highly relevant\n",
        "            desc_core = ' '.join(random.sample(lexicon, k=random.randint(2, 5)))\n",
        "            desc = f\"A leading economist focused on {desc_core}. {fake.paragraph(nb_sentences=2)}\"\n",
        "            tags = ['economics', 'finance']\n",
        "        elif i % 5 == 1: # 20% will mention a person to test the NER filter\n",
        "            desc = f\"An analyst working with {fake.name()} on European trade policy. {fake.paragraph(nb_sentences=2)}\"\n",
        "            tags = ['policy']\n",
        "        else: # 60% are irrelevant\n",
        "            desc = fake.paragraph(nb_sentences=3)\n",
        "            tags = ['general', fake.word()]\n",
        "            \n",
        "        record = {\n",
        "            'id': f'persona_{i:05d}',\n",
        "            'description': desc,\n",
        "            'domain_tags': tags,\n",
        "            'language_code': 'en',\n",
        "            'token_count': len(desc.split()),\n",
        "            'sha256_description_hash': hashlib.sha256(desc.encode('utf-8')).hexdigest(),\n",
        "            'shard_id': i % 10\n",
        "        }\n",
        "        data.append(record)\n",
        "    \n",
        "    df = pd.DataFrame(data)\n",
        "    df.to_parquet(path, engine='pyarrow')\n",
        "    logger.info(f\"Synthetic persona hub saved to '{path}'.\")\n",
        "    return num_rows\n",
        "\n",
        "def generate_synthetic_contextual_data(path: Path, config: Dict[str, Any]):\n",
        "    \"\"\"Generates a mock contextual_data.csv file for all 50 rounds.\"\"\"\n",
        "    logger.info(\"Generating synthetic contextual data...\")\n",
        "    rounds = pd.period_range(start=\"2013Q1\", end=\"2025Q2\", freq='Q').strftime('%YQ%q')\n",
        "    data = []\n",
        "    hicpx_start = config['phase_2_parameters']['oos_and_availability']['hicpx_available_from_round']\n",
        "    \n",
        "    for r in rounds:\n",
        "        year = int(r[:4])\n",
        "        quarter = int(r[-1])\n",
        "        survey_date = pd.to_datetime(f'{year}-{(quarter-1)*3+1}-15')\n",
        "        \n",
        "        # Generate plausible nested data\n",
        "        realized = {\n",
        "            'hicp': {'period': f'{year-1}Dec', 'value': round(random.uniform(0.5, 3.0), 2)},\n",
        "            'hicpx': {'period': f'{year-1}Dec', 'value': round(random.uniform(0.5, 2.5), 2)} if r >= hicpx_start else {'period': None, 'value': np.nan},\n",
        "            'unr': {'period': f'{year-1}Nov', 'value': round(random.uniform(6.0, 12.0), 2)},\n",
        "            'rgdp': {'period': f'{year-1}Q4', 'value': round(random.uniform(-1.0, 2.0), 2)}\n",
        "        }\n",
        "        \n",
        "        lt_year = year + 4\n",
        "        medians = {\n",
        "            'hicp': {y: round(random.uniform(1.5, 2.2), 2) for y in [year, year+1, year+2, lt_year]},\n",
        "            'hicpx': {y: round(random.uniform(1.2, 2.0), 2) for y in [year, year+1, year+2, lt_year]} if r >= hicpx_start else {},\n",
        "            'unr': {y: round(random.uniform(7.0, 10.0), 2) for y in [year, year+1, year+2, lt_year]},\n",
        "            'rgdp': {y: round(random.uniform(0.5, 2.5), 2) for y in [year, year+1, year+2, lt_year]}\n",
        "        }\n",
        "        \n",
        "        record = {\n",
        "            'survey_round': r,\n",
        "            'survey_date': survey_date.strftime('%d/%m/%Y'),\n",
        "            'ecb_meeting_date': (survey_date - pd.Timedelta(days=7)).strftime('%d/%m/%Y'),\n",
        "            'ecb_communication_text': f\"ECB statement for {r}. {fake.paragraph(nb_sentences=10)}\",\n",
        "            'ecb_communication_token_count': 250,\n",
        "            'ecb_communication_language_code': 'en',\n",
        "            'latest_realized_data': json.dumps(realized),\n",
        "            'previous_spf_medians': json.dumps(medians),\n",
        "            'lt_year': lt_year,\n",
        "            'variable_horizon_availability': json.dumps({'hicp': True, 'hicpx': r >= hicpx_start, 'unr': True, 'rgdp': True}),\n",
        "            'data_vintage_timestamp': pd.Timestamp.now().isoformat()\n",
        "        }\n",
        "        data.append(record)\n",
        "        \n",
        "    df = pd.DataFrame(data)\n",
        "    df.to_csv(path, index=False)\n",
        "    logger.info(f\"Synthetic contextual data saved to '{path}'.\")\n",
        "\n",
        "def generate_synthetic_human_data(contextual_path: Path, micro_path: Path, benchmark_path: Path):\n",
        "    \"\"\"Generates mock human micro and benchmark forecast data.\"\"\"\n",
        "    logger.info(\"Generating synthetic human forecast data...\")\n",
        "    context_df = pd.read_csv(contextual_path)\n",
        "    variables = ['HICP', 'HICPX', 'RGDP', 'UNR']\n",
        "    horizons = ['CY', 'CY+1', 'CY+2', 'LT']\n",
        "    forecaster_ids = [f'human_{i:02d}' for i in range(10)]\n",
        "    \n",
        "    micro_data = []\n",
        "    for _, row in context_df.iterrows():\n",
        "        for var in variables:\n",
        "            for hor in horizons:\n",
        "                base_val = random.uniform(0, 10)\n",
        "                for fid in forecaster_ids:\n",
        "                    micro_data.append({\n",
        "                        'round': row['survey_round'],\n",
        "                        'variable': var,\n",
        "                        'horizon': hor,\n",
        "                        'forecaster_id': fid,\n",
        "                        'value': round(base_val + random.normalvariate(0, 0.2), 2),\n",
        "                        'target_year': 0 # Placeholder, will be computed by pipeline\n",
        "                    })\n",
        "    \n",
        "    micro_df = pd.DataFrame(micro_data)\n",
        "    # Create benchmark by taking the median\n",
        "    benchmark_df = micro_df.groupby(['round', 'variable', 'horizon'])['value'].median().reset_index()\n",
        "    benchmark_df['panel_size'] = 10\n",
        "    benchmark_df['target_year'] = 0 # Placeholder\n",
        "    \n",
        "    micro_df.to_csv(micro_path, index=False)\n",
        "    benchmark_df.to_csv(benchmark_path, index=False)\n",
        "    logger.info(f\"Synthetic human micro and benchmark data saved.\")\n",
        "\n",
        "def generate_synthetic_realized_data(path: Path):\n",
        "    \"\"\"Generates mock realized outcomes data.\"\"\"\n",
        "    logger.info(\"Generating synthetic realized outcomes...\")\n",
        "    years = range(2013, 2028)\n",
        "    variables = ['HICP', 'HICPX', 'RGDP', 'UNR']\n",
        "    methods = {\n",
        "        'HICP': 'simple average of 12 monthly YoY rates',\n",
        "        'HICPX': 'simple average of 12 monthly YoY rates',\n",
        "        'RGDP': 'annual growth percent',\n",
        "        'UNR': 'simple average of 12 monthly rates'\n",
        "    }\n",
        "    data = []\n",
        "    for year in years:\n",
        "        for var in variables:\n",
        "            data.append({\n",
        "                'reference_year': year,\n",
        "                'variable': var,\n",
        "                'value': round(random.uniform(0, 10), 2),\n",
        "                'source_series_id': f'FAKE_{var}_{year}',\n",
        "                'first_release_date': pd.to_datetime(f'{year+1}-01-25').isoformat(),\n",
        "                'retrieval_date': pd.Timestamp.now().isoformat(),\n",
        "                'revision_status': 'first',\n",
        "                'aggregation_method': methods[var]\n",
        "            })\n",
        "    df = pd.DataFrame(data)\n",
        "    df.to_csv(path, index=False)\n",
        "    logger.info(f\"Synthetic realized outcomes saved to '{path}'.\")\n",
        "\n",
        "# --- Generate all synthetic data files ---\n",
        "data_paths = {\n",
        "    'persona_hub': str(raw_data_dir / \"persona_hub_raw.parquet\"),\n",
        "    'contextual_data': str(raw_data_dir / \"contextual_data.csv\"),\n",
        "    'human_benchmark': str(raw_data_dir / \"human_benchmark.csv\"),\n",
        "    'human_micro': str(raw_data_dir / \"human_micro.csv\"),\n",
        "    'realized_outcomes': str(raw_data_dir / \"realized_outcomes.csv\"),\n",
        "    'human_annotations': str(raw_data_dir / \"human_annotations.csv\") # Assuming this exists for kappa\n",
        "}\n",
        "\n",
        "# For the large persona hub, we generate a small but representative sample.\n",
        "TOTAL_SYNTHETIC_PERSONAS = 10000\n",
        "generate_synthetic_persona_hub(data_paths['persona_hub'], TOTAL_SYNTHETIC_PERSONAS, config)\n",
        "generate_synthetic_contextual_data(data_paths['contextual_data'], config)\n",
        "generate_synthetic_human_data(data_paths['contextual_data'], data_paths['human_micro'], data_paths['human_benchmark'])\n",
        "generate_synthetic_realized_data(data_paths['realized_outcomes'])\n",
        "# Create a dummy human annotations file for the kappa validation step\n",
        "pd.DataFrame({\n",
        "    'persona_id': [f'persona_{i:05d}' for i in range(50)],\n",
        "    'annotator_id': ['A1']*25 + ['A2']*25,\n",
        "    'criterion': ['eu_centrality']*50,\n",
        "    'judgment': [True]*50\n",
        "}).to_csv(data_paths['human_annotations'], index=False)\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Step 3: Pipeline Execution\n",
        "# ------------------------------------------------------------------------------\n",
        "logger.info(\"=\"*80)\n",
        "logger.info(\"Starting the full end-to-end pipeline execution with synthetic data.\")\n",
        "logger.info(\"=\"*80)\n",
        "\n",
        "# Call the top-level orchestrator function with the prepared paths and config.\n",
        "# This will run the entire study from start to finish.\n",
        "final_artifacts = run_synthetic_economist_study(\n",
        "    data_paths=data_paths,\n",
        "    config=config,\n",
        "    output_dir=str(output_dir),\n",
        "    total_persona_rows=TOTAL_SYNTHETIC_PERSONAS,\n",
        "    run_kappa_validation=True\n",
        ")\n",
        "\n",
        "logger.info(\"=\"*80)\n",
        "logger.info(\"Pipeline execution finished.\")\n",
        "logger.info(\"=\"*80)\n",
        "\n",
        "# --- Inspect the final results ---\n",
        "logger.info(\"Final reports and artifacts have been generated in the output directory.\")\n",
        "logger.info(\"Displaying the final ablation study reports:\")\n",
        "\n",
        "# Display the t-test report\n",
        "print(\"\\n--- Ablation Paired T-Test Report ---\")\n",
        "print(final_artifacts['ablation_ttest_report'].to_markdown(index=False))\n",
        "\n",
        "# Display the KS-test report\n",
        "print(\"\\n--- Ablation Kolmogorov-Smirnov Test Report ---\")\n",
        "print(final_artifacts['ablation_ks_test_report'].to_markdown(index=False))\n",
        "\n",
        "# Display the main reports summary\n",
        "print(\"\\n--- Full Pipeline Report Summary ---\")\n",
        "with open(output_dir / \"results\" / \"full_pipeline_report.json\", 'r') as f:\n",
        "    full_report = json.load(f)\n",
        "print(f\"Overall Status: {full_report.get('pipeline_status')}\")\n",
        "if full_report.get('pipeline_status') == 'FAILURE':\n",
        "    print(f\"Failure Reason: {full_report.get('failure_reason')}\")\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "xO1UhC1b5TzM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 1 — Validate persona_hub_raw_df schema and quality\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 1: Validate persona_hub_raw_df schema and quality\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 1, Step 1 Helper: Schema and dtype enforcement\n",
        "# ------------------------------------------------------------------------------\n",
        "def _validate_chunk_schema(\n",
        "    chunk: pd.DataFrame,\n",
        "    expected_schema: Dict[str, Any],\n",
        "    chunk_number: int\n",
        ") -> List[str]:\n",
        "    \"\"\"\n",
        "    Validates the schema and dtypes of a single DataFrame chunk.\n",
        "\n",
        "    Args:\n",
        "        chunk: The pandas DataFrame chunk to validate.\n",
        "        expected_schema: A dictionary defining expected column names and their\n",
        "                         corresponding pandas dtypes or type-checking functions.\n",
        "        chunk_number: The sequential number of the chunk for error reporting.\n",
        "\n",
        "    Returns:\n",
        "        A list of error messages. An empty list indicates success.\n",
        "    \"\"\"\n",
        "    # Initialize a list to collect validation error messages for this chunk.\n",
        "    errors = []\n",
        "\n",
        "    # --- Column Presence Check ---\n",
        "    # Get the set of columns present in the current chunk.\n",
        "    actual_columns = set(chunk.columns)\n",
        "    # Get the set of expected column names from the schema definition.\n",
        "    expected_columns = set(expected_schema.keys())\n",
        "\n",
        "    # Check for any columns that are expected but missing in the chunk.\n",
        "    missing_columns = expected_columns - actual_columns\n",
        "    # If there are missing columns, format an error message.\n",
        "    if missing_columns:\n",
        "        errors.append(\n",
        "            f\"[Chunk {chunk_number}] Missing expected columns: {sorted(list(missing_columns))}\"\n",
        "        )\n",
        "\n",
        "    # Check for any columns that are present in the chunk but not expected.\n",
        "    extra_columns = actual_columns - expected_columns\n",
        "    # If there are extra columns, format an error message.\n",
        "    if extra_columns:\n",
        "        errors.append(\n",
        "            f\"[Chunk {chunk_number}] Found unexpected columns: {sorted(list(extra_columns))}\"\n",
        "        )\n",
        "\n",
        "    # If there are any column presence/absence errors, return immediately\n",
        "    # as subsequent dtype checks on missing/extra columns would fail.\n",
        "    if errors:\n",
        "        return errors\n",
        "\n",
        "    # --- Dtype Enforcement Check ---\n",
        "    # Iterate through each expected column and its specified dtype to validate.\n",
        "    for col, expected_dtype in expected_schema.items():\n",
        "        # Get the actual dtype of the column in the chunk.\n",
        "        actual_dtype = chunk[col].dtype\n",
        "        # Check if the actual dtype matches the expected dtype.\n",
        "        if str(actual_dtype) != str(expected_dtype):\n",
        "            errors.append(\n",
        "                f\"[Chunk {chunk_number}] Column '{col}' has incorrect dtype. \"\n",
        "                f\"Expected: {expected_dtype}, Actual: {actual_dtype}\"\n",
        "            )\n",
        "\n",
        "    # Return the list of all collected error messages.\n",
        "    return errors\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 1, Step 2 Helper: Content and distribution sanity checks\n",
        "# ------------------------------------------------------------------------------\n",
        "def _update_content_stats(\n",
        "    chunk: pd.DataFrame,\n",
        "    stats: Dict[str, Any],\n",
        "    config: Dict[str, Any]\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Updates running statistics for content validation from a single chunk.\n",
        "\n",
        "    Args:\n",
        "        chunk: The pandas DataFrame chunk to process.\n",
        "        stats: A dictionary holding the accumulated statistics across all chunks.\n",
        "        config: The study configuration dictionary.\n",
        "    \"\"\"\n",
        "    # --- Description Null Check ---\n",
        "    # Count the number of null values in the 'description' column for this chunk.\n",
        "    stats['null_description_count'] += chunk['description'].isna().sum()\n",
        "\n",
        "    # --- Token Count Distribution ---\n",
        "    # Count records where 'token_count' is less than 1.\n",
        "    stats['token_count_lt_1'] += (chunk['token_count'] < 1).sum()\n",
        "\n",
        "    # --- Language Code Validation ---\n",
        "    # Get the list of allowed language codes from the configuration.\n",
        "    language_allowlist = set(\n",
        "        config[\"phase_1_parameters\"][\"filtering\"][\"keyword_filtering\"][\"language_allowlist\"]\n",
        "    )\n",
        "    # Find unique language codes in the chunk that are not in the allowlist.\n",
        "    disallowed_languages = set(chunk['language_code'].unique()) - language_allowlist\n",
        "    # Update the set of all disallowed languages found so far.\n",
        "    stats['disallowed_languages'].update(disallowed_languages)\n",
        "\n",
        "    # --- Domain Tags Distribution ---\n",
        "    # Process the 'domain_tags' column, which may contain lists.\n",
        "    # Drop nulls, then flatten the list of lists into a single sequence of tags.\n",
        "    valid_tags = chunk['domain_tags'].dropna()\n",
        "    # Use a generator expression for memory-efficient iteration over tags.\n",
        "    flattened_tags = (tag for tag_list in valid_tags if isinstance(tag_list, list) for tag in tag_list)\n",
        "    # Update the frequency counter with the tags from this chunk.\n",
        "    stats['domain_tags_counter'].update(flattened_tags)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 1, Step 3 Helper: Hash integrity and shard coverage\n",
        "# ------------------------------------------------------------------------------\n",
        "# ==============================================================================\n",
        "# Standard Library and Third-Party Imports\n",
        "# ==============================================================================\n",
        "def _update_integrity_stats(\n",
        "    chunk: pd.DataFrame,\n",
        "    stats: Dict[str, Any],\n",
        "    total_rows: int,\n",
        "    sample_size: int = 10000\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Updates running statistics for hash integrity and shard coverage from a chunk.\n",
        "\n",
        "    This function performs two main operations on a given DataFrame chunk:\n",
        "    1.  **Hash Integrity Check (Sample-based):** It draws a probabilistic,\n",
        "        reproducible random sample of rows. For each sampled row, it re-computes\n",
        "        the SHA-256 hash of the 'description' and compares it to the stored hash.\n",
        "        It explicitly handles null descriptions to ensure deterministic hashing.\n",
        "    2.  **Shard Coverage:** It counts the occurrences of each 'shard_id' in the\n",
        "        chunk and updates a global counter to track the size of each shard.\n",
        "\n",
        "    This function is designed to be called iteratively in a streaming/chunked\n",
        "    data processing pipeline.\n",
        "\n",
        "    Args:\n",
        "        chunk: The pandas DataFrame chunk to process.\n",
        "        stats: A dictionary holding the accumulated statistics across all chunks.\n",
        "               This dictionary is modified in place.\n",
        "        total_rows: The total number of rows in the entire dataset, required for\n",
        "                    accurate probabilistic sampling.\n",
        "        sample_size: The desired size of the random sample for the hash check.\n",
        "\n",
        "    Raises:\n",
        "        KeyError: If required columns are missing from the chunk or stats dict.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    # Ensure required keys exist in the stats dictionary to prevent runtime errors.\n",
        "    required_stats_keys = [\n",
        "        'mismatched_hash_ids',\n",
        "        'rows_sampled_for_hash_check',\n",
        "        'shard_coverage_counter'\n",
        "    ]\n",
        "    if not all(key in stats for key in required_stats_keys):\n",
        "        raise KeyError(f\"Stats dictionary is missing one of the required keys: {required_stats_keys}\")\n",
        "\n",
        "    # --- Hash Integrity Check (Sample-based) ---\n",
        "    # Calculate the probability for selecting any given row to be part of the final sample.\n",
        "    # This ensures a uniform random sample across the entire dataset when processing in chunks.\n",
        "    sampling_prob = sample_size / total_rows\n",
        "\n",
        "    # Generate a random boolean mask to select rows for the sample based on the calculated probability.\n",
        "    sample_mask = np.random.rand(len(chunk)) < sampling_prob\n",
        "\n",
        "    # Apply the mask to the chunk to extract the sample for this chunk.\n",
        "    sample = chunk[sample_mask]\n",
        "\n",
        "    # Proceed only if the sample for this chunk is not empty.\n",
        "    if not sample.empty:\n",
        "        # Explicitly handle potential nulls in the description by replacing them with an empty string.\n",
        "        # This is a critical step for ensuring deterministic and correct hash re-computation.\n",
        "        descriptions_to_hash = sample['description'].fillna('').astype(str)\n",
        "\n",
        "        # Recompute the SHA-256 hash for the 'description' of each sampled row.\n",
        "        # The lambda function encodes the string to UTF-8 bytes, computes the hash, and gets the hex digest.\n",
        "        recomputed_hashes = descriptions_to_hash.apply(\n",
        "            lambda x: hashlib.sha256(x.encode('utf-8')).hexdigest()\n",
        "        )\n",
        "\n",
        "        # Identify rows where the newly computed hash does not match the stored hash.\n",
        "        mismatched_mask = recomputed_hashes != sample['sha256_description_hash']\n",
        "\n",
        "        # Extract the subset of the sample where mismatches occurred.\n",
        "        mismatched_rows = sample[mismatched_mask]\n",
        "\n",
        "        # If any mismatches were found, record their unique IDs for the final report.\n",
        "        if not mismatched_rows.empty:\n",
        "            stats['mismatched_hash_ids'].extend(mismatched_rows['id'].tolist())\n",
        "\n",
        "        # Update the counter for the total number of rows sampled so far.\n",
        "        stats['rows_sampled_for_hash_check'] += len(sample)\n",
        "\n",
        "    # --- Shard Coverage ---\n",
        "    # Calculate the frequency of each 'shard_id' within the current chunk.\n",
        "    shard_counts_in_chunk = chunk['shard_id'].value_counts()\n",
        "\n",
        "    # Update the global shard coverage counter with the counts from this chunk.\n",
        "    # The Counter's update method efficiently adds the new counts.\n",
        "    stats['shard_coverage_counter'].update(shard_counts_in_chunk.to_dict())\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 1, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "def validate_persona_hub_df(\n",
        "    df_path: str,\n",
        "    config: Dict[str, Any],\n",
        "    total_rows: int,\n",
        "    chunksize: int = 1_000_000\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Performs a comprehensive, streaming validation of the raw persona hub dataset.\n",
        "\n",
        "    This orchestrator function processes a very large dataset in chunks to maintain\n",
        "    a low memory footprint. It performs a battery of checks to ensure data\n",
        "    integrity before any costly processing begins. The validation includes:\n",
        "    1.  **Schema Enforcement**: Verifies that all columns are present with the\n",
        "        correct data types in every chunk.\n",
        "    2.  **Global ID Uniqueness**: Employs a robust, memory-aware method to ensure\n",
        "        every `id` across the ~370M records is unique.\n",
        "    3.  **Content Quality**: Checks for null descriptions, valid token counts,\n",
        "        and adherence to language allowlists.\n",
        "    4.  **Data Integrity**: Performs a sample-based SHA-256 hash check and\n",
        "        verifies complete shard coverage.\n",
        "\n",
        "    Args:\n",
        "        df_path: The file path to the Parquet dataset.\n",
        "        config: The study configuration dictionary containing validation parameters.\n",
        "        total_rows: The total number of rows in the dataset, which is essential\n",
        "                    for accurate probabilistic sampling and progress tracking.\n",
        "        chunksize: The number of rows to process in each memory-efficient chunk.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing a detailed validation report, including an\n",
        "        overall status, a list of specific errors, and computed statistics.\n",
        "    \"\"\"\n",
        "    # Define the expected schema for the persona_hub_raw_df DataFrame.\n",
        "    # This serves as the ground truth for schema validation.\n",
        "    expected_schema = {\n",
        "        'id': 'object', # Read as object initially for flexibility, then cast to string\n",
        "        'description': 'object',\n",
        "        'domain_tags': 'object',\n",
        "        'language_code': 'object',\n",
        "        'token_count': 'int64',\n",
        "        'sha256_description_hash': 'object',\n",
        "        'shard_id': 'int64'\n",
        "    }\n",
        "\n",
        "    # Initialize a dictionary to hold all accumulated statistics and validation results.\n",
        "    stats = {\n",
        "        'total_rows_processed': 0,\n",
        "        'id_uniqueness': {'is_unique': True, 'duplicate_ids': set()},\n",
        "        'null_description_count': 0,\n",
        "        'token_count_lt_1': 0,\n",
        "        'disallowed_languages': set(),\n",
        "        'domain_tags_counter': Counter(),\n",
        "        'mismatched_hash_ids': [],\n",
        "        'rows_sampled_for_hash_check': 0,\n",
        "        'shard_coverage_counter': Counter(),\n",
        "    }\n",
        "\n",
        "    # Initialize a set to track all unique IDs encountered for global uniqueness check.\n",
        "    # This is memory-intensive but necessary for a single-pass guarantee.\n",
        "    seen_ids: set[str] = set()\n",
        "    # Initialize a list to store any validation errors found.\n",
        "    validation_errors: List[str] = []\n",
        "    # Flag to ensure schema is checked only on the first chunk.\n",
        "    is_first_chunk = True\n",
        "\n",
        "    # Set a fixed random seed for reproducible sampling for the hash check.\n",
        "    np.random.seed(42)\n",
        "\n",
        "    # Log the start of the validation process.\n",
        "    logger.info(f\"Starting validation for '{df_path}' with {total_rows} total rows.\")\n",
        "\n",
        "    try:\n",
        "        # Create an iterator to read the Parquet file in chunks.\n",
        "        chunk_iterator = pd.read_parquet(df_path, chunksize=chunksize, engine='pyarrow')\n",
        "\n",
        "        # Loop through each chunk from the iterator with a progress bar.\n",
        "        for i, chunk in enumerate(chunk_iterator):\n",
        "            # --- Schema Validation (Step 1) ---\n",
        "            # On the first chunk, perform a thorough schema validation.\n",
        "            if is_first_chunk:\n",
        "                # Call the schema validation helper.\n",
        "                schema_errors = _validate_chunk_schema(chunk, expected_schema, i + 1)\n",
        "                # If any schema errors are found, add them to the main error list.\n",
        "                if schema_errors:\n",
        "                    # Add all found schema errors to the main list.\n",
        "                    validation_errors.extend(schema_errors)\n",
        "                    # If the schema is fundamentally wrong, stop processing immediately.\n",
        "                    logger.error(\"Fatal schema error on first chunk. Aborting validation.\")\n",
        "                    # Break the loop as further processing is unreliable.\n",
        "                    break\n",
        "                # Mark that the first chunk's schema has been successfully validated.\n",
        "                is_first_chunk = False\n",
        "\n",
        "            # --- ID Uniqueness Check (Corrected Logic) ---\n",
        "            # Ensure the 'id' column is treated as a string for consistency.\n",
        "            chunk_ids_series = chunk['id'].astype(str)\n",
        "\n",
        "            # 1. Check for duplicates *within* the current chunk.\n",
        "            if chunk_ids_series.nunique() != len(chunk_ids_series):\n",
        "                # Mark global uniqueness as failed.\n",
        "                stats['id_uniqueness']['is_unique'] = False\n",
        "                # Find and store the specific duplicate IDs within this chunk.\n",
        "                duplicates_in_chunk = chunk_ids_series[chunk_ids_series.duplicated()].unique()\n",
        "                stats['id_uniqueness']['duplicate_ids'].update(duplicates_in_chunk)\n",
        "\n",
        "            # 2. Check for duplicates *across* chunks.\n",
        "            # Get the set of unique IDs in the current chunk.\n",
        "            chunk_ids_set = set(chunk_ids_series.unique())\n",
        "            # Find the intersection with IDs seen in all previous chunks.\n",
        "            cross_chunk_duplicates = seen_ids.intersection(chunk_ids_set)\n",
        "            # If the intersection is not empty, a cross-chunk duplicate exists.\n",
        "            if cross_chunk_duplicates:\n",
        "                # Mark global uniqueness as failed.\n",
        "                stats['id_uniqueness']['is_unique'] = False\n",
        "                # Add these duplicates to our tracking set.\n",
        "                stats['id_uniqueness']['duplicate_ids'].update(cross_chunk_duplicates)\n",
        "\n",
        "            # 3. Update the global set of seen IDs with the unique IDs from this chunk.\n",
        "            seen_ids.update(chunk_ids_set)\n",
        "\n",
        "            # --- Content & Integrity Updates (Steps 2 & 3) ---\n",
        "            # Update content statistics (nulls, token counts, etc.) by calling the helper.\n",
        "            _update_content_stats(chunk, stats, config)\n",
        "            # Update integrity statistics (hash check sample, shard coverage) by calling the helper.\n",
        "            _update_integrity_stats(chunk, stats, total_rows)\n",
        "\n",
        "            # Update the total number of rows processed so far.\n",
        "            stats['total_rows_processed'] += len(chunk)\n",
        "            # Log progress periodically to provide feedback on the long-running process.\n",
        "            if (i + 1) % 5 == 0:\n",
        "                logger.info(f\"Processed chunk {i + 1}... ({stats['total_rows_processed']}/{total_rows} rows)\")\n",
        "\n",
        "    # Handle potential errors during file processing, such as the file not being found.\n",
        "    except FileNotFoundError:\n",
        "        validation_errors.append(f\"File not found at path: {df_path}\")\n",
        "    # Catch any other unexpected exceptions during the process.\n",
        "    except Exception as e:\n",
        "        validation_errors.append(f\"An unexpected error occurred during processing: {e}\")\n",
        "\n",
        "    # --- Final Report Generation ---\n",
        "    # Log the completion of the chunk processing phase.\n",
        "    logger.info(\"Chunk processing complete. Generating final validation report.\")\n",
        "\n",
        "    # Final check on token count requirement (>=99.9% of rows must have token_count >= 1).\n",
        "    token_count_pass = (stats['token_count_lt_1'] / stats['total_rows_processed']) < 0.001 if stats['total_rows_processed'] > 0 else True\n",
        "\n",
        "    # Construct the final, detailed validation report dictionary.\n",
        "    report = {\n",
        "        \"file_path\": df_path,\n",
        "        \"total_rows_validated\": stats['total_rows_processed'],\n",
        "        \"validation_status\": \"SUCCESS\" if not validation_errors and stats['id_uniqueness']['is_unique'] else \"FAILURE\",\n",
        "        \"errors\": validation_errors,\n",
        "        \"id_uniqueness\": {\n",
        "            \"status\": \"SUCCESS\" if stats['id_uniqueness']['is_unique'] else \"FAILURE\",\n",
        "            \"is_unique\": stats['id_uniqueness']['is_unique'],\n",
        "            \"duplicate_count\": len(stats['id_uniqueness']['duplicate_ids']),\n",
        "            \"duplicate_ids_sample\": sorted(list(stats['id_uniqueness']['duplicate_ids']))[:10],\n",
        "        },\n",
        "        \"content_quality\": {\n",
        "            \"description_null_ratio\": stats['null_description_count'] / stats['total_rows_processed'] if stats['total_rows_processed'] > 0 else 0,\n",
        "            \"token_count_ge_1_ratio\": 1.0 - (stats['token_count_lt_1'] / stats['total_rows_processed']) if stats['total_rows_processed'] > 0 else 0,\n",
        "            \"token_count_check_status\": \"SUCCESS\" if token_count_pass else \"FAILURE (>=99.9% requirement not met)\",\n",
        "            \"disallowed_languages_found\": sorted(list(stats['disallowed_languages'])),\n",
        "            \"top_10_domain_tags\": stats['domain_tags_counter'].most_common(10),\n",
        "        },\n",
        "        \"integrity_checks\": {\n",
        "            \"hash_check_sample_size\": stats['rows_sampled_for_hash_check'],\n",
        "            \"hash_mismatch_count\": len(stats['mismatched_hash_ids']),\n",
        "            \"hash_mismatch_ids_sample\": stats['mismatched_hash_ids'][:10],\n",
        "            \"hash_check_status\": \"SUCCESS\" if not stats['mismatched_hash_ids'] else \"FAILURE\",\n",
        "            \"shard_count\": len(stats['shard_coverage_counter']),\n",
        "            \"shard_size_summary\": {\n",
        "                \"min\": min(stats['shard_coverage_counter'].values()) if stats['shard_coverage_counter'] else 0,\n",
        "                \"max\": max(stats['shard_coverage_counter'].values()) if stats['shard_coverage_counter'] else 0,\n",
        "                \"mean\": np.mean(list(stats['shard_coverage_counter'].values())) if stats['shard_coverage_counter'] else 0,\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Log the final status of the validation.\n",
        "    if report[\"validation_status\"] == \"SUCCESS\":\n",
        "        logger.info(\"Validation successful. All checks passed.\")\n",
        "    else:\n",
        "        logger.warning(f\"Validation failed. See report for details.\")\n",
        "\n",
        "    # Return the comprehensive report.\n",
        "    return report\n"
      ],
      "metadata": {
        "id": "l_3TqoX2sIQF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 2 — Validate contextual_data_df, human benchmarks, realized outcomes, and config\n",
        "\n",
        "# ===================================================================================\n",
        "# Task 2: Validate contextual_data_df, human benchmarks, realized outcomes, & config\n",
        "# ===================================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 2, Step 1 Helper: Validate contextual_data_df\n",
        "# ------------------------------------------------------------------------------\n",
        "def _validate_contextual_df(\n",
        "    contextual_data_df: pd.DataFrame,\n",
        "    config: Dict[str, Any]\n",
        ") -> List[str]:\n",
        "    \"\"\"\n",
        "    Validates the schema, content, and integrity of contextual_data_df.\n",
        "\n",
        "    Args:\n",
        "        contextual_data_df: The DataFrame containing contextual data for prompts.\n",
        "        config: The study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        A list of error messages. An empty list indicates success.\n",
        "    \"\"\"\n",
        "    # Initialize a list to collect validation error messages.\n",
        "    errors = []\n",
        "    # Define the expected number of survey rounds.\n",
        "    EXPECTED_ROUNDS = 50\n",
        "    # Define the start and end rounds for the survey period.\n",
        "    START_ROUND, END_ROUND = \"2013Q1\", \"2025Q2\"\n",
        "\n",
        "    # --- Row Count and Round Coverage Validation ---\n",
        "    # Check if the DataFrame contains the expected number of rows.\n",
        "    if len(contextual_data_df) != EXPECTED_ROUNDS:\n",
        "        errors.append(f\"Expected {EXPECTED_ROUNDS} rows, but found {len(contextual_data_df)}.\")\n",
        "\n",
        "    # Generate the full set of expected survey rounds.\n",
        "    expected_rounds = set(pd.period_range(start=START_ROUND, end=END_ROUND, freq='Q').strftime('%YQ%q'))\n",
        "    # Get the set of actual survey rounds from the DataFrame.\n",
        "    actual_rounds = set(contextual_data_df['survey_round'])\n",
        "    # Check for any missing or extra survey rounds.\n",
        "    if expected_rounds != actual_rounds:\n",
        "        missing = sorted(list(expected_rounds - actual_rounds))\n",
        "        extra = sorted(list(actual_rounds - expected_rounds))\n",
        "        errors.append(f\"Mismatch in survey rounds. Missing: {missing}. Extra: {extra}.\")\n",
        "\n",
        "    # --- Date and Text Field Validation ---\n",
        "    # Iterate through each row to perform detailed content validation.\n",
        "    for _, row in contextual_data_df.iterrows():\n",
        "        round_id = row['survey_round']\n",
        "        # Validate the survey_round format using a regular expression.\n",
        "        if not re.match(r'^\\d{4}Q[1-4]$', round_id):\n",
        "            errors.append(f\"Round '{round_id}': Invalid format for 'survey_round'.\")\n",
        "\n",
        "        # Validate temporal consistency: ECB meeting must not be after the survey date.\n",
        "        if pd.notna(row['ecb_meeting_date']) and pd.notna(row['survey_date']):\n",
        "            if row['ecb_meeting_date'] > row['survey_date']:\n",
        "                errors.append(f\"Round '{round_id}': 'ecb_meeting_date' is after 'survey_date'.\")\n",
        "\n",
        "        # Validate ECB communication text content.\n",
        "        if not isinstance(row['ecb_communication_text'], str) or not row['ecb_communication_text'].strip():\n",
        "            errors.append(f\"Round '{round_id}': 'ecb_communication_text' is empty or not a string.\")\n",
        "\n",
        "        # Validate token count is a positive integer.\n",
        "        if not isinstance(row['ecb_communication_token_count'], (int, np.integer)) or row['ecb_communication_token_count'] <= 0:\n",
        "            errors.append(f\"Round '{round_id}': 'ecb_communication_token_count' is not a positive integer.\")\n",
        "\n",
        "    # --- Nested Dictionary Validation ---\n",
        "    # Define the required keys for the nested dictionaries.\n",
        "    MACRO_VARS = {'hicp', 'hicpx', 'unr', 'rgdp'}\n",
        "    # Retrieve the round from which HICPX data is expected to be available.\n",
        "    hicpx_start_round = config['phase_2_parameters']['oos_and_availability']['hicpx_available_from_round']\n",
        "\n",
        "    # Iterate through each row again for nested structure validation.\n",
        "    for _, row in contextual_data_df.iterrows():\n",
        "        round_id = row['survey_round']\n",
        "\n",
        "        # Validate 'latest_realized_data' structure.\n",
        "        realized_data = row['latest_realized_data']\n",
        "        if not isinstance(realized_data, dict) or set(realized_data.keys()) != MACRO_VARS:\n",
        "            errors.append(f\"Round '{round_id}': 'latest_realized_data' has incorrect keys.\")\n",
        "        else:\n",
        "            # Check sub-dictionary structure and HICPX availability logic.\n",
        "            for var, data in realized_data.items():\n",
        "                if not isinstance(data, dict) or set(data.keys()) != {'period', 'value'}:\n",
        "                    errors.append(f\"Round '{round_id}': 'latest_realized_data' for '{var}' has incorrect structure.\")\n",
        "                # If the round is before HICPX availability, its value must be NaN.\n",
        "                if var == 'hicpx' and round_id < hicpx_start_round and pd.notna(data.get('value')):\n",
        "                    errors.append(f\"Round '{round_id}': HICPX value should be NaN before {hicpx_start_round}.\")\n",
        "\n",
        "        # Validate 'previous_spf_medians' structure.\n",
        "        prev_medians = row['previous_spf_medians']\n",
        "        if not isinstance(prev_medians, dict) or set(prev_medians.keys()) != MACRO_VARS:\n",
        "            errors.append(f\"Round '{round_id}': 'previous_spf_medians' has incorrect keys.\")\n",
        "        else:\n",
        "            # Check that sub-dictionary keys are integers (years).\n",
        "            for var, data in prev_medians.items():\n",
        "                if not all(isinstance(k, int) for k in data.keys()):\n",
        "                    errors.append(f\"Round '{round_id}': 'previous_spf_medians' for '{var}' has non-integer keys.\")\n",
        "\n",
        "    # Return the list of all collected error messages.\n",
        "    return errors\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 2, Step 2 Helper: Validate human survey data\n",
        "# ------------------------------------------------------------------------------\n",
        "def _validate_human_surveys(\n",
        "    benchmark_df: pd.DataFrame,\n",
        "    micro_df: pd.DataFrame,\n",
        "    contextual_data_df: pd.DataFrame,\n",
        "    config: Dict[str, Any]\n",
        ") -> List[str]:\n",
        "    \"\"\"\n",
        "    Validates human_survey_predictions_benchmark_df and human_survey_micro_df.\n",
        "\n",
        "    Args:\n",
        "        benchmark_df: DataFrame with human median forecasts.\n",
        "        micro_df: DataFrame with individual human forecasts.\n",
        "        contextual_data_df: DataFrame with contextual data for cross-validation.\n",
        "        config: The study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        A list of error messages. An empty list indicates success.\n",
        "    \"\"\"\n",
        "    # Initialize a list to collect validation error messages.\n",
        "    errors = []\n",
        "    # Define expected categorical values.\n",
        "    EXPECTED_VARS = {'HICP', 'HICPX', 'RGDP', 'UNR'}\n",
        "    EXPECTED_HORIZONS = {'CY', 'CY+1', 'CY+2', 'LT'}\n",
        "\n",
        "    # Retrieve configuration parameters for OOS and HICPX rules.\n",
        "    oos_rounds = set(config['phase_2_parameters']['oos_and_availability']['oos_rounds'])\n",
        "    oos_horizons = set(config['phase_2_parameters']['oos_and_availability']['oos_scored_horizons'])\n",
        "    hicpx_start_round = config['phase_2_parameters']['oos_and_availability']['hicpx_available_from_round']\n",
        "\n",
        "    # --- Loop through both human forecast DataFrames for consistent validation ---\n",
        "    for df_name, df in [(\"benchmark\", benchmark_df), (\"micro\", micro_df)]:\n",
        "        # Validate categorical columns.\n",
        "        if not set(df['variable'].unique()).issubset(EXPECTED_VARS):\n",
        "            errors.append(f\"[{df_name}] Contains unexpected values in 'variable' column.\")\n",
        "        if not set(df['horizon'].unique()).issubset(EXPECTED_HORIZONS):\n",
        "            errors.append(f\"[{df_name}] Contains unexpected values in 'horizon' column.\")\n",
        "\n",
        "        # --- Target Year Integrity Check ---\n",
        "        # Recompute target_year to validate against the provided column.\n",
        "        df_copy = df.copy()\n",
        "        df_copy['base_year'] = df_copy['round'].str[:4].astype(int)\n",
        "        horizon_map = {'CY': 0, 'CY+1': 1, 'CY+2': 2}\n",
        "        df_copy['computed_target_year'] = df_copy['base_year'] + df_copy['horizon'].map(horizon_map)\n",
        "\n",
        "        # Handle the 'LT' horizon by joining with contextual data.\n",
        "        lt_map = contextual_data_df.set_index('survey_round')['lt_year']\n",
        "        lt_mask = df_copy['horizon'] == 'LT'\n",
        "        df_copy.loc[lt_mask, 'computed_target_year'] = df_copy.loc[lt_mask, 'round'].map(lt_map)\n",
        "\n",
        "        # Check if the computed target year matches the one in the DataFrame.\n",
        "        if not df_copy['target_year'].equals(df_copy['computed_target_year']):\n",
        "            errors.append(f\"[{df_name}] 'target_year' column is inconsistent with 'round' and 'horizon'.\")\n",
        "\n",
        "        # --- HICPX and OOS Business Rule Enforcement ---\n",
        "        # Check for HICPX data appearing before its official start round.\n",
        "        if not df.query(\"variable == 'HICPX' and round < @hicpx_start_round\").empty:\n",
        "            errors.append(f\"[{df_name}] Found HICPX data before {hicpx_start_round}.\")\n",
        "\n",
        "        # Check for disallowed horizons in out-of-sample rounds.\n",
        "        oos_df = df[df['round'].isin(oos_rounds)]\n",
        "        if not oos_df[~oos_df['horizon'].isin(oos_horizons)].empty:\n",
        "            errors.append(f\"[{df_name}] Found disallowed horizons in out-of-sample rounds.\")\n",
        "\n",
        "    # --- Uniqueness Check for Micro Data ---\n",
        "    # Verify that each forecaster has only one prediction per round/variable/horizon.\n",
        "    if micro_df.duplicated(subset=['round', 'variable', 'horizon', 'forecaster_id']).any():\n",
        "        errors.append(\"[micro] Found duplicate entries for the same forecaster, round, variable, and horizon.\")\n",
        "\n",
        "    # Return the list of all collected error messages.\n",
        "    return errors\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 2, Step 3 Helper: Validate realized outcomes and config\n",
        "# ------------------------------------------------------------------------------\n",
        "def _validate_realized_and_config(\n",
        "    realized_macro_data_df: pd.DataFrame,\n",
        "    config: Dict[str, Any]\n",
        ") -> List[str]:\n",
        "    \"\"\"\n",
        "    Validates realized_macro_data_df and the study configuration dictionary.\n",
        "\n",
        "    Args:\n",
        "        realized_macro_data_df: DataFrame with realized macroeconomic outcomes.\n",
        "        config: The study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        A list of error messages. An empty list indicates success.\n",
        "    \"\"\"\n",
        "    # Initialize a list to collect validation error messages.\n",
        "    errors = []\n",
        "\n",
        "    # --- Realized Data Validation ---\n",
        "    # Check for duplicate entries for the same year and variable.\n",
        "    if realized_macro_data_df.duplicated(subset=['reference_year', 'variable']).any():\n",
        "        errors.append(\"Found duplicate entries in 'realized_macro_data_df'.\")\n",
        "\n",
        "    # Validate the 'first_release_date' to prevent look-ahead bias.\n",
        "    # The first release of an annual figure should not be before the end of that year.\n",
        "    # A stricter check: it must be after Jan 1 of the following year.\n",
        "    df_copy = realized_macro_data_df.copy()\n",
        "    df_copy['min_release_date'] = pd.to_datetime(df_copy['reference_year'] + 1, format='%Y')\n",
        "    if (df_copy['first_release_date'] < df_copy['min_release_date']).any():\n",
        "        errors.append(\"Found 'first_release_date' values that imply look-ahead bias.\")\n",
        "\n",
        "    # --- Configuration Dictionary Validation ---\n",
        "    # This is a critical check of key parameters that define the experiment.\n",
        "    try:\n",
        "        # Phase 1: Persona Generation Parameters\n",
        "        p1 = config['phase_1_parameters']\n",
        "        if p1['llm_as_judge']['model_name'] != 'gpt-4o-mini':\n",
        "            errors.append(\"Config: Judge LLM must be 'gpt-4o-mini'.\")\n",
        "        if p1['llm_as_judge']['api_settings']['temperature'] != 1.0:\n",
        "            errors.append(\"Config: Judge LLM temperature must be 1.0.\")\n",
        "\n",
        "        # Phase 2: Forecasting and Evaluation Parameters\n",
        "        p2 = config['phase_2_parameters']\n",
        "        if p2['forecasting_llm']['model_name'] != 'gpt-4o-2024-11-20':\n",
        "            errors.append(\"Config: Forecasting LLM must be 'gpt-4o-2024-11-20'.\")\n",
        "        if p2['forecasting_llm']['api_settings']['temperature'] != 1.0:\n",
        "            errors.append(\"Config: Forecasting LLM temperature must be 1.0.\")\n",
        "        if p2['scoring_and_inference']['monte_carlo_simulations'] != 10000:\n",
        "            errors.append(\"Config: Monte Carlo simulations must be 10000.\")\n",
        "        if p2['scoring_and_inference']['monte_carlo_seed'] != 42:\n",
        "            errors.append(\"Config: Monte Carlo seed must be 42.\")\n",
        "        if p2['ablation_study']['baseline_runs_per_round'] != 100:\n",
        "            errors.append(\"Config: Baseline runs per round must be 100.\")\n",
        "\n",
        "        # Check OOS rounds and HICPX start date for consistency.\n",
        "        expected_oos = [\"2024Q1\", \"2024Q2\", \"2024Q3\", \"2024Q4\", \"2025Q1\", \"2025Q2\"]\n",
        "        if p2['oos_and_availability']['oos_rounds'] != expected_oos:\n",
        "            errors.append(\"Config: 'oos_rounds' list is incorrect.\")\n",
        "        if p2['oos_and_availability']['hicpx_available_from_round'] != '2016Q4':\n",
        "            errors.append(\"Config: 'hicpx_available_from_round' must be '2016Q4'.\")\n",
        "\n",
        "    except KeyError as e:\n",
        "        errors.append(f\"Config is missing a required key: {e}\")\n",
        "    except Exception as e:\n",
        "        errors.append(f\"An unexpected error occurred during config validation: {e}\")\n",
        "\n",
        "    # Return the list of all collected error messages.\n",
        "    return errors\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 2, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "def validate_analytical_inputs(\n",
        "    contextual_data_df: pd.DataFrame,\n",
        "    human_survey_predictions_benchmark_df: pd.DataFrame,\n",
        "    human_survey_micro_df: pd.DataFrame,\n",
        "    realized_macro_data_df: pd.DataFrame,\n",
        "    config: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrates the validation of all analytical inputs for the study.\n",
        "\n",
        "    This function serves as a master validator for the core datasets and the\n",
        "    configuration dictionary that drive the forecasting and analysis phases.\n",
        "    It ensures data integrity, alignment between tables, and adherence to the\n",
        "    study's methodological parameters before proceeding with computation.\n",
        "\n",
        "    Args:\n",
        "        contextual_data_df: The DataFrame with contextual data for prompts.\n",
        "        human_survey_predictions_benchmark_df: DataFrame with human median forecasts.\n",
        "        human_survey_micro_df: DataFrame with individual human forecasts.\n",
        "        realized_macro_data_df: DataFrame with realized macroeconomic outcomes.\n",
        "        config: The study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing a detailed validation report.\n",
        "        Raises ValueError if any critical validation fails.\n",
        "    \"\"\"\n",
        "    # Log the start of the analytical input validation process.\n",
        "    logger.info(\"Starting validation of all analytical inputs...\")\n",
        "\n",
        "    # Initialize a dictionary to hold all validation errors.\n",
        "    all_errors = {}\n",
        "\n",
        "    # --- Step 1: Validate contextual_data_df ---\n",
        "    # This DataFrame is central, so its validation is critical.\n",
        "    logger.info(\"Validating 'contextual_data_df'...\")\n",
        "    contextual_errors = _validate_contextual_df(contextual_data_df, config)\n",
        "    if contextual_errors:\n",
        "        all_errors['contextual_data_df'] = contextual_errors\n",
        "\n",
        "    # --- Step 2: Validate human survey data ---\n",
        "    # These DataFrames must be consistent with each other and with contextual data.\n",
        "    logger.info(\"Validating human survey DataFrames...\")\n",
        "    human_errors = _validate_human_surveys(\n",
        "        human_survey_predictions_benchmark_df,\n",
        "        human_survey_micro_df,\n",
        "        contextual_data_df,\n",
        "        config\n",
        "    )\n",
        "    if human_errors:\n",
        "        all_errors['human_surveys'] = human_errors\n",
        "\n",
        "    # --- Step 3: Validate realized outcomes and config ---\n",
        "    # These components define the ground truth for scoring and the experiment's rules.\n",
        "    logger.info(\"Validating realized outcomes and configuration...\")\n",
        "    realized_config_errors = _validate_realized_and_config(\n",
        "        realized_macro_data_df,\n",
        "        config\n",
        "    )\n",
        "    if realized_config_errors:\n",
        "        all_errors['realized_and_config'] = realized_config_errors\n",
        "\n",
        "    # --- Final Report Generation ---\n",
        "    # Determine the overall validation status based on whether any errors were found.\n",
        "    validation_status = \"SUCCESS\" if not all_errors else \"FAILURE\"\n",
        "\n",
        "    # Construct the final, comprehensive validation report.\n",
        "    report = {\n",
        "        \"overall_status\": validation_status,\n",
        "        \"validation_details\": {\n",
        "            \"contextual_data\": \"SUCCESS\" if 'contextual_data_df' not in all_errors else \"FAILURE\",\n",
        "            \"human_surveys\": \"SUCCESS\" if 'human_surveys' not in all_errors else \"FAILURE\",\n",
        "            \"realized_and_config\": \"SUCCESS\" if 'realized_and_config' not in all_errors else \"FAILURE\",\n",
        "        },\n",
        "        \"error_messages\": all_errors if all_errors else \"All checks passed.\"\n",
        "    }\n",
        "\n",
        "    # If validation failed, log the details and raise a critical error.\n",
        "    if validation_status == \"FAILURE\":\n",
        "        logger.error(\"Analytical input validation failed. See report for details.\")\n",
        "        logger.error(f\"Error details: {all_errors}\")\n",
        "        raise ValueError(\"Critical validation of analytical inputs failed.\")\n",
        "\n",
        "    # If validation succeeded, log the success message.\n",
        "    logger.info(\"Validation of all analytical inputs completed successfully.\")\n",
        "\n",
        "    # Return the comprehensive report.\n",
        "    return report\n"
      ],
      "metadata": {
        "id": "9F-wvGhM1qYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 3 — Cleanse persona_hub_raw_df (minimal, lossless)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 3: Cleanse persona_hub_raw_df (minimal, lossless)\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 3, Step 1 Helper: Text normalization (description)\n",
        "# ------------------------------------------------------------------------------\n",
        "def _normalize_descriptions(description_series: pd.Series) -> pd.Series:\n",
        "    \"\"\"\n",
        "    Applies a series of normalization steps to the description text.\n",
        "\n",
        "    Args:\n",
        "        description_series: A pandas Series containing the raw description strings.\n",
        "\n",
        "    Returns:\n",
        "        A pandas Series with the normalized description strings.\n",
        "    \"\"\"\n",
        "    # Ensure the input is treated as string type, converting NaNs to empty strings for processing.\n",
        "    normalized = description_series.fillna('').astype(str)\n",
        "\n",
        "    # 1. Strip leading/trailing whitespace.\n",
        "    normalized = normalized.str.strip()\n",
        "\n",
        "    # 2. Replace consecutive whitespace characters with a single space.\n",
        "    normalized = normalized.str.replace(r'\\s+', ' ', regex=True)\n",
        "\n",
        "    # 3. Remove ASCII control characters (U+0000–U+001F, U+007F–U+009F),\n",
        "    #    preserving tab (\\t, \\x09) and newline (\\n, \\x0A).\n",
        "    control_char_regex = r'[\\x00-\\x08\\x0B\\x0C\\x0E-\\x1F\\x7F-\\x9F]'\n",
        "    normalized = normalized.str.replace(control_char_regex, '', regex=True)\n",
        "\n",
        "    # 4. Normalize Unicode to NFC form for consistent representation.\n",
        "    #    .apply() is used as there's no direct vectorized method in pandas.\n",
        "    normalized = normalized.apply(lambda x: unicodedata.normalize('NFC', x) if isinstance(x, str) else x)\n",
        "\n",
        "    # Return the fully normalized series.\n",
        "    return normalized\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 3, Step 3 Helper: Validate and clean auxiliary fields\n",
        "# ------------------------------------------------------------------------------\n",
        "def _normalize_domain_tags(tag_data: Union[None, str, List[str]]) -> Optional[List[str]]:\n",
        "    \"\"\"\n",
        "    Normalizes a single entry from the 'domain_tags' column.\n",
        "\n",
        "    Handles JSON strings, lists, and nulls, returning a list of lowercase strings\n",
        "    or None if the input is invalid or results in an empty list.\n",
        "\n",
        "    Args:\n",
        "        tag_data: The input data from a single cell in the 'domain_tags' column.\n",
        "\n",
        "    Returns:\n",
        "        A list of lowercase strings or None.\n",
        "    \"\"\"\n",
        "    # If the input is already a list, process it.\n",
        "    if isinstance(tag_data, list):\n",
        "        # Create a new list containing only lowercase string elements.\n",
        "        processed_tags = [str(tag).lower() for tag in tag_data if isinstance(tag, (str, int, float))]\n",
        "        # Return the list if it's not empty, otherwise return None.\n",
        "        return processed_tags if processed_tags else None\n",
        "\n",
        "    # If the input is a string, attempt to parse it as JSON.\n",
        "    if isinstance(tag_data, str):\n",
        "        try:\n",
        "            # Decode the JSON string into a Python object.\n",
        "            data = json.loads(tag_data)\n",
        "            # If the decoded object is a list, recursively call this function.\n",
        "            if isinstance(data, list):\n",
        "                return _normalize_domain_tags(data)\n",
        "        except json.JSONDecodeError:\n",
        "            # If JSON parsing fails, the string is considered invalid tag data.\n",
        "            return None\n",
        "\n",
        "    # For any other type (e.g., None, NaN, int), return None.\n",
        "    return None\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 3, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "def cleanse_persona_hub_df(\n",
        "    raw_df_path: str,\n",
        "    cleansed_df_path: str,\n",
        "    config: Dict[str, Any],\n",
        "    chunksize: int = 1_000_000\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Cleanses the raw persona hub dataset and saves it to a new Parquet file.\n",
        "\n",
        "    This function processes the raw dataset in a streaming fashion to handle its\n",
        "    large size efficiently. The cleansing pipeline includes:\n",
        "    1.  Normalizing the 'description' text (whitespace, Unicode, control chars).\n",
        "    2.  Filtering out records with empty descriptions, disallowed language codes,\n",
        "        or invalid token counts.\n",
        "    3.  Normalizing the 'domain_tags' field into a consistent list format.\n",
        "    4.  Re-computing the 'sha256_description_hash' from the cleansed description.\n",
        "    5.  Setting the unique 'id' column as the DataFrame index and preserving it\n",
        "        in the output Parquet file for efficient downstream lookups.\n",
        "\n",
        "    Args:\n",
        "        raw_df_path: Path to the raw input Parquet file.\n",
        "        cleansed_df_path: Path to save the cleansed output Parquet file.\n",
        "        config: The study configuration dictionary.\n",
        "        chunksize: The number of rows to process in each memory-efficient chunk.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing a detailed report of the cleansing process.\n",
        "    \"\"\"\n",
        "    # Log the start of the cleansing process.\n",
        "    logger.info(f\"Starting cleansing of '{raw_df_path}'. Output will be saved to '{cleansed_df_path}'.\")\n",
        "\n",
        "    # Initialize a report dictionary to track statistics across all chunks.\n",
        "    report = {\n",
        "        'rows_read': 0,\n",
        "        'rows_written': 0,\n",
        "        'rows_dropped': {\n",
        "            'empty_description': 0,\n",
        "            'disallowed_language': 0,\n",
        "            'invalid_token_count': 0,\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Retrieve the language allowlist from the configuration for filtering.\n",
        "    language_allowlist = set(\n",
        "        config[\"phase_1_parameters\"][\"filtering\"][\"keyword_filtering\"][\"language_allowlist\"]\n",
        "    )\n",
        "\n",
        "    # Open the raw Parquet file for reading.\n",
        "    parquet_file = pq.ParquetFile(raw_df_path)\n",
        "    # Initialize the Parquet writer, which will be created with the schema of the first valid chunk.\n",
        "    writer: Optional[pq.ParquetWriter] = None\n",
        "\n",
        "    try:\n",
        "        # Create an iterator to read the file in batches (chunks).\n",
        "        batch_iterator = parquet_file.iter_batches(batch_size=chunksize, use_pandas_metadata=True)\n",
        "\n",
        "        # Process each chunk from the iterator.\n",
        "        for i, chunk_batch in enumerate(batch_iterator):\n",
        "            # Convert the Arrow Batch to a pandas DataFrame for processing.\n",
        "            chunk_df = chunk_batch.to_pandas()\n",
        "            # Get the number of rows in the current chunk.\n",
        "            rows_in_chunk = len(chunk_df)\n",
        "            # Update the total number of rows read from the source file.\n",
        "            report['rows_read'] += rows_in_chunk\n",
        "\n",
        "            # --- Step 1: Text Normalization ---\n",
        "            # Apply the normalization function to the 'description' column.\n",
        "            chunk_df['description'] = _normalize_descriptions(chunk_df['description'])\n",
        "\n",
        "            # --- Step 3 (Filtering Part): Create validity masks ---\n",
        "            # Mask 1: Identify rows with empty descriptions after normalization.\n",
        "            mask_empty_desc = chunk_df['description'].str.strip() == ''\n",
        "            # Update the count of rows dropped for this reason.\n",
        "            report['rows_dropped']['empty_description'] += mask_empty_desc.sum()\n",
        "\n",
        "            # Mask 2: Identify rows with language codes not in the allowlist.\n",
        "            mask_lang = ~chunk_df['language_code'].isin(language_allowlist)\n",
        "            # Update the count of rows dropped for this reason.\n",
        "            report['rows_dropped']['disallowed_language'] += mask_lang.sum()\n",
        "\n",
        "            # Mask 3: Identify rows with invalid token counts (e.g., less than 1).\n",
        "            mask_token = chunk_df['token_count'] < 1\n",
        "            # Update the count of rows dropped for this reason.\n",
        "            report['rows_dropped']['invalid_token_count'] += mask_token.sum()\n",
        "\n",
        "            # Combine all filtering criteria into a single validity mask.\n",
        "            # A row is kept if it is NOT in any of the drop masks.\n",
        "            valid_mask = ~(mask_empty_desc | mask_lang | mask_token)\n",
        "\n",
        "            # Apply the mask to filter the chunk, creating a new DataFrame with only valid rows.\n",
        "            cleansed_chunk = chunk_df[valid_mask].copy()\n",
        "\n",
        "            # If no rows in this chunk are valid, skip to the next chunk.\n",
        "            if cleansed_chunk.empty:\n",
        "                logger.info(f\"Chunk {i+1}: No valid rows after filtering.\")\n",
        "                continue\n",
        "\n",
        "            # --- Step 2 & 3 (Transformation Part) ---\n",
        "            # Cast the 'id' column to the pandas 'string' dtype for consistency.\n",
        "            cleansed_chunk['id'] = cleansed_chunk['id'].astype(\"string\")\n",
        "\n",
        "            # Re-compute the SHA-256 hash based on the *normalized* description text.\n",
        "            cleansed_chunk['sha256_description_hash'] = cleansed_chunk['description'].apply(\n",
        "                lambda x: hashlib.sha256(x.encode('utf-8')).hexdigest()\n",
        "            )\n",
        "\n",
        "            # Normalize the 'domain_tags' column using the dedicated helper function.\n",
        "            cleansed_chunk['domain_tags'] = cleansed_chunk['domain_tags'].apply(_normalize_domain_tags)\n",
        "\n",
        "            # --- CORRECTED LOGIC: Set 'id' as index before writing ---\n",
        "            # Set the 'id' column as the DataFrame index. This is the primary key.\n",
        "            # `verify_integrity=True` ensures there are no duplicate IDs within this chunk.\n",
        "            cleansed_chunk.set_index('id', inplace=True, verify_integrity=True)\n",
        "\n",
        "            # --- Streaming Write ---\n",
        "            # Convert the cleansed pandas DataFrame (with its index) to an Arrow Table.\n",
        "            # `preserve_index=True` is the critical correction to include the 'id' index in the output file.\n",
        "            table = pa.Table.from_pandas(cleansed_chunk, preserve_index=True)\n",
        "\n",
        "            # If this is the first chunk being written, initialize the ParquetWriter with the table's schema.\n",
        "            if writer is None:\n",
        "                # The schema will now correctly include the index.\n",
        "                writer = pq.ParquetWriter(cleansed_df_path, table.schema)\n",
        "\n",
        "            # Write the current cleansed chunk (as an Arrow Table) to the output Parquet file.\n",
        "            writer.write_table(table)\n",
        "\n",
        "            # Update the total count of rows successfully written to the new file.\n",
        "            report['rows_written'] += len(cleansed_chunk)\n",
        "            # Log progress for the current chunk.\n",
        "            logger.info(f\"Processed chunk {i+1}: Read {rows_in_chunk}, Wrote {len(cleansed_chunk)}\")\n",
        "\n",
        "    finally:\n",
        "        # This block ensures that the writer is always closed properly, finalizing the file.\n",
        "        if writer:\n",
        "            # Close the Parquet writer to complete the file writing process.\n",
        "            writer.close()\n",
        "            logger.info(f\"Parquet writer closed. Cleansed file saved to '{cleansed_df_path}'.\")\n",
        "\n",
        "    # --- Final Report ---\n",
        "    # Calculate the total number of rows dropped across all criteria.\n",
        "    total_dropped = sum(report['rows_dropped'].values())\n",
        "    # Calculate the overall rejection rate.\n",
        "    rejection_rate = 1 - (report['rows_written'] / report['rows_read']) if report['rows_read'] > 0 else 0\n",
        "    # Add the rejection rate to the report, formatted as a percentage.\n",
        "    report['rejection_rate'] = f\"{rejection_rate:.4%}\"\n",
        "\n",
        "    # Log the final summary of the entire cleansing process.\n",
        "    logger.info(f\"Cleansing complete. Total rows read: {report['rows_read']}.\")\n",
        "    logger.info(f\"Total rows written: {report['rows_written']}. Total rows dropped: {total_dropped}.\")\n",
        "\n",
        "    # Return the final, comprehensive report.\n",
        "    return report\n"
      ],
      "metadata": {
        "id": "LrwnIHso200r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 4 — Cleanse contextual_data_df, human panels, and realized outcomes\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 4: Cleanse contextual_data_df, human panels, and realized outcomes\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 4, Step 1 Helper: Clean contextual_data_df text and dates\n",
        "# ------------------------------------------------------------------------------\n",
        "def _cleanse_contextual_df(\n",
        "    contextual_data_df: pd.DataFrame,\n",
        "    config: Dict[str, Any]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Cleanses the contextual_data_df by normalizing text, parsing dates,\n",
        "    and re-computing token counts.\n",
        "\n",
        "    Args:\n",
        "        contextual_data_df: The raw (but validated) contextual data DataFrame.\n",
        "        config: The study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        A cleansed copy of the contextual data DataFrame.\n",
        "    \"\"\"\n",
        "    # Create a deep copy to avoid modifying the original DataFrame.\n",
        "    df = contextual_data_df.copy()\n",
        "\n",
        "    # --- Text Normalization (ecb_communication_text) ---\n",
        "    # Define the regex for control characters, preserving tab and newline.\n",
        "    control_char_regex = r'[\\x00-\\x08\\x0B\\x0C\\x0E-\\x1F\\x7F-\\x9F]'\n",
        "    # Apply a sequence of cleansing operations.\n",
        "    df['ecb_communication_text'] = (\n",
        "        df['ecb_communication_text']\n",
        "        .str.strip()\n",
        "        .str.replace(r'\\s+', ' ', regex=True)\n",
        "        .str.replace(control_char_regex, '', regex=True)\n",
        "        .apply(lambda x: unicodedata.normalize('NFC', x))\n",
        "    )\n",
        "\n",
        "    # --- Date Parsing ---\n",
        "    # Convert date columns to datetime objects, assuming European date format.\n",
        "    df['survey_date'] = pd.to_datetime(df['survey_date'], dayfirst=True, errors='coerce')\n",
        "    df['ecb_meeting_date'] = pd.to_datetime(df['ecb_meeting_date'], dayfirst=True, errors='coerce')\n",
        "    # Post-parsing validation for temporal consistency.\n",
        "    if (df['ecb_meeting_date'] > df['survey_date']).any():\n",
        "        raise ValueError(\"Cleansing failed: Found 'ecb_meeting_date' after 'survey_date'.\")\n",
        "\n",
        "    # --- Token Count Recomputation ---\n",
        "    # Re-compute token counts to ensure accuracy for the specified model.\n",
        "    try:\n",
        "        # Get the model name from the config.\n",
        "        model_name = config['phase_2_parameters']['forecasting_llm']['model_name']\n",
        "        # Get the tokenizer for the specified model.\n",
        "        encoding = tiktoken.encoding_for_model(model_name)\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Could not get tiktoken encoding for model '{model_name}'. Error: {e}\")\n",
        "        raise\n",
        "\n",
        "    # Apply the tokenizer to re-calculate the token count for each communication text.\n",
        "    df['ecb_communication_token_count'] = df['ecb_communication_text'].apply(\n",
        "        lambda x: len(encoding.encode(x))\n",
        "    )\n",
        "\n",
        "    # Return the cleansed DataFrame.\n",
        "    return df\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 4, Step 2 Helper: Normalize nested dicts and compute target_year\n",
        "# ------------------------------------------------------------------------------\n",
        "def _normalize_nested_dicts_and_compute_target_year(\n",
        "    contextual_df: pd.DataFrame,\n",
        "    benchmark_df: pd.DataFrame,\n",
        "    micro_df: pd.DataFrame,\n",
        "    config: Dict[str, Any]\n",
        ") -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Normalizes nested dictionaries in contextual_df and computes the canonical\n",
        "    'target_year' for human survey DataFrames.\n",
        "\n",
        "    Args:\n",
        "        contextual_df: The cleansed contextual data DataFrame.\n",
        "        benchmark_df: The raw benchmark survey DataFrame.\n",
        "        micro_df: The raw micro survey DataFrame.\n",
        "        config: The study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing the cleansed versions of (contextual_df,\n",
        "        benchmark_df, micro_df).\n",
        "    \"\"\"\n",
        "    # Create copies to avoid side effects.\n",
        "    contextual_df_c = contextual_df.copy()\n",
        "    benchmark_df_c = benchmark_df.copy()\n",
        "    micro_df_c = micro_df.copy()\n",
        "\n",
        "    # --- Normalize Nested Dictionaries in contextual_df ---\n",
        "    # Get the HICPX availability start round from the config.\n",
        "    hicpx_start_round = config['phase_2_parameters']['oos_and_availability']['hicpx_available_from_round']\n",
        "\n",
        "    # Define a function to apply to each row for normalization.\n",
        "    def normalize_row_dicts(row):\n",
        "        # Standardize 'latest_realized_data' for HICPX before its availability.\n",
        "        if row['survey_round'] < hicpx_start_round:\n",
        "            row['latest_realized_data']['hicpx'] = {'period': None, 'value': np.nan}\n",
        "        # Ensure keys in 'previous_spf_medians' are integers.\n",
        "        for var in row['previous_spf_medians']:\n",
        "            row['previous_spf_medians'][var] = {\n",
        "                int(k): v for k, v in row['previous_spf_medians'][var].items()\n",
        "            }\n",
        "        return row\n",
        "\n",
        "    # Apply the normalization function row-wise.\n",
        "    contextual_df_c = contextual_df_c.apply(normalize_row_dicts, axis=1)\n",
        "\n",
        "    # --- Compute 'target_year' for Human Survey DataFrames ---\n",
        "    # Create a mapping from survey round to the long-term target year.\n",
        "    lt_year_map = contextual_df_c.set_index('survey_round')['lt_year']\n",
        "\n",
        "    # Define a reusable function to compute 'target_year'.\n",
        "    def compute_target_year(df: pd.DataFrame) -> pd.DataFrame:\n",
        "        # Extract the base year from the 'round' string.\n",
        "        df['base_year'] = df['round'].str[:4].astype(int)\n",
        "        # Define the offset for standard horizons.\n",
        "        horizon_offset_map = {'CY': 0, 'CY+1': 1, 'CY+2': 2}\n",
        "        # Apply the offset to calculate the target year for standard horizons.\n",
        "        df['target_year'] = df['base_year'] + df['horizon'].map(horizon_offset_map)\n",
        "\n",
        "        # Identify rows with the 'LT' horizon.\n",
        "        lt_mask = df['horizon'] == 'LT'\n",
        "        # Use the pre-built map to fill in the target year for 'LT' horizons.\n",
        "        df.loc[lt_mask, 'target_year'] = df.loc[lt_mask, 'round'].map(lt_year_map)\n",
        "\n",
        "        # Convert the final 'target_year' to a nullable integer type and drop helper column.\n",
        "        df['target_year'] = df['target_year'].astype('Int64')\n",
        "        df = df.drop(columns=['base_year'])\n",
        "        return df\n",
        "\n",
        "    # Apply the function to both human survey DataFrames.\n",
        "    benchmark_df_c = compute_target_year(benchmark_df_c)\n",
        "    micro_df_c = compute_target_year(micro_df_c)\n",
        "\n",
        "    # Return the three cleansed DataFrames.\n",
        "    return contextual_df_c, benchmark_df_c, micro_df_c\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 4, Step 3 Helper: Clean realized_macro_data_df\n",
        "# ------------------------------------------------------------------------------\n",
        "def _cleanse_realized_df(realized_macro_data_df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Cleanses the realized_macro_data_df by deduplicating and validating\n",
        "    aggregation methods.\n",
        "\n",
        "    Args:\n",
        "        realized_macro_data_df: The raw realized outcomes DataFrame.\n",
        "\n",
        "    Returns:\n",
        "        A cleansed copy of the realized outcomes DataFrame.\n",
        "    \"\"\"\n",
        "    # Create a copy to avoid modifying the original.\n",
        "    df = realized_macro_data_df.copy()\n",
        "\n",
        "    # --- Deduplication ---\n",
        "    # Check for and count duplicates based on the composite key.\n",
        "    duplicates = df.duplicated(subset=['reference_year', 'variable'])\n",
        "    if duplicates.any():\n",
        "        logger.warning(f\"Found and dropped {duplicates.sum()} duplicate rows in realized_macro_data_df.\")\n",
        "        # Drop duplicates, keeping the first occurrence.\n",
        "        df = df.drop_duplicates(subset=['reference_year', 'variable'], keep='first')\n",
        "\n",
        "    # --- Aggregation Method Validation ---\n",
        "    # Define the canonical aggregation methods required by the study.\n",
        "    EXPECTED_AGGREGATION_METHODS = {\n",
        "        'HICP': 'simple average of 12 monthly YoY rates',\n",
        "        'HICPX': 'simple average of 12 monthly YoY rates',\n",
        "        'RGDP': 'annual growth percent',\n",
        "        'UNR': 'simple average of 12 monthly rates'\n",
        "    }\n",
        "    # Create a mapping from variable to its expected method.\n",
        "    expected_method_series = df['variable'].map(EXPECTED_AGGREGATION_METHODS)\n",
        "\n",
        "    # Normalize the actual method strings for robust comparison.\n",
        "    actual_method_series = df['aggregation_method'].str.strip().str.lower()\n",
        "    # Check for mismatches.\n",
        "    mismatches = actual_method_series != expected_method_series.str.lower()\n",
        "    if mismatches.any():\n",
        "        mismatch_details = df[mismatches][['reference_year', 'variable', 'aggregation_method']]\n",
        "        raise ValueError(f\"Found mismatched aggregation methods:\\n{mismatch_details}\")\n",
        "\n",
        "    # Return the cleansed DataFrame.\n",
        "    return df\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 4, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "def cleanse_analytical_inputs(\n",
        "    contextual_data_df: pd.DataFrame,\n",
        "    human_survey_predictions_benchmark_df: pd.DataFrame,\n",
        "    human_survey_micro_df: pd.DataFrame,\n",
        "    realized_macro_data_df: pd.DataFrame,\n",
        "    config: Dict[str, Any]\n",
        ") -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Orchestrates the cleansing of all analytical input DataFrames.\n",
        "\n",
        "    This function applies a series of cleansing and transformation steps to\n",
        "    prepare the core analytical datasets for the forecasting pipeline. It ensures\n",
        "    data types are correct, text and dates are normalized, nested structures\n",
        "    are standardized, and critical relational keys like 'target_year' are\n",
        "    computed consistently.\n",
        "\n",
        "    Args:\n",
        "        contextual_data_df: The raw (but validated) contextual data DataFrame.\n",
        "        human_survey_predictions_benchmark_df: The raw benchmark survey DataFrame.\n",
        "        human_survey_micro_df: The raw micro survey DataFrame.\n",
        "        realized_macro_data_df: The raw realized outcomes DataFrame.\n",
        "        config: The study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing the four cleansed and prepared DataFrames:\n",
        "        (contextual_df, benchmark_df, micro_df, realized_df).\n",
        "    \"\"\"\n",
        "    # Log the start of the analytical data cleansing process.\n",
        "    logger.info(\"Starting cleansing of analytical input DataFrames...\")\n",
        "\n",
        "    # --- Step 1: Clean contextual_data_df ---\n",
        "    # This step is performed first as its outputs are needed by Step 2.\n",
        "    logger.info(\"Cleansing 'contextual_data_df'...\")\n",
        "    cleansed_contextual_df = _cleanse_contextual_df(contextual_data_df, config)\n",
        "\n",
        "    # --- Step 2: Normalize dicts and compute target_year ---\n",
        "    # This step uses the cleansed contextual data to process the human survey data.\n",
        "    logger.info(\"Normalizing nested data and computing 'target_year' for human surveys...\")\n",
        "    cleansed_contextual_df, cleansed_benchmark_df, cleansed_micro_df = \\\n",
        "        _normalize_nested_dicts_and_compute_target_year(\n",
        "            cleansed_contextual_df,\n",
        "            human_survey_predictions_benchmark_df,\n",
        "            human_survey_micro_df,\n",
        "            config\n",
        "        )\n",
        "\n",
        "    # --- Step 3: Clean realized_macro_data_df ---\n",
        "    # This step is independent of the others.\n",
        "    logger.info(\"Cleansing 'realized_macro_data_df'...\")\n",
        "    cleansed_realized_df = _cleanse_realized_df(realized_macro_data_df)\n",
        "\n",
        "    # Log the successful completion of the process.\n",
        "    logger.info(\"Cleansing of all analytical inputs completed successfully.\")\n",
        "\n",
        "    # Return the tuple of cleansed DataFrames.\n",
        "    return (\n",
        "        cleansed_contextual_df,\n",
        "        cleansed_benchmark_df,\n",
        "        cleansed_micro_df,\n",
        "        cleansed_realized_df\n",
        "    )\n"
      ],
      "metadata": {
        "id": "QbT0Yd2G3oD6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 5 — Keyword and domain pre-screen (persona filter step 1)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 5: Keyword and domain pre-screen (persona filter step 1)\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 5, Step 1 Helper: Count lexicon hits in descriptions\n",
        "# ------------------------------------------------------------------------------\n",
        "def _compute_keyword_filter_flags(\n",
        "    description_series: pd.Series,\n",
        "    lexicon: List[str],\n",
        "    min_token_count: int\n",
        ") -> Tuple[pd.Series, pd.Series]:\n",
        "    \"\"\"\n",
        "    Computes keyword hit counts and filter flags for a series of descriptions.\n",
        "\n",
        "    This function uses a single compiled regex to efficiently find all occurrences\n",
        "    of lexicon phrases within the descriptions. It counts the number of *unique*\n",
        "    phrases found in each description and creates a boolean flag indicating\n",
        "    if the count meets the minimum threshold.\n",
        "\n",
        "    Args:\n",
        "        description_series: A pandas Series of cleansed persona descriptions.\n",
        "        lexicon: A list of keyword phrases to search for.\n",
        "        min_token_count: The minimum number of unique lexicon phrases required to pass.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "        - A pandas Series of integers (keyword_hit_count).\n",
        "        - A pandas Series of booleans (keyword_pass).\n",
        "    \"\"\"\n",
        "    # Input validation: Ensure lexicon is not empty.\n",
        "    if not lexicon:\n",
        "        raise ValueError(\"Lexicon cannot be empty for keyword filtering.\")\n",
        "\n",
        "    # Prepare the descriptions for matching by converting to lowercase.\n",
        "    descriptions_lower = description_series.str.lower()\n",
        "\n",
        "    # Escape special regex characters in lexicon terms and join them with the OR operator '|'.\n",
        "    # Use word boundaries (\\b) to ensure whole-phrase matching.\n",
        "    # This creates a single, highly efficient regex pattern.\n",
        "    regex_pattern = r'\\b(' + '|'.join(re.escape(term) for term in lexicon) + r')\\b'\n",
        "    # Compile the regex for performance.\n",
        "    compiled_regex = re.compile(regex_pattern)\n",
        "\n",
        "    # Apply the regex to find all matches in each description.\n",
        "    # The result is a Series of lists, where each list contains all phrases found.\n",
        "    matches = descriptions_lower.str.findall(compiled_regex)\n",
        "\n",
        "    # For each list of matches, count the number of unique phrases.\n",
        "    # Equation: c_i = |{unique phrases found in description_i}|\n",
        "    keyword_hit_count = matches.apply(lambda found_list: len(set(found_list)))\n",
        "\n",
        "    # Create a boolean flag based on the minimum token count threshold.\n",
        "    # Equation: I_i^kw = 1{c_i >= min_token_count}\n",
        "    keyword_pass = keyword_hit_count >= min_token_count\n",
        "\n",
        "    # Return the hit counts and the pass flags.\n",
        "    return keyword_hit_count, keyword_pass\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 5, Step 2 Helper: Apply domain_tags filter\n",
        "# ------------------------------------------------------------------------------\n",
        "def _compute_domain_filter_flags(\n",
        "    domain_tags_series: pd.Series,\n",
        "    allowed_domains: Set[str]\n",
        ") -> pd.Series:\n",
        "    \"\"\"\n",
        "    Computes domain filter flags based on set intersection.\n",
        "\n",
        "    Args:\n",
        "        domain_tags_series: A pandas Series where each element is a list of\n",
        "                            domain tags or None.\n",
        "        allowed_domains: A set of allowed domain strings.\n",
        "\n",
        "    Returns:\n",
        "        A pandas Series of booleans (domain_pass).\n",
        "    \"\"\"\n",
        "    # Define a helper function to apply to each row.\n",
        "    def check_intersection(tags: Optional[List[str]]) -> bool:\n",
        "        # If the tags are not a list (e.g., None), it fails the check.\n",
        "        if not isinstance(tags, list):\n",
        "            return False\n",
        "        # Check if the intersection between the persona's tags and allowed domains is non-empty.\n",
        "        # Equation: I_i^dom = 1{domain_tags_i ∩ Dom_allow ≠ ∅}\n",
        "        return not allowed_domains.isdisjoint(tags)\n",
        "\n",
        "    # Apply the function to the Series to generate the boolean flags.\n",
        "    domain_pass = domain_tags_series.apply(check_intersection)\n",
        "\n",
        "    # Return the pass flags.\n",
        "    return domain_pass\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 5, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "def apply_keyword_domain_filter(\n",
        "    cleansed_df_path: str,\n",
        "    filtered_df_path: str,\n",
        "    config: Dict[str, Any],\n",
        "    chunksize: int = 1_000_000\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Applies keyword and domain filters to the cleansed persona dataset.\n",
        "\n",
        "    This function reads the cleansed persona data in chunks, applies two parallel\n",
        "    filters, and writes the records that pass both filters to a new Parquet file.\n",
        "    1. Keyword Filter: Checks if a description contains at least a minimum number\n",
        "       of unique phrases from a predefined lexicon.\n",
        "    2. Domain Filter: Checks if the persona's domain tags intersect with a set\n",
        "       of allowed domains.\n",
        "\n",
        "    Args:\n",
        "        cleansed_df_path: Path to the cleansed input Parquet file.\n",
        "        filtered_df_path: Path to save the filtered output Parquet file.\n",
        "        config: The study configuration dictionary.\n",
        "        chunksize: The number of rows to process in each chunk.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing a report of the filtering process.\n",
        "    \"\"\"\n",
        "    # Log the start of the filtering process.\n",
        "    logger.info(f\"Starting keyword and domain filtering for '{cleansed_df_path}'.\")\n",
        "\n",
        "    # --- Configuration Extraction ---\n",
        "    # Extract relevant parameters from the config dictionary.\n",
        "    filter_config = config['phase_1_parameters']['filtering']['keyword_filtering']\n",
        "    lexicon = filter_config['lexicon']\n",
        "    min_token_count = filter_config['min_token_count']\n",
        "    # Define the set of allowed domains for the domain filter.\n",
        "    allowed_domains = frozenset({\"economics\", \"finance\", \"banking\", \"monetary\", \"central bank\"})\n",
        "\n",
        "    # --- Initialization ---\n",
        "    # Initialize a report to track statistics across all chunks.\n",
        "    report = {'rows_read': 0, 'rows_written': 0, 'passed_keyword_only': 0, 'passed_domain_only': 0}\n",
        "    # Open the input Parquet file for chunked reading.\n",
        "    parquet_file = pq.ParquetFile(cleansed_df_path)\n",
        "    # Initialize the Parquet writer for streaming output.\n",
        "    writer = None\n",
        "\n",
        "    try:\n",
        "        # Iterate over the input file in batches (chunks).\n",
        "        for i, chunk in enumerate(parquet_file.iter_batches(batch_size=chunksize, use_pandas_metadata=True)):\n",
        "            # Convert the Arrow Batch to a pandas DataFrame.\n",
        "            chunk_df = chunk.to_pandas()\n",
        "            # Update the total number of rows read.\n",
        "            report['rows_read'] += len(chunk_df)\n",
        "\n",
        "            # --- Step 1: Compute Keyword Filter Flags ---\n",
        "            # Call the helper to get hit counts and pass flags for the keyword filter.\n",
        "            keyword_hit_count, keyword_pass = _compute_keyword_filter_flags(\n",
        "                chunk_df['description'], lexicon, min_token_count\n",
        "            )\n",
        "\n",
        "            # --- Step 2: Compute Domain Filter Flags ---\n",
        "            # Call the helper to get pass flags for the domain filter.\n",
        "            domain_pass = _compute_domain_filter_flags(chunk_df['domain_tags'], allowed_domains)\n",
        "\n",
        "            # --- Step 3: Combine Filters and Reduce Dataset ---\n",
        "            # Combine the two boolean masks using a logical AND.\n",
        "            # Equation: pass_{1,i} = I_i^kw · I_i^dom\n",
        "            final_pass_mask = keyword_pass & domain_pass\n",
        "\n",
        "            # Create the filtered DataFrame containing only rows that passed both checks.\n",
        "            filtered_chunk = chunk_df[final_pass_mask].copy()\n",
        "\n",
        "            # Add new audit columns to the filtered chunk for provenance.\n",
        "            filtered_chunk['keyword_hit_count'] = keyword_hit_count[final_pass_mask]\n",
        "\n",
        "            # Update report statistics.\n",
        "            report['passed_keyword_only'] += keyword_pass.sum()\n",
        "            report['passed_domain_only'] += domain_pass.sum()\n",
        "            report['rows_written'] += len(filtered_chunk)\n",
        "\n",
        "            # If there are no rows that passed, skip to the next chunk.\n",
        "            if filtered_chunk.empty:\n",
        "                logger.info(f\"Processed chunk {i+1}: No rows passed the filter.\")\n",
        "                continue\n",
        "\n",
        "            # --- Streaming Write ---\n",
        "            # Convert the filtered pandas DataFrame to an Arrow Table.\n",
        "            table = pa.Table.from_pandas(filtered_chunk, preserve_index=True)\n",
        "\n",
        "            # Initialize the Parquet writer with the schema from the first non-empty table.\n",
        "            if writer is None:\n",
        "                writer = pq.ParquetWriter(filtered_df_path, table.schema)\n",
        "\n",
        "            # Write the current filtered table to the output file.\n",
        "            writer.write_table(table)\n",
        "\n",
        "            # Log progress.\n",
        "            logger.info(f\"Processed chunk {i+1}: Read {len(chunk_df)}, Wrote {len(filtered_chunk)}\")\n",
        "\n",
        "    finally:\n",
        "        # Ensure the writer is closed to finalize the file.\n",
        "        if writer:\n",
        "            writer.close()\n",
        "\n",
        "    # --- Final Report ---\n",
        "    # Calculate the final rejection rate.\n",
        "    rejection_rate = 1 - (report['rows_written'] / report['rows_read']) if report['rows_read'] > 0 else 0\n",
        "    report['rejection_rate'] = f\"{rejection_rate:.4%}\"\n",
        "\n",
        "    # Log the final summary of the filtering process.\n",
        "    logger.info(\"Keyword and domain filtering complete.\")\n",
        "    logger.info(f\"Total rows read: {report['rows_read']}\")\n",
        "    logger.info(f\"Total rows written: {report['rows_written']} ({report['rejection_rate']} rejection rate)\")\n",
        "\n",
        "    # Return the final report.\n",
        "    return report\n"
      ],
      "metadata": {
        "id": "GPGgMHzH4ZQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 6 — NER PERSON exclusion (persona filter step 2)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 6: NER PERSON exclusion (persona filter step 2)\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 6, Step 1 Helper: Load and configure NER model\n",
        "# ------------------------------------------------------------------------------\n",
        "def _load_ner_model(model_name: str) -> Language:\n",
        "    \"\"\"\n",
        "    Loads and configures the spaCy NER model for efficient processing.\n",
        "\n",
        "    This function loads the specified spaCy model and disables all unnecessary\n",
        "    pipeline components (e.g., parser, lemmatizer) to optimize for speed and\n",
        "    memory usage, as only the Named Entity Recognizer (NER) is required.\n",
        "\n",
        "    Args:\n",
        "        model_name: The name of the spaCy model to load (e.g., 'en_core_web_trf').\n",
        "\n",
        "    Returns:\n",
        "        The loaded and configured spaCy Language object.\n",
        "\n",
        "    Raises:\n",
        "        IOError: If the specified spaCy model is not found.\n",
        "        ValueError: If the model name is not provided.\n",
        "    \"\"\"\n",
        "    # Input validation: Ensure a model name is provided.\n",
        "    if not model_name:\n",
        "        raise ValueError(\"A spaCy model name must be provided.\")\n",
        "\n",
        "    # Log the model loading attempt.\n",
        "    logger.info(f\"Loading spaCy NER model: '{model_name}'...\")\n",
        "\n",
        "    try:\n",
        "        # Define components to disable for performance optimization.\n",
        "        # We only need the 'ner' component and its dependencies (e.g., 'tok2vec').\n",
        "        disable_pipes = [\"parser\", \"lemmatizer\", \"tagger\", \"attribute_ruler\"]\n",
        "        # Load the spaCy model with unnecessary components disabled.\n",
        "        nlp = spacy.load(model_name, disable=disable_pipes)\n",
        "        # Log successful loading.\n",
        "        logger.info(f\"Successfully loaded model '{model_name}'.\")\n",
        "        # Return the loaded model object.\n",
        "        return nlp\n",
        "    except OSError:\n",
        "        # If the model is not found, provide a helpful error message.\n",
        "        error_msg = (\n",
        "            f\"spaCy model '{model_name}' not found. Please download it by running:\\n\"\n",
        "            f\"python -m spacy download {model_name}\"\n",
        "        )\n",
        "        logger.error(error_msg)\n",
        "        # Raise an IOError to indicate a file/resource-related problem.\n",
        "        raise IOError(error_msg)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 6, Step 2 Helper: Apply NER and construct exclusion indicator\n",
        "# ------------------------------------------------------------------------------\n",
        "def _generate_ner_pass_flags(\n",
        "    df: pd.DataFrame,\n",
        "    nlp: Language,\n",
        "    batch_size: int = 256\n",
        ") -> pd.Series:\n",
        "    \"\"\"\n",
        "    Applies the NER model to descriptions and generates pass/fail flags.\n",
        "\n",
        "    This function processes the 'description' column in batches using spaCy's\n",
        "    efficient `nlp.pipe()` method. It creates a boolean flag for each persona,\n",
        "    marking it as `False` (fail) if any entity with the label 'PERSON' is\n",
        "    detected, and `True` (pass) otherwise.\n",
        "\n",
        "    Args:\n",
        "        df: The input DataFrame containing the 'description' column.\n",
        "        nlp: The loaded spaCy Language object.\n",
        "        batch_size: The number of documents to process in each batch.\n",
        "\n",
        "    Returns:\n",
        "        A pandas Series of booleans ('ner_pass') aligned with the input DataFrame's index.\n",
        "    \"\"\"\n",
        "    # Log the start of the NER processing.\n",
        "    logger.info(f\"Applying NER to {len(df)} descriptions to detect 'PERSON' entities...\")\n",
        "\n",
        "    # Create a boolean Series to store the results, initialized to False.\n",
        "    ner_pass = pd.Series(False, index=df.index, dtype=bool)\n",
        "\n",
        "    # Extract the descriptions and their corresponding IDs (index) for processing.\n",
        "    # Using .values is faster for iteration than accessing the Series directly.\n",
        "    texts = df['description'].values\n",
        "    ids = df.index.values\n",
        "\n",
        "    # Process the texts in batches using nlp.pipe for high efficiency.\n",
        "    # The `as_tuples=True` argument allows us to pass context (the ID) along with the text.\n",
        "    # A progress bar is added for better user experience.\n",
        "    doc_iterator = nlp.pipe(zip(texts, ids), as_tuples=True, batch_size=batch_size)\n",
        "\n",
        "    # Iterate through the processed documents and their context (IDs).\n",
        "    for doc, doc_id in tqdm(doc_iterator, total=len(df), desc=\"NER Processing\"):\n",
        "        # Assume the persona passes until a 'PERSON' entity is found.\n",
        "        passes_check = True\n",
        "        # Iterate through the entities detected in the document.\n",
        "        for ent in doc.ents:\n",
        "            # If an entity with the label 'PERSON' is found...\n",
        "            # Equation: Check if ∃ e ∈ E_i such that label(e) = 'PERSON'\n",
        "            if ent.label_ == 'PERSON':\n",
        "                # Mark the persona as failing the check and break the inner loop.\n",
        "                passes_check = False\n",
        "                break\n",
        "        # Store the final pass/fail result for this document's ID.\n",
        "        # Equation: I_i^NER = 1{no 'PERSON' entity found}\n",
        "        ner_pass.loc[doc_id] = passes_check\n",
        "\n",
        "    # Log the completion of the NER processing.\n",
        "    logger.info(\"NER processing complete.\")\n",
        "    # Return the final boolean Series of pass/fail flags.\n",
        "    return ner_pass\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 6, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "def apply_ner_person_filter(\n",
        "    persona_step1_df: pd.DataFrame,\n",
        "    config: Dict[str, Any]\n",
        ") -> Tuple[pd.DataFrame, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Applies a Named Entity Recognition (NER) filter to exclude personas\n",
        "    mentioning specific individuals.\n",
        "\n",
        "    This function orchestrates the second step of the persona filtering pipeline.\n",
        "    It loads a spaCy NER model, processes all persona descriptions to detect\n",
        "    entities labeled as 'PERSON', and filters out any personas where such\n",
        "    an entity is found.\n",
        "\n",
        "    Args:\n",
        "        persona_step1_df: The DataFrame of personas that passed the initial\n",
        "                          keyword and domain filters.\n",
        "        config: The study configuration dictionary, containing the NER model name.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "        - persona_step2_df (pd.DataFrame): The filtered DataFrame containing only\n",
        "          personas that passed the NER check.\n",
        "        - report (Dict[str, Any]): A dictionary summarizing the filtering process.\n",
        "    \"\"\"\n",
        "    # Log the start of Task 6.\n",
        "    logger.info(\"Starting Task 6: NER PERSON exclusion filter.\")\n",
        "\n",
        "    # --- Input Validation ---\n",
        "    # Ensure the input DataFrame is not empty.\n",
        "    if persona_step1_df.empty:\n",
        "        logger.warning(\"Input DataFrame for NER filtering is empty. Skipping.\")\n",
        "        report = {'rows_read': 0, 'rows_written': 0, 'rejection_rate': 'N/A'}\n",
        "        return pd.DataFrame(), report\n",
        "\n",
        "    # --- Step 1: Load and Configure NER Model ---\n",
        "    # Extract the model name from the configuration.\n",
        "    model_name = config['phase_1_parameters']['filtering']['ner_filtering']['model_name']\n",
        "    # Load the configured spaCy model using the helper function.\n",
        "    nlp_model = _load_ner_model(model_name)\n",
        "\n",
        "    # --- Step 2: Apply NER and Construct Exclusion Indicator ---\n",
        "    # Generate the pass/fail flags for each persona using the NER model.\n",
        "    ner_pass_flags = _generate_ner_pass_flags(persona_step1_df, nlp_model)\n",
        "\n",
        "    # --- Step 3: Persist NER Flags and Reduce Dataset ---\n",
        "    # Add the 'ner_pass' column to the DataFrame for audit purposes.\n",
        "    df_with_flags = persona_step1_df.copy()\n",
        "    df_with_flags['ner_pass'] = ner_pass_flags\n",
        "\n",
        "    # Filter the DataFrame to keep only the rows that passed the NER check.\n",
        "    persona_step2_df = df_with_flags[df_with_flags['ner_pass']].copy()\n",
        "\n",
        "    # Drop the temporary flag column from the final output for cleanliness.\n",
        "    persona_step2_df = persona_step2_df.drop(columns=['ner_pass'])\n",
        "\n",
        "    # --- Report Generation ---\n",
        "    # Compile a report summarizing the results of the filtering step.\n",
        "    rows_read = len(persona_step1_df)\n",
        "    rows_written = len(persona_step2_df)\n",
        "    rejection_rate = 1 - (rows_written / rows_read) if rows_read > 0 else 0\n",
        "\n",
        "    report = {\n",
        "        'rows_read': rows_read,\n",
        "        'rows_written': rows_written,\n",
        "        'rows_rejected': rows_read - rows_written,\n",
        "        'rejection_rate': f\"{rejection_rate:.4%}\"\n",
        "    }\n",
        "\n",
        "    # Log the final summary.\n",
        "    logger.info(\"NER filtering complete.\")\n",
        "    logger.info(f\"Rows read: {report['rows_read']}, Rows written: {report['rows_written']} ({report['rejection_rate']} rejection rate).\")\n",
        "\n",
        "    # Return the filtered DataFrame and the summary report.\n",
        "    return persona_step2_df, report\n"
      ],
      "metadata": {
        "id": "j-K8gbSn5n2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 7 — Embedding-based deduplication (persona filter step 3)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 7: Embedding-based deduplication (persona filter step 3)\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 7, Step 1 Helper: Compute and normalize embeddings\n",
        "# ------------------------------------------------------------------------------\n",
        "def _generate_normalized_embeddings(\n",
        "    descriptions: List[str],\n",
        "    model_name: str,\n",
        "    batch_size: int = 32\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Generates L2-normalized embeddings for a list of text descriptions.\n",
        "\n",
        "    Args:\n",
        "        descriptions: A list of strings to be encoded.\n",
        "        model_name: The name of the sentence-transformer model to use.\n",
        "        batch_size: The batch size for the encoding process.\n",
        "\n",
        "    Returns:\n",
        "        A numpy array of shape (n_descriptions, embedding_dim) containing\n",
        "        the L2-normalized embeddings.\n",
        "    \"\"\"\n",
        "    # Log the start of the embedding generation process.\n",
        "    logger.info(f\"Loading sentence-transformer model: '{model_name}'...\")\n",
        "    # Load the specified pre-trained sentence-transformer model.\n",
        "    model = SentenceTransformer(model_name)\n",
        "\n",
        "    # Log the start of the encoding process.\n",
        "    logger.info(f\"Generating embeddings for {len(descriptions)} descriptions...\")\n",
        "    # Generate embeddings for all descriptions in batches. A progress bar is shown.\n",
        "    embeddings = model.encode(\n",
        "        descriptions,\n",
        "        batch_size=batch_size,\n",
        "        show_progress_bar=True,\n",
        "        normalize_embeddings=True # Use built-in normalization for efficiency\n",
        "    )\n",
        "\n",
        "    # The model.encode with normalize_embeddings=True already performs L2 normalization.\n",
        "    # Equation: e_i <- e_i / ||e_i||_2\n",
        "    # The output embeddings are already unit vectors.\n",
        "\n",
        "    # Convert to float32 to save memory, which is sufficient for this task.\n",
        "    embeddings = embeddings.astype(np.float32)\n",
        "    logger.info(f\"Generated and normalized embeddings of shape {embeddings.shape}.\")\n",
        "\n",
        "    # Return the final matrix of normalized embeddings.\n",
        "    return embeddings\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 7, Step 2 Helper: Find similar pairs using HNSW\n",
        "# ------------------------------------------------------------------------------\n",
        "def _find_similar_pairs_hnsw(\n",
        "    embeddings: np.ndarray,\n",
        "    threshold: float,\n",
        "    ef_construction: int = 200,\n",
        "    M: int = 16,\n",
        "    ef_search: int = 50\n",
        ") -> List[Tuple[int, int]]:\n",
        "    \"\"\"\n",
        "    Finds pairs of embeddings with cosine similarity above a threshold using HNSW.\n",
        "\n",
        "    Args:\n",
        "        embeddings: A numpy array of L2-normalized embeddings.\n",
        "        threshold: The cosine similarity threshold (e.g., 0.90).\n",
        "        ef_construction: HNSW build-time parameter.\n",
        "        M: HNSW build-time parameter.\n",
        "        ef_search: HNSW search-time parameter.\n",
        "\n",
        "    Returns:\n",
        "        A list of tuples, where each tuple contains the integer indices of a\n",
        "        similar pair.\n",
        "    \"\"\"\n",
        "    # Get the number of items and the embedding dimension.\n",
        "    num_items, dim = embeddings.shape\n",
        "    # Log the start of the index building process.\n",
        "    logger.info(f\"Building HNSW index for {num_items} items...\")\n",
        "\n",
        "    # Initialize the HNSW index.\n",
        "    # 'ip' (inner product) is equivalent to cosine similarity for L2-normalized vectors.\n",
        "    p = hnswlib.Index(space='ip', dim=dim)\n",
        "    # Initialize the index structure.\n",
        "    p.init_index(max_elements=num_items, ef_construction=ef_construction, M=M)\n",
        "    # Add the embeddings to the index.\n",
        "    p.add_items(embeddings, np.arange(num_items))\n",
        "    # Set the search-time effort parameter. Higher is more accurate but slower.\n",
        "    p.set_ef(ef_search)\n",
        "\n",
        "    # Log the start of the similarity search.\n",
        "    logger.info(f\"Querying index to find pairs with similarity >= {threshold}...\")\n",
        "    # Initialize a set to store the found pairs to avoid duplicates.\n",
        "    similar_pairs = set()\n",
        "\n",
        "    # Query the index for each item to find its neighbors.\n",
        "    for i in tqdm(range(num_items), desc=\"Finding Similar Pairs\"):\n",
        "        # Query for the k nearest neighbors. We set k to a reasonable number.\n",
        "        # If a cluster is larger than k, we might miss some pairs, so k should be generous.\n",
        "        k = 50\n",
        "        labels, distances = p.knn_query(embeddings[i], k=k)\n",
        "\n",
        "        # The 'distances' from hnswlib with 'ip' space are 1 - cosine_similarity.\n",
        "        # So, cosine_similarity = 1 - distance.\n",
        "        # We want pairs where cosine_similarity >= threshold.\n",
        "        # This is equivalent to 1 - distance >= threshold, or distance <= 1 - threshold.\n",
        "        for j_label, dist in zip(labels[0], distances[0]):\n",
        "            # The query will always find the item itself (i) with distance 0. Skip it.\n",
        "            if i == j_label:\n",
        "                continue\n",
        "            # If the similarity meets the threshold...\n",
        "            if (1 - dist) >= threshold:\n",
        "                # Add the pair to the set, ensuring order (min, max) to store each pair only once.\n",
        "                pair = tuple(sorted((i, j_label)))\n",
        "                similar_pairs.add(pair)\n",
        "\n",
        "    # Log the number of unique similar pairs found.\n",
        "    logger.info(f\"Found {len(similar_pairs)} unique similar pairs.\")\n",
        "    # Return the pairs as a list.\n",
        "    return list(similar_pairs)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 7, Step 3 Helper: Cluster pairs and select representatives\n",
        "# ------------------------------------------------------------------------------\n",
        "def _get_deduplication_mask(\n",
        "    num_items: int,\n",
        "    item_ids: pd.Index,\n",
        "    similar_pairs: List[Tuple[int, int]]\n",
        ") -> pd.Series:\n",
        "    \"\"\"\n",
        "    Identifies duplicate clusters from similar pairs and selects a representative for each.\n",
        "\n",
        "    Args:\n",
        "        num_items: The total number of items (personas).\n",
        "        item_ids: The pandas Index of persona IDs, corresponding to the item indices.\n",
        "        similar_pairs: A list of tuples representing similar pairs by integer index.\n",
        "\n",
        "    Returns:\n",
        "        A pandas Series of booleans (keep_mask) aligned with item_ids, where\n",
        "        `True` indicates the item is a representative to be kept.\n",
        "    \"\"\"\n",
        "    # Log the start of the clustering process.\n",
        "    logger.info(\"Building graph and finding duplicate clusters...\")\n",
        "    # Create a graph from the list of similar pairs (edges).\n",
        "    G = nx.Graph()\n",
        "    G.add_edges_from(similar_pairs)\n",
        "\n",
        "    # Find all connected components in the graph. Each component is a duplicate cluster.\n",
        "    clusters = list(nx.connected_components(G))\n",
        "\n",
        "    # Initialize a set to store the integer indices of the representatives to keep.\n",
        "    representatives_to_keep = set()\n",
        "\n",
        "    # Process clusters of size > 1 (i.e., actual duplicates).\n",
        "    for cluster in clusters:\n",
        "        if len(cluster) > 1:\n",
        "            # Map the integer indices in the cluster back to their actual persona IDs.\n",
        "            cluster_ids = [item_ids[i] for i in cluster]\n",
        "            # Select the representative as the one with the lexicographically smallest ID.\n",
        "            representative_id = min(cluster_ids)\n",
        "            # Find the integer index corresponding to the chosen representative ID.\n",
        "            representative_index = item_ids.get_loc(representative_id)\n",
        "            # Add this index to the set of items to keep.\n",
        "            representatives_to_keep.add(representative_index)\n",
        "\n",
        "    # Process singletons (items not in any similar pair). They are unique and kept by default.\n",
        "    all_nodes_in_clusters = set.union(*clusters) if clusters else set()\n",
        "    singletons = set(range(num_items)) - all_nodes_in_clusters\n",
        "    representatives_to_keep.update(singletons)\n",
        "\n",
        "    # Create the final boolean mask.\n",
        "    logger.info(f\"Identified {len(representatives_to_keep)} unique representatives.\")\n",
        "    keep_mask = pd.Series(False, index=item_ids)\n",
        "    # Get the IDs of the representatives.\n",
        "    representative_ids = item_ids[list(representatives_to_keep)]\n",
        "    # Set the mask to True for these IDs.\n",
        "    keep_mask.loc[representative_ids] = True\n",
        "\n",
        "    # Return the final mask.\n",
        "    return keep_mask\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 7, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "def apply_embedding_deduplication(\n",
        "    persona_step2_df: pd.DataFrame,\n",
        "    config: Dict[str, Any]\n",
        ") -> Tuple[pd.DataFrame, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Applies embedding-based semantic deduplication to the persona dataset.\n",
        "\n",
        "    This function orchestrates the third step of the filtering pipeline:\n",
        "    1.  Generates dense vector embeddings for each persona's description.\n",
        "    2.  Uses an efficient Approximate Nearest Neighbor (ANN) search to find\n",
        "        pairs of personas with high semantic similarity.\n",
        "    3.  Clusters these pairs into groups of duplicates and selects a single,\n",
        "        deterministic representative from each group.\n",
        "\n",
        "    Args:\n",
        "        persona_step2_df: The DataFrame of personas that passed the NER filter.\n",
        "        config: The study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "        - persona_step3_df (pd.DataFrame): The deduplicated DataFrame.\n",
        "        - report (Dict[str, Any]): A dictionary summarizing the process.\n",
        "    \"\"\"\n",
        "    # Log the start of Task 7.\n",
        "    logger.info(\"Starting Task 7: Embedding-based deduplication.\")\n",
        "\n",
        "    # --- Input Validation ---\n",
        "    if persona_step2_df.empty:\n",
        "        logger.warning(\"Input DataFrame for deduplication is empty. Skipping.\")\n",
        "        report = {'rows_read': 0, 'rows_written': 0, 'rejection_rate': 'N/A'}\n",
        "        return pd.DataFrame(), report\n",
        "\n",
        "    # --- Configuration Extraction ---\n",
        "    dedup_config = config['phase_1_parameters']['filtering']['deduplication']\n",
        "    model_name = dedup_config['embedding_model_name']\n",
        "    threshold = dedup_config['cosine_similarity_threshold']\n",
        "\n",
        "    # --- Step 1: Compute Embeddings ---\n",
        "    # The descriptions must be in a list for the encoder.\n",
        "    descriptions = persona_step2_df['description'].tolist()\n",
        "    embeddings = _generate_normalized_embeddings(descriptions, model_name)\n",
        "\n",
        "    # --- Step 2: Find Similar Pairs ---\n",
        "    # Use the HNSW helper to find all pairs of indices with similarity >= threshold.\n",
        "    similar_pairs = _find_similar_pairs_hnsw(embeddings, threshold)\n",
        "\n",
        "    # --- Step 3: Cluster and Select Representatives ---\n",
        "    # Get the boolean mask indicating which personas to keep.\n",
        "    keep_mask = _get_deduplication_mask(\n",
        "        num_items=len(persona_step2_df),\n",
        "        item_ids=persona_step2_df.index,\n",
        "        similar_pairs=similar_pairs\n",
        "    )\n",
        "\n",
        "    # Apply the mask to filter the DataFrame.\n",
        "    persona_step3_df = persona_step2_df[keep_mask].copy()\n",
        "\n",
        "    # --- Report Generation ---\n",
        "    rows_read = len(persona_step2_df)\n",
        "    rows_written = len(persona_step3_df)\n",
        "    rejection_rate = 1 - (rows_written / rows_read) if rows_read > 0 else 0\n",
        "\n",
        "    report = {\n",
        "        'rows_read': rows_read,\n",
        "        'rows_written': rows_written,\n",
        "        'rows_rejected': rows_read - rows_written,\n",
        "        'rejection_rate': f\"{rejection_rate:.4%}\",\n",
        "        'num_similar_pairs_found': len(similar_pairs),\n",
        "    }\n",
        "\n",
        "    # Log the final summary.\n",
        "    logger.info(\"Embedding-based deduplication complete.\")\n",
        "    logger.info(f\"Rows read: {report['rows_read']}, Rows written: {report['rows_written']} ({report['rejection_rate']} rejection rate).\")\n",
        "\n",
        "    # Return the deduplicated DataFrame and the summary report.\n",
        "    return persona_step3_df, report\n"
      ],
      "metadata": {
        "id": "VTMXp4hc6fmD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 8 — LLM-as-judge triage with majority vote (persona filter step 4)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 8: LLM-as-judge triage with majority vote (persona filter step 4)\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 8, Step 1 & 2 Helper: Asynchronous API call and parsing\n",
        "# ------------------------------------------------------------------------------\n",
        "async def _fetch_and_parse_judgment(\n",
        "    client: AsyncOpenAI,\n",
        "    persona_id: str,\n",
        "    description: str,\n",
        "    run_number: int,\n",
        "    system_prompt: str,\n",
        "    model_name: str,\n",
        "    temperature: float,\n",
        "    max_retries: int,\n",
        "    semaphore: asyncio.Semaphore\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Asynchronously calls the LLM API to get a judgment for a single persona\n",
        "    and run, with retry logic and robust parsing.\n",
        "\n",
        "    Args:\n",
        "        client: The asynchronous OpenAI API client.\n",
        "        persona_id: The unique identifier for the persona.\n",
        "        description: The persona description text.\n",
        "        run_number: The judgment run number (1, 2, or 3).\n",
        "        system_prompt: The verbatim system prompt for the judge.\n",
        "        model_name: The name of the LLM to use (e.g., 'gpt-4o-mini').\n",
        "        temperature: The sampling temperature for the LLM.\n",
        "        max_retries: The maximum number of retries on API failure.\n",
        "        semaphore: An asyncio.Semaphore to control concurrency.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing the parsed judgment results for the run.\n",
        "    \"\"\"\n",
        "    # Acquire the semaphore to limit concurrent requests.\n",
        "    async with semaphore:\n",
        "        # Loop for the specified number of retries.\n",
        "        for attempt in range(max_retries + 1):\n",
        "            try:\n",
        "                # Make the asynchronous API call.\n",
        "                response = await client.chat.completions.create(\n",
        "                    model=model_name,\n",
        "                    messages=[\n",
        "                        {\"role\": \"system\", \"content\": system_prompt},\n",
        "                        {\"role\": \"user\", \"content\": description},\n",
        "                    ],\n",
        "                    temperature=temperature,\n",
        "                    response_format={\"type\": \"json_object\"},\n",
        "                )\n",
        "                # Extract the JSON content from the response.\n",
        "                raw_json = response.choices[0].message.content\n",
        "                # Parse the JSON string into a Python dictionary.\n",
        "                parsed_data = json.loads(raw_json)\n",
        "\n",
        "                # --- Schema Validation ---\n",
        "                # Define the expected keys for the JSON response.\n",
        "                expected_keys = {\"euro_area_centrality\", \"monetary_policy_depth\", \"neutrality\", \"notes\"}\n",
        "                # Check if all expected keys are present.\n",
        "                if not expected_keys.issubset(parsed_data.keys()):\n",
        "                    raise ValueError(\"Response JSON is missing required keys.\")\n",
        "\n",
        "                # Extract pass/fail status for each criterion.\n",
        "                # Convert \"pass\" to True, and anything else (e.g., \"fail\") to False.\n",
        "                eu_pass = parsed_data.get(\"euro_area_centrality\", \"fail\").lower() == \"pass\"\n",
        "                mp_pass = parsed_data.get(\"monetary_policy_depth\", \"fail\").lower() == \"pass\"\n",
        "                neu_pass = parsed_data.get(\"neutrality\", \"fail\").lower() == \"pass\"\n",
        "\n",
        "                # Return a structured dictionary with the results.\n",
        "                return {\n",
        "                    \"persona_id\": persona_id,\n",
        "                    \"run_number\": run_number,\n",
        "                    \"status\": \"SUCCESS\",\n",
        "                    \"eu_centrality_pass\": eu_pass,\n",
        "                    \"monetary_policy_depth_pass\": mp_pass,\n",
        "                    \"neutrality_pass\": neu_pass,\n",
        "                    \"raw_response\": raw_json,\n",
        "                }\n",
        "\n",
        "            except (APIError, RateLimitError, json.JSONDecodeError, ValueError) as e:\n",
        "                # Log the error with details about the attempt.\n",
        "                logger.warning(f\"Run {run_number} for persona {persona_id} failed on attempt {attempt + 1}: {e}\")\n",
        "                # If this was the last attempt, break the loop to return a failure.\n",
        "                if attempt >= max_retries:\n",
        "                    break\n",
        "                # Wait before retrying.\n",
        "                await asyncio.sleep(2.0) # Simple backoff\n",
        "\n",
        "    # If all attempts fail, return a failure record.\n",
        "    return {\n",
        "        \"persona_id\": persona_id,\n",
        "        \"run_number\": run_number,\n",
        "        \"status\": \"FAILURE\",\n",
        "        \"eu_centrality_pass\": False,\n",
        "        \"monetary_policy_depth_pass\": False,\n",
        "        \"neutrality_pass\": False,\n",
        "        \"raw_response\": None,\n",
        "    }\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 8, Step 3 Helper: Apply majority vote and filter\n",
        "# ------------------------------------------------------------------------------\n",
        "def _apply_majority_vote_and_filter(\n",
        "    persona_df: pd.DataFrame,\n",
        "    judgments_df: pd.DataFrame\n",
        ") -> Tuple[pd.DataFrame, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Applies the majority vote rule to the judgments and filters the persona DataFrame.\n",
        "\n",
        "    Args:\n",
        "        persona_df: The DataFrame of candidate personas.\n",
        "        judgments_df: A DataFrame containing the results of all judgment runs.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "        - The final, filtered DataFrame of personas (P*).\n",
        "        - A dictionary with statistics from the voting process.\n",
        "    \"\"\"\n",
        "    # Log the start of the majority vote process.\n",
        "    logger.info(\"Applying majority vote rule to judgment results...\")\n",
        "\n",
        "    # Group the judgments by persona_id to aggregate the votes.\n",
        "    # For each criterion, sum the boolean 'pass' flags (True=1, False=0).\n",
        "    # Equation: vote_c(i) = Σ_{r=1 to 3} 1{run r returned pass for c}\n",
        "    vote_counts = judgments_df.groupby(\"persona_id\")[\n",
        "        [\"eu_centrality_pass\", \"monetary_policy_depth_pass\", \"neutrality_pass\"]\n",
        "    ].sum()\n",
        "\n",
        "    # Apply the majority vote rule: a criterion passes if it received 2 or more 'pass' votes.\n",
        "    # Equation: pass_c(i) = 1{vote_c(i) >= 2}\n",
        "    vote_counts[\"eu_pass_maj\"] = vote_counts[\"eu_centrality_pass\"] >= 2\n",
        "    vote_counts[\"mp_pass_maj\"] = vote_counts[\"monetary_policy_depth_pass\"] >= 2\n",
        "    vote_counts[\"neu_pass_maj\"] = vote_counts[\"neutrality_pass\"] >= 2\n",
        "\n",
        "    # Determine the final decision: a persona is kept only if it passes all three criteria.\n",
        "    # Equation: keep_i = pass_EU(i) * pass_MP(i) * pass_NEU(i)\n",
        "    vote_counts[\"final_pass\"] = (\n",
        "        vote_counts[\"eu_pass_maj\"] &\n",
        "        vote_counts[\"mp_pass_maj\"] &\n",
        "        vote_counts[\"neu_pass_maj\"]\n",
        "    )\n",
        "\n",
        "    # Get the list of persona IDs that passed the final check.\n",
        "    passed_ids = vote_counts[vote_counts[\"final_pass\"]].index\n",
        "\n",
        "    # Filter the original persona DataFrame to keep only the passing personas.\n",
        "    final_persona_df = persona_df.loc[passed_ids].copy()\n",
        "\n",
        "    # --- Generate Statistics ---\n",
        "    # Calculate pass rates for each criterion and the final pass rate.\n",
        "    stats = {\n",
        "        \"pass_rate_eu_centrality\": vote_counts[\"eu_pass_maj\"].mean(),\n",
        "        \"pass_rate_monetary_policy\": vote_counts[\"mp_pass_maj\"].mean(),\n",
        "        \"pass_rate_neutrality\": vote_counts[\"neu_pass_maj\"].mean(),\n",
        "        \"final_pass_rate\": vote_counts[\"final_pass\"].mean(),\n",
        "        \"final_count\": len(final_persona_df),\n",
        "    }\n",
        "\n",
        "    # Return the final DataFrame and the statistics.\n",
        "    return final_persona_df, stats\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 8, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "async def apply_llm_judge_filter(\n",
        "    persona_step3_df: pd.DataFrame,\n",
        "    config: Dict[str, Any],\n",
        "    checkpoint_path: str\n",
        ") -> Tuple[pd.DataFrame, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Orchestrates the LLM-as-judge filtering pipeline.\n",
        "\n",
        "    This function manages the entire process of using an LLM to evaluate personas:\n",
        "    1.  Sets up an asynchronous API client and concurrency controls.\n",
        "    2.  Executes three independent judgment calls for each persona.\n",
        "    3.  Handles API errors, retries, and resumability via checkpointing.\n",
        "    4.  Applies a majority vote rule to the collected judgments.\n",
        "    5.  Filters the dataset to produce the final set of personas (P*).\n",
        "\n",
        "    Args:\n",
        "        persona_step3_df: The deduplicated DataFrame of candidate personas.\n",
        "        config: The study configuration dictionary.\n",
        "        checkpoint_path: Path to a file for saving/resuming raw judgment results.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "        - persona_final_df (pd.DataFrame): The final, filtered DataFrame.\n",
        "        - report (Dict[str, Any]): A dictionary summarizing the process.\n",
        "    \"\"\"\n",
        "    # Log the start of the task.\n",
        "    logger.info(\"Starting Task 8: LLM-as-judge triage.\")\n",
        "\n",
        "    # --- Configuration Extraction ---\n",
        "    judge_config = config['phase_1_parameters']['llm_as_judge']\n",
        "    api_config = judge_config['api_settings']\n",
        "    model_name = judge_config['model_name']\n",
        "    num_runs = judge_config['judging_runs']\n",
        "\n",
        "    # --- System Prompt ---\n",
        "    # This must be the exact prompt from the paper's Appendix A.\n",
        "    system_prompt = \"\"\"You are assessing expert personas for their suitability in euro-area monetary-policy research. Return one JSON object only-no additional text.\n",
        "                    TASK\n",
        "                    - Read the biography supplied by the user.\n",
        "                    - Evaluate it against the three pass-fail criteria below.\n",
        "                    - Provide a concise one-sentence reason for each decision.\n",
        "                    CRITERIA\n",
        "                    1. Euro-area centrality\n",
        "                      Fail: Focus is non-EU or purely global with no euro-area anchor.\n",
        "                      Pass: The euro area or an ECB institution is mentioned - this includes references to EU countries, central banks or Europe in general, - references to other contexts are allowed as long as the euro-area context is mentioned.\n",
        "                    2. Monetary-policy depth\n",
        "                      Fail: Monetary policy is not mentioned at all, or only mentioned in passing with none of the above signals present.\n",
        "                      Pass: The biography engages substantively with monetary policy by satisfying at least one of:\n",
        "                            - names an operational tool (e.g., deposit rate, APP/PEPP, LSAP),\n",
        "                            - discusses a recognised policy rule or doctrine (e.g., Taylor rule, money - growth targeting, rules vs. discretion),\n",
        "                            - analyses a transmission channel or macro outcome (inflation, output, employment, exchange rate, asset prices),\n",
        "                            - references an empirical method used to evaluate policy (event study, VAR, DSGE, natural experiment).\n",
        "                    3. Neutrality\n",
        "                      Fail: The biography expresses opinion, advocacy or bias. Look for:\n",
        "                            - Emotive or value-laden terms (\"reckless\", \"dangerous\", \"unsustainable\").\n",
        "                            - Framing of personal advocacy or judgment (\"skeptical of...\", \"a critic of...\", \"optimistic about...\").\n",
        "                            - Any implicit stance that goes beyond analysis.\n",
        "                      Pass: Tone is descriptive, analytical, or exploratory, without any judgment, prescription, or stance.\n",
        "                    OUTPUT SCHEMA\n",
        "                    {\n",
        "                      \"euro_area_centrality\": \"pass\" | \"fail\",\n",
        "                      \"monetary_policy_depth\": \"pass\" | \"fail\",\n",
        "                      \"neutrality\": \"pass\" | \"fail\",\n",
        "                      \"notes\": {\n",
        "                        \"euro_area_centrality\": \"<one-sentence reason>\",\n",
        "                        \"monetary_policy_depth\": \"<one-sentence reason>\",\n",
        "                        \"neutrality\": \"<one-sentence reason>\"\n",
        "                      }\n",
        "                    }\"\"\"\n",
        "\n",
        "    # --- Resumability and Task Preparation ---\n",
        "    # Load existing results from the checkpoint file if it exists.\n",
        "    try:\n",
        "        with open(checkpoint_path, 'r') as f:\n",
        "            completed_results = [json.loads(line) for line in f]\n",
        "        completed_tasks = {(res['persona_id'], res['run_number']) for res in completed_results}\n",
        "        logger.info(f\"Resuming from checkpoint. Found {len(completed_results)} completed judgments.\")\n",
        "    except FileNotFoundError:\n",
        "        completed_results = []\n",
        "        completed_tasks = set()\n",
        "        logger.info(\"No checkpoint file found. Starting from scratch.\")\n",
        "\n",
        "    # --- Create Asynchronous Tasks ---\n",
        "    # Initialize the async OpenAI client.\n",
        "    client = AsyncOpenAI()\n",
        "    # Create a semaphore to limit concurrency.\n",
        "    semaphore = asyncio.Semaphore(config['phase_1_parameters']['orchestration']['max_concurrency'])\n",
        "    # Prepare the list of asynchronous tasks to run.\n",
        "    tasks: List[Coroutine] = []\n",
        "    for persona_id, row in persona_step3_df.iterrows():\n",
        "        for run in range(1, num_runs + 1):\n",
        "            # If this task has already been completed, skip it.\n",
        "            if (persona_id, run) in completed_tasks:\n",
        "                continue\n",
        "            # Create a coroutine for the API call.\n",
        "            task = _fetch_and_parse_judgment(\n",
        "                client, str(persona_id), row['description'], run, system_prompt,\n",
        "                model_name, api_config['temperature'], api_config['max_retries'], semaphore\n",
        "            )\n",
        "            tasks.append(task)\n",
        "\n",
        "    # --- Execute Tasks and Save Results ---\n",
        "    # If there are tasks to run...\n",
        "    if tasks:\n",
        "        logger.info(f\"Executing {len(tasks)} new judgment API calls...\")\n",
        "        # Open the checkpoint file in append mode.\n",
        "        with open(checkpoint_path, 'a') as f:\n",
        "            # Use tqdm_asyncio for a progress bar over the async tasks.\n",
        "            for result_coro in tqdm_asyncio.as_completed(tasks, total=len(tasks), desc=\"LLM Judging\"):\n",
        "                # Await the result of the next completed task.\n",
        "                result = await result_coro\n",
        "                # Write the result as a new line in the JSONL checkpoint file.\n",
        "                f.write(json.dumps(result) + '\\n')\n",
        "                # Add the completed task to our set to avoid re-running it if resumed.\n",
        "                completed_tasks.add((result['persona_id'], result['run_number']))\n",
        "        # Add the new results to our in-memory list.\n",
        "        with open(checkpoint_path, 'r') as f:\n",
        "             completed_results = [json.loads(line) for line in f]\n",
        "    else:\n",
        "        logger.info(\"All judgment tasks were already complete.\")\n",
        "\n",
        "    # Convert the full list of results to a DataFrame.\n",
        "    judgments_df = pd.DataFrame(completed_results)\n",
        "\n",
        "    # --- Step 3: Apply Majority Vote and Filter ---\n",
        "    # Call the helper to perform the final filtering step.\n",
        "    persona_final_df, vote_stats = _apply_majority_vote_and_filter(\n",
        "        persona_step3_df, judgments_df\n",
        "    )\n",
        "\n",
        "    # --- Report Generation ---\n",
        "    # Compile the final report.\n",
        "    report = {\n",
        "        'rows_read': len(persona_step3_df),\n",
        "        'rows_written': len(persona_final_df),\n",
        "        'rows_rejected': len(persona_step3_df) - len(persona_final_df),\n",
        "        'rejection_rate': f\"{1 - len(persona_final_df) / len(persona_step3_df):.4%}\",\n",
        "        'total_api_calls': len(judgments_df),\n",
        "        'failed_api_calls': (judgments_df['status'] == 'FAILURE').sum(),\n",
        "        'vote_statistics': vote_stats\n",
        "    }\n",
        "\n",
        "    # Check if the final count matches the paper's expected count.\n",
        "    EXPECTED_FINAL_COUNT = 2368\n",
        "    if report['vote_statistics']['final_count'] != EXPECTED_FINAL_COUNT:\n",
        "        logger.warning(\n",
        "            f\"Final persona count is {report['vote_statistics']['final_count']}, \"\n",
        "            f\"which does not match the paper's expected count of {EXPECTED_FINAL_COUNT}.\"\n",
        "        )\n",
        "\n",
        "    # Log the final summary.\n",
        "    logger.info(\"LLM-as-judge filtering complete.\")\n",
        "    logger.info(f\"Final persona count: {report['rows_written']}.\")\n",
        "\n",
        "    # Return the final DataFrame and the report.\n",
        "    return persona_final_df, report\n"
      ],
      "metadata": {
        "id": "2c3iBx667Uz9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 9 — Validate LLM-as-judge reliability (Cohen's kappa)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 9: Validate LLM-as-judge reliability (Cohen's kappa)\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 9, Step 1 Helper: Sample personas and align judgments\n",
        "# ------------------------------------------------------------------------------\n",
        "def _prepare_kappa_validation_data(\n",
        "    persona_step3_df: pd.DataFrame,\n",
        "    all_judgments_df: pd.DataFrame,\n",
        "    human_annotations_df: pd.DataFrame,\n",
        "    sample_size: int,\n",
        "    random_seed: int\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Samples personas and aligns judgments from the LLM and human annotators.\n",
        "\n",
        "    Args:\n",
        "        persona_step3_df: The deduplicated DataFrame of candidate personas.\n",
        "        all_judgments_df: DataFrame with all LLM judgment runs.\n",
        "        human_annotations_df: DataFrame with human annotations, in tidy format.\n",
        "        sample_size: The number of personas to sample for validation.\n",
        "        random_seed: A random seed for reproducible sampling.\n",
        "\n",
        "    Returns:\n",
        "        A DataFrame where each row is a persona and columns contain the\n",
        "        judgments from each rater for each criterion.\n",
        "    \"\"\"\n",
        "    # --- Sample Personas ---\n",
        "    # Draw a reproducible random sample of persona IDs.\n",
        "    logger.info(f\"Sampling {sample_size} personas for kappa validation...\")\n",
        "    sample_ids = persona_step3_df.sample(n=sample_size, random_state=random_seed).index\n",
        "\n",
        "    # --- Process LLM Judgments ---\n",
        "    # Filter the full judgment set to only include the sampled personas.\n",
        "    llm_judgments_sample = all_judgments_df[all_judgments_df['persona_id'].isin(sample_ids)]\n",
        "    # Group by persona and calculate the majority vote for each criterion.\n",
        "    llm_vote_counts = llm_judgments_sample.groupby(\"persona_id\")[\n",
        "        [\"eu_centrality_pass\", \"monetary_policy_depth_pass\", \"neutrality_pass\"]\n",
        "    ].sum()\n",
        "    # The LLM's final judgment is 'pass' (True) if it got >= 2 votes.\n",
        "    llm_majority_judgments = (llm_vote_counts >= 2).reset_index()\n",
        "    llm_majority_judgments = llm_majority_judgments.rename(columns={\n",
        "        \"eu_centrality_pass\": \"llm_eu\",\n",
        "        \"monetary_policy_depth_pass\": \"llm_mp\",\n",
        "        \"neutrality_pass\": \"llm_neu\"\n",
        "    })\n",
        "\n",
        "    # --- Process Human Annotations ---\n",
        "    # Filter human annotations to the sampled personas.\n",
        "    human_annotations_sample = human_annotations_df[human_annotations_df['persona_id'].isin(sample_ids)]\n",
        "    # Pivot the tidy data into a wide format for easier comparison.\n",
        "    # One row per persona, with columns for each annotator's judgment on each criterion.\n",
        "    human_judgments_wide = human_annotations_sample.pivot_table(\n",
        "        index='persona_id',\n",
        "        columns=['annotator_id', 'criterion'],\n",
        "        values='judgment'\n",
        "    ).reset_index()\n",
        "    # Flatten the multi-level column index.\n",
        "    human_judgments_wide.columns = ['_'.join(col).strip() if isinstance(col, tuple) and col[1] else col[0] for col in human_judgments_wide.columns.values]\n",
        "\n",
        "    # --- Align All Judgments ---\n",
        "    # Merge the LLM and human judgments into a single DataFrame.\n",
        "    aligned_df = pd.merge(llm_majority_judgments, human_judgments_wide, on='persona_id')\n",
        "\n",
        "    logger.info(f\"Successfully aligned judgments for {len(aligned_df)} personas.\")\n",
        "    return aligned_df\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 9, Step 2 & 3 Helper: Compute kappa scores and generate report\n",
        "# ------------------------------------------------------------------------------\n",
        "def _compute_and_report_kappa(aligned_judgments_df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Computes Cohen's kappa scores and confusion matrices for all rater pairs.\n",
        "\n",
        "    Args:\n",
        "        aligned_judgments_df: A DataFrame with aligned judgments from all raters.\n",
        "\n",
        "    Returns:\n",
        "        A DataFrame summarizing the reliability analysis results.\n",
        "    \"\"\"\n",
        "    # Define the criteria and the raters to be compared.\n",
        "    criteria = {\"eu\": \"EU Centrality\", \"mp\": \"MP Depth\", \"neu\": \"Neutrality\"}\n",
        "    # Assuming annotator IDs are 'A1' and 'A2' from the pivoted columns.\n",
        "    raters = {\"A1\": \"Human 1\", \"A2\": \"Human 2\", \"llm\": \"LLM Majority\"}\n",
        "    rater_pairs = [(\"A1\", \"A2\"), (\"A1\", \"llm\"), (\"A2\", \"llm\")]\n",
        "\n",
        "    # Initialize a list to store the results.\n",
        "    results = []\n",
        "\n",
        "    # Iterate through each criterion to compute kappa scores.\n",
        "    for crit_key, crit_name in criteria.items():\n",
        "        # Iterate through each pair of raters.\n",
        "        for rater1_key, rater2_key in rater_pairs:\n",
        "            # Construct the column names for the current criterion and raters.\n",
        "            col1 = f\"{rater1_key}_{crit_key}\"\n",
        "            col2 = f\"{rater2_key}_{crit_key}\"\n",
        "\n",
        "            # Extract the judgment vectors for the two raters.\n",
        "            y1 = aligned_judgments_df[col1]\n",
        "            y2 = aligned_judgments_df[col2]\n",
        "\n",
        "            # --- Compute Cohen's Kappa ---\n",
        "            # Equation: κ = (p_o - p_e) / (1 - p_e)\n",
        "            kappa_score = cohen_kappa_score(y1, y2)\n",
        "\n",
        "            # --- Compute Confusion Matrix ---\n",
        "            # tn, fp, fn, tp\n",
        "            cm = confusion_matrix(y1, y2, labels=[False, True]).ravel()\n",
        "\n",
        "            # Store the results in a dictionary.\n",
        "            results.append({\n",
        "                \"Criterion\": crit_name,\n",
        "                \"Rater 1\": raters[rater1_key],\n",
        "                \"Rater 2\": raters[rater2_key],\n",
        "                \"Cohen's Kappa\": kappa_score,\n",
        "                \"Confusion Matrix (tn, fp, fn, tp)\": tuple(cm)\n",
        "            })\n",
        "\n",
        "    # Convert the list of results into a DataFrame.\n",
        "    report_df = pd.DataFrame(results)\n",
        "\n",
        "    # --- Assess Reliability ---\n",
        "    # Add a qualitative interpretation of the kappa score.\n",
        "    def interpret_kappa(score: float) -> str:\n",
        "        if score < 0.2: return \"Poor\"\n",
        "        if score < 0.4: return \"Fair\"\n",
        "        if score < 0.6: return \"Moderate\"\n",
        "        if score >= 0.6 and score < 0.8: return \"Substantial\"\n",
        "        if score >= 0.8: return \"Almost Perfect\"\n",
        "        return \"N/A\"\n",
        "\n",
        "    report_df[\"Interpretation\"] = report_df[\"Cohen's Kappa\"].apply(interpret_kappa)\n",
        "\n",
        "    # Check if the results meet the paper's reported reliability standard.\n",
        "    # The paper reports kappa values in the range [0.61, 0.81].\n",
        "    report_df[\"Meets Paper Standard (κ > 0.6)\"] = report_df[\"Cohen's Kappa\"] > 0.6\n",
        "\n",
        "    return report_df\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 9, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "def validate_llm_judge_reliability(\n",
        "    persona_step3_df: pd.DataFrame,\n",
        "    all_judgments_df: pd.DataFrame,\n",
        "    human_annotations_df: pd.DataFrame,\n",
        "    config: Dict[str, Any]\n",
        ") -> Tuple[pd.DataFrame, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Orchestrates the validation of the LLM-as-judge's reliability.\n",
        "\n",
        "    This function performs a full inter-rater reliability analysis:\n",
        "    1.  Draws a reproducible random sample of personas.\n",
        "    2.  Aligns the LLM's majority-vote judgments with judgments from two\n",
        "        human annotators for the sampled personas.\n",
        "    3.  Computes Cohen's kappa and confusion matrices for all pairs of raters\n",
        "        (Human-Human, Human-LLM) across all three judgment criteria.\n",
        "    4.  Generates a comprehensive report assessing if the LLM's reliability\n",
        "        meets the \"substantial agreement\" standard reported in the paper.\n",
        "\n",
        "    Args:\n",
        "        persona_step3_df: The deduplicated DataFrame of candidate personas.\n",
        "        all_judgments_df: DataFrame containing all raw LLM judgment runs.\n",
        "        human_annotations_df: A tidy DataFrame with human annotations, containing\n",
        "                              columns ['persona_id', 'annotator_id', 'criterion', 'judgment'].\n",
        "        config: The study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "        - A pandas DataFrame summarizing the kappa analysis results.\n",
        "        - A dictionary containing a high-level summary and overall assessment.\n",
        "    \"\"\"\n",
        "    # Log the start of the task.\n",
        "    logger.info(\"Starting Task 9: Validate LLM-as-judge reliability.\")\n",
        "\n",
        "    # --- Configuration Extraction ---\n",
        "    kappa_config = config['phase_1_parameters']['llm_as_judge']['kappa_validation']\n",
        "    sample_size = kappa_config['human_annotated_sample_size']\n",
        "    # Use the main Monte Carlo seed for reproducibility in sampling.\n",
        "    random_seed = config['phase_2_parameters']['scoring_and_inference']['monte_carlo_seed']\n",
        "\n",
        "    # --- Step 1: Sample and Align Data ---\n",
        "    # Prepare the analysis-ready DataFrame with judgments from all sources.\n",
        "    aligned_judgments_df = _prepare_kappa_validation_data(\n",
        "        persona_step3_df, all_judgments_df, human_annotations_df, sample_size, random_seed\n",
        "    )\n",
        "\n",
        "    # --- Step 2 & 3: Compute Kappa and Generate Report ---\n",
        "    # Compute all statistics and generate the detailed report DataFrame.\n",
        "    kappa_report_df = _compute_and_report_kappa(aligned_judgments_df)\n",
        "\n",
        "    # --- Final Assessment ---\n",
        "    # Determine if the overall process meets the paper's standard.\n",
        "    # The validation passes if all computed kappa scores are > 0.6.\n",
        "    overall_assessment = \"SUCCESS\" if kappa_report_df[\"Meets Paper Standard (κ > 0.6)\"].all() else \"FAILURE\"\n",
        "\n",
        "    # Create a high-level summary dictionary.\n",
        "    summary = {\n",
        "        \"overall_assessment\": overall_assessment,\n",
        "        \"assessment_details\": (\n",
        "            \"All computed Cohen's kappa scores exceed 0.6, indicating 'Substantial Agreement' \"\n",
        "            \"and successfully replicating the paper's reliability findings.\"\n",
        "            if overall_assessment == \"SUCCESS\"\n",
        "            else \"One or more kappa scores were below the 0.6 threshold for 'Substantial Agreement'.\"\n",
        "        ),\n",
        "        \"sample_size\": sample_size,\n",
        "    }\n",
        "\n",
        "    # Log the final assessment.\n",
        "    logger.info(f\"LLM judge reliability validation complete. Overall Assessment: {overall_assessment}\")\n",
        "\n",
        "    # Return the detailed report and the high-level summary.\n",
        "    return kappa_report_df, summary\n"
      ],
      "metadata": {
        "id": "SQyDxvHl8kVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 10 — Assemble forecasting prompts (persona and no-persona arms)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 10: Assemble forecasting prompts (persona and no-persona arms)\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 10, Step 1 & 2 Helper: Prompt Templates\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "# Define the verbatim system prompt template for the persona arm.\n",
        "_SYSTEM_PROMPT_PERSONA = \"\"\"You are participating in the European Central Bank's Survey of Professional Forecasters (ECB-SPF) for round: {survey_round}.\n",
        "Today's date is {survey_date_str}.\n",
        "You will be asked to provide point forecasts for a set of key macroeconomic indicators (inflation, core inflation, GDP growth and unemployment) for the euro area at different time horizons.\n",
        "\n",
        "You are: {persona_blurb}\"\"\"\n",
        "\n",
        "# Define the verbatim system prompt template for the no-persona (ablation) arm.\n",
        "_SYSTEM_PROMPT_NO_PERSONA = \"\"\"You are participating in the European Central Bank's Survey of Professional Forecasters (ECB-SPF) for round: {survey_round}.\n",
        "Today's date is {survey_date_str}.\n",
        "You will be asked to provide point forecasts for a set of key macroeconomic indicators (inflation, core inflation, GDP growth and unemployment) for the euro area at different time horizons.\"\"\"\n",
        "\n",
        "# Define the verbatim user prompt template, shared by both arms.\n",
        "_USER_PROMPT_TEMPLATE = \"\"\"ECONOMIC CONTEXT:\n",
        "- Latest realized data:\n",
        "Variable  Period       Value\n",
        "HICP      {hicp_period:<9} {hicp_value}\n",
        "HICPX     {hicpx_period:<9} {hicpx_value}\n",
        "UNR       {unr_period:<9} {unr_value}\n",
        "RGDP      {rgdp_period:<9} {rgdp_value}\n",
        "\n",
        "- Median forecasts from the previous SPF round:\n",
        "Variable  Horizon  Median forecast\n",
        "HICP      {year_cy:<7}  {prev_spf_hicp_cy}\n",
        "HICP      {year_cy1:<7}  {prev_spf_hicp_cy1}\n",
        "HICP      {year_cy2:<7}  {prev_spf_hicp_cy2}\n",
        "HICP      {year_lt:<7}  {prev_spf_hicp_lt}\n",
        "HICPX     {year_cy:<7}  {prev_spf_hicpx_cy}\n",
        "HICPX     {year_cy1:<7}  {prev_spf_hicpx_cy1}\n",
        "HICPX     {year_cy2:<7}  {prev_spf_hicpx_cy2}\n",
        "HICPX     {year_lt:<7}  {prev_spf_hicpx_lt}\n",
        "UNR       {year_cy:<7}  {prev_spf_unr_cy}\n",
        "UNR       {year_cy1:<7}  {prev_spf_unr_cy1}\n",
        "UNR       {year_cy2:<7}  {prev_spf_unr_cy2}\n",
        "UNR       {year_lt:<7}  {prev_spf_unr_lt}\n",
        "RGDP      {year_cy:<7}  {prev_spf_rgdp_cy}\n",
        "RGDP      {year_cy1:<7}  {prev_spf_rgdp_cy1}\n",
        "RGDP      {year_cy2:<7}  {prev_spf_rgdp_cy2}\n",
        "RGDP      {year_lt:<7}  {prev_spf_rgdp_lt}\n",
        "\n",
        "- ECB monetary policy communication from the latest Governing Council meeting on {ecb_meeting_date_str}:\n",
        "{ecb_communication_text}\n",
        "\n",
        "TASK:\n",
        "You are asked to provide one numeric point forecast for each target macroeconomic variable listed below, at multiple time horizons.\n",
        "Do not use ranges or confidence intervals.\n",
        "All forecasts should be expressed in percent (%), do not include units in the answer.\n",
        "\n",
        "TARGETS:\n",
        "For each of the following variables, provide a point forecast:\n",
        "- HICP: HICP inflation\n",
        "- HICPX: HICP inflation excluding food and energy\n",
        "- RGDP: Real GDP growth\n",
        "- UNR: Unemployment rate\n",
        "\n",
        "Each variable should be forecast at the following horizons:\n",
        "- t0: current calendar year ({year_cy})\n",
        "- t1: next year ({year_cy1})\n",
        "- t2: year after next ({year_cy2})\n",
        "- lt: long-term ({year_lt})\n",
        "\n",
        "OUTPUT SCHEMA:\n",
        "{{\n",
        "  \"forecasts\": [\n",
        "    {{ \"variable\": \"hicp\",  \"horizon\": \"t0\", \"value\": \"<<numeric>>\" }},\n",
        "    {{ \"variable\": \"hicp\",  \"horizon\": \"t1\", \"value\": \"<<numeric>>\" }},\n",
        "    {{ \"variable\": \"hicp\",  \"horizon\": \"t2\", \"value\": \"<<numeric>>\" }},\n",
        "    {{ \"variable\": \"hicp\",  \"horizon\": \"lt\", \"value\": \"<<numeric>>\" }},\n",
        "    {{ \"variable\": \"hicpx\", \"horizon\": \"t0\", \"value\": \"<<numeric>>\" }},\n",
        "    {{ \"variable\": \"hicpx\", \"horizon\": \"t1\", \"value\": \"<<numeric>>\" }},\n",
        "    {{ \"variable\": \"hicpx\", \"horizon\": \"t2\", \"value\": \"<<numeric>>\" }},\n",
        "    {{ \"variable\": \"hicpx\", \"horizon\": \"lt\", \"value\": \"<<numeric>>\" }},\n",
        "    {{ \"variable\": \"rgdp\",  \"horizon\": \"t0\", \"value\": \"<<numeric>>\" }},\n",
        "    {{ \"variable\": \"rgdp\",  \"horizon\": \"t1\", \"value\": \"<<numeric>>\" }},\n",
        "    {{ \"variable\": \"rgdp\",  \"horizon\": \"t2\", \"value\": \"<<numeric>>\" }},\n",
        "    {{ \"variable\": \"rgdp\",  \"horizon\": \"lt\", \"value\": \"<<numeric>>\" }},\n",
        "    {{ \"variable\": \"unr\",   \"horizon\": \"t0\", \"value\": \"<<numeric>>\" }},\n",
        "    {{ \"variable\": \"unr\",   \"horizon\": \"t1\", \"value\": \"<<numeric>>\" }},\n",
        "    {{ \"variable\": \"unr\",   \"horizon\": \"t2\", \"value\": \"<<numeric>>\" }},\n",
        "    {{ \"variable\": \"unr\",   \"horizon\": \"lt\", \"value\": \"<<numeric>>\" }}\n",
        "  ]\n",
        "}}\n",
        "\n",
        "Reply only with the JSON, no additional text.\"\"\"\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 10, Step 3 Helper: Populate placeholders for a single round\n",
        "# ------------------------------------------------------------------------------\n",
        "def _prepare_context_placeholders(\n",
        "    context_row: pd.Series\n",
        ") -> Dict[str, Union[str, int, float]]:\n",
        "    \"\"\"\n",
        "    Prepares a dictionary of placeholder values from a row of contextual_data_df.\n",
        "\n",
        "    Args:\n",
        "        context_row: A pandas Series representing one row (one survey round).\n",
        "\n",
        "    Returns:\n",
        "        A dictionary where keys are placeholder names and values are the\n",
        "        formatted data to be injected into the prompt templates.\n",
        "    \"\"\"\n",
        "    # Helper to format values, converting NaN to \"N/A\".\n",
        "    def fmt(val: Any, precision: int = 2) -> str:\n",
        "        if pd.isna(val):\n",
        "            return \"N/A\"\n",
        "        if isinstance(val, (float, np.floating)):\n",
        "            return f\"{val:.{precision}f}\"\n",
        "        return str(val)\n",
        "\n",
        "    # --- Date and Year Placeholders ---\n",
        "    base_year = context_row['survey_date'].year\n",
        "    placeholders = {\n",
        "        \"survey_round\": context_row['survey_round'],\n",
        "        \"survey_date_str\": context_row['survey_date'].strftime('%d/%m/%Y'),\n",
        "        \"ecb_meeting_date_str\": context_row['ecb_meeting_date'].strftime('%Y-%m-%d'),\n",
        "        \"year_cy\": base_year,\n",
        "        \"year_cy1\": base_year + 1,\n",
        "        \"year_cy2\": base_year + 2,\n",
        "        \"year_lt\": context_row['lt_year'],\n",
        "    }\n",
        "\n",
        "    # --- Latest Realized Data Placeholders ---\n",
        "    realized = context_row['latest_realized_data']\n",
        "    placeholders.update({\n",
        "        \"hicp_period\": realized['hicp']['period'] or \"N/A\",\n",
        "        \"hicp_value\": fmt(realized['hicp']['value']),\n",
        "        \"hicpx_period\": realized['hicpx']['period'] or \"N/A\",\n",
        "        \"hicpx_value\": fmt(realized['hicpx']['value']),\n",
        "        \"unr_period\": realized['unr']['period'] or \"N/A\",\n",
        "        \"unr_value\": fmt(realized['unr']['value']),\n",
        "        \"rgdp_period\": realized['rgdp']['period'] or \"N/A\",\n",
        "        \"rgdp_value\": fmt(realized['rgdp']['value']),\n",
        "    })\n",
        "\n",
        "    # --- Previous SPF Medians Placeholders ---\n",
        "    medians = context_row['previous_spf_medians']\n",
        "    placeholders.update({\n",
        "        \"prev_spf_hicp_cy\": fmt(medians['hicp'].get(base_year)),\n",
        "        \"prev_spf_hicp_cy1\": fmt(medians['hicp'].get(base_year + 1)),\n",
        "        \"prev_spf_hicp_cy2\": fmt(medians['hicp'].get(base_year + 2)),\n",
        "        \"prev_spf_hicp_lt\": fmt(medians['hicp'].get(context_row['lt_year'])),\n",
        "        \"prev_spf_hicpx_cy\": fmt(medians['hicpx'].get(base_year)),\n",
        "        \"prev_spf_hicpx_cy1\": fmt(medians['hicpx'].get(base_year + 1)),\n",
        "        \"prev_spf_hicpx_cy2\": fmt(medians['hicpx'].get(base_year + 2)),\n",
        "        \"prev_spf_hicpx_lt\": fmt(medians['hicpx'].get(context_row['lt_year'])),\n",
        "        \"prev_spf_unr_cy\": fmt(medians['unr'].get(base_year)),\n",
        "        \"prev_spf_unr_cy1\": fmt(medians['unr'].get(base_year + 1)),\n",
        "        \"prev_spf_unr_cy2\": fmt(medians['unr'].get(base_year + 2)),\n",
        "        \"prev_spf_unr_lt\": fmt(medians['unr'].get(context_row['lt_year'])),\n",
        "        \"prev_spf_rgdp_cy\": fmt(medians['rgdp'].get(base_year)),\n",
        "        \"prev_spf_rgdp_cy1\": fmt(medians['rgdp'].get(base_year + 1)),\n",
        "        \"prev_spf_rgdp_cy2\": fmt(medians['rgdp'].get(base_year + 2)),\n",
        "        \"prev_spf_rgdp_lt\": fmt(medians['rgdp'].get(context_row['lt_year'])),\n",
        "    })\n",
        "\n",
        "    # --- ECB Communication Text ---\n",
        "    placeholders[\"ecb_communication_text\"] = context_row['ecb_communication_text']\n",
        "\n",
        "    return placeholders\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 10, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "def assemble_forecasting_prompts(\n",
        "    persona_final_df: pd.DataFrame,\n",
        "    contextual_data_df: pd.DataFrame,\n",
        "    config: Dict[str, Any]\n",
        ") -> Generator[Dict[str, Any], None, None]:\n",
        "    \"\"\"\n",
        "    Assembles and yields all forecasting prompts for both experimental arms.\n",
        "\n",
        "    This function iterates through each survey round and generates the full set\n",
        "    of prompts required for the experiment:\n",
        "    1.  Persona Arm: One prompt for each of the 2,368 final personas for each\n",
        "        of the 50 survey rounds.\n",
        "    2.  No-Persona Arm: 100 baseline prompts for each of the 50 survey rounds.\n",
        "\n",
        "    The function is implemented as a generator to be memory-efficient, yielding\n",
        "    one prompt object at a time.\n",
        "\n",
        "    Args:\n",
        "        persona_final_df: The final, filtered DataFrame of personas (P*).\n",
        "        contextual_data_df: The cleansed DataFrame of contextual data.\n",
        "        config: The study configuration dictionary.\n",
        "\n",
        "    Yields:\n",
        "        A dictionary representing a single, fully-formed prompt object,\n",
        "        containing API messages and metadata.\n",
        "    \"\"\"\n",
        "    # Log the start of the prompt assembly process.\n",
        "    logger.info(\"Assembling forecasting prompts for both persona and no-persona arms...\")\n",
        "\n",
        "    # Extract the number of baseline runs from the configuration.\n",
        "    baseline_runs = config['phase_2_parameters']['ablation_study']['baseline_runs_per_round']\n",
        "\n",
        "    # Iterate through each survey round in the contextual data.\n",
        "    for _, context_row in contextual_data_df.iterrows():\n",
        "        # Prepare the dictionary of placeholder values for the current round.\n",
        "        placeholders = _prepare_context_placeholders(context_row)\n",
        "        # Format the shared user message for this round.\n",
        "        user_message = _USER_PROMPT_TEMPLATE.format(**placeholders)\n",
        "\n",
        "        # --- Persona Arm Prompt Generation ---\n",
        "        # Iterate through each persona in the final set.\n",
        "        for persona_id, persona_row in persona_final_df.iterrows():\n",
        "            # Add the persona-specific placeholder.\n",
        "            placeholders[\"persona_blurb\"] = persona_row['description']\n",
        "            # Format the persona-specific system message.\n",
        "            system_message = _SYSTEM_PROMPT_PERSONA.format(**placeholders)\n",
        "\n",
        "            # Yield a structured prompt object.\n",
        "            yield {\n",
        "                \"metadata\": {\n",
        "                    \"arm\": \"persona\",\n",
        "                    \"id\": str(persona_id),\n",
        "                    \"round\": context_row['survey_round'],\n",
        "                },\n",
        "                \"messages\": [\n",
        "                    {\"role\": \"system\", \"content\": system_message},\n",
        "                    {\"role\": \"user\", \"content\": user_message},\n",
        "                ],\n",
        "            }\n",
        "\n",
        "        # --- No-Persona Arm Prompt Generation ---\n",
        "        # Iterate to create the specified number of baseline prompts.\n",
        "        for i in range(baseline_runs):\n",
        "            # Format the no-persona system message.\n",
        "            system_message = _SYSTEM_PROMPT_NO_PERSONA.format(**placeholders)\n",
        "\n",
        "            # Yield a structured prompt object.\n",
        "            yield {\n",
        "                \"metadata\": {\n",
        "                    \"arm\": \"baseline\",\n",
        "                    \"id\": i + 1, # Use a 1-based index for baseline runs.\n",
        "                    \"round\": context_row['survey_round'],\n",
        "                },\n",
        "                \"messages\": [\n",
        "                    {\"role\": \"system\", \"content\": system_message},\n",
        "                    {\"role\": \"user\", \"content\": user_message},\n",
        "                ],\n",
        "            }\n",
        "\n",
        "    # Log the completion of the generator.\n",
        "    logger.info(\"Finished yielding all forecasting prompts.\")\n"
      ],
      "metadata": {
        "id": "thCrExsJ9W38"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 11 — Generate forecasts (persona arm)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 11: Generate forecasts (persona arm)\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 11, Step 1 Helper: Configure LLM forecaster\n",
        "# ------------------------------------------------------------------------------\n",
        "def _initialize_openai_client(config: Dict[str, Any]) -> AsyncOpenAI:\n",
        "    \"\"\"\n",
        "    Initializes the asynchronous OpenAI client.\n",
        "\n",
        "    Args:\n",
        "        config: The study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        An initialized AsyncOpenAI client.\n",
        "    \"\"\"\n",
        "    # Input validation: Ensure an API key is available.\n",
        "    if not config.get(\"openai_api_key\"):\n",
        "        raise ValueError(\"OpenAI API key not found in environment variables or config.\")\n",
        "\n",
        "    # Initialize the asynchronous OpenAI client.\n",
        "    client = AsyncOpenAI(api_key=config[\"openai_api_key\"])\n",
        "\n",
        "    # Log the successful initialization.\n",
        "    logger.info(\"OpenAI client initialized successfully.\")\n",
        "\n",
        "    # Return the client.\n",
        "    return client\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 11, Step 2 Helpers: Concurrent API call execution and response capture\n",
        "# ------------------------------------------------------------------------------\n",
        "async def _fetch_forecast(\n",
        "    client: AsyncOpenAI,\n",
        "    prompt_obj: Dict[str, Any],\n",
        "    api_config: Dict[str, Any],\n",
        "    semaphore: asyncio.Semaphore\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Asynchronously calls the LLM API for a single forecasting prompt.\n",
        "\n",
        "    This is a dedicated helper function for the forecasting task. It correctly\n",
        "    uses the `messages` list from the provided prompt object. It includes robust\n",
        "    error handling and a retry mechanism.\n",
        "\n",
        "    Args:\n",
        "        client: The asynchronous OpenAI API client.\n",
        "        prompt_obj: The prompt object, containing `metadata` and `messages`.\n",
        "        api_config: The API configuration dictionary for the forecasting LLM.\n",
        "        semaphore: An asyncio.Semaphore to control concurrency.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing the original metadata and the API response,\n",
        "        or a failure record.\n",
        "    \"\"\"\n",
        "    # Acquire the semaphore to ensure we do not exceed the max concurrency limit.\n",
        "    async with semaphore:\n",
        "        # Extract metadata from the prompt object for precise logging and error reporting.\n",
        "        metadata = prompt_obj[\"metadata\"]\n",
        "\n",
        "        # Loop to implement the retry mechanism. It will run for `max_retries` + 1 attempts.\n",
        "        for attempt in range(api_config['api_settings']['max_retries'] + 1):\n",
        "            try:\n",
        "                # Make the asynchronous API call to the chat completions endpoint.\n",
        "                response = await client.chat.completions.create(\n",
        "                    # The model to use for the completion.\n",
        "                    model=api_config['model_name'],\n",
        "                    # The list of messages comprising the conversation, passed directly from the prompt object.\n",
        "                    messages=prompt_obj[\"messages\"],\n",
        "                    # The sampling temperature. 1.0 is used for stochasticity.\n",
        "                    temperature=api_config['api_settings']['temperature'],\n",
        "                    # Instruct the model to return a guaranteed JSON object.\n",
        "                    response_format={\"type\": \"json_object\"},\n",
        "                )\n",
        "                # If the call is successful, return a structured success record.\n",
        "                return {\n",
        "                    \"metadata\": metadata,\n",
        "                    \"status\": \"SUCCESS\",\n",
        "                    \"raw_response\": response.choices[0].message.content,\n",
        "                }\n",
        "            # Catch specific, retryable API errors (e.g., rate limits, server errors).\n",
        "            except (APIError, RateLimitError) as e:\n",
        "                # Log the failure of the current attempt.\n",
        "                logger.warning(f\"API call for {metadata['id']} round {metadata['round']} failed on attempt {attempt + 1}: {e}\")\n",
        "                # If this was the final attempt, break the loop to return a failure.\n",
        "                if attempt >= api_config['api_settings']['max_retries']:\n",
        "                    break\n",
        "                # Wait for the specified backoff period before the next attempt.\n",
        "                await asyncio.sleep(api_config['api_settings']['retry_backoff_sec'])\n",
        "            # Catch any other unexpected exceptions (e.g., network issues, validation errors).\n",
        "            except Exception as e:\n",
        "                # Log the unexpected error. These are typically not retried.\n",
        "                logger.error(f\"Unexpected error for {metadata['id']} round {metadata['round']} on attempt {attempt + 1}: {e}\")\n",
        "                # Break the loop immediately as the error is likely not transient.\n",
        "                break\n",
        "\n",
        "    # If all attempts fail, return a structured failure record.\n",
        "    return {\"metadata\": metadata, \"status\": \"FAILURE\", \"raw_response\": None}\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# API Call Orchestrator (previously _execute_api_calls)\n",
        "# ------------------------------------------------------------------------------\n",
        "async def _run_forecast_generation(\n",
        "    prompt_generator: Generator[Dict[str, Any], None, None],\n",
        "    arm_name: str,\n",
        "    config: Dict[str, Any],\n",
        "    checkpoint_path: str\n",
        ") -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Orchestrates the concurrent generation of forecasts for a specific arm.\n",
        "\n",
        "    This function manages the entire asynchronous workflow for making a high\n",
        "    volume of API calls. It is designed for resilience and efficiency:\n",
        "    1.  Filters prompts for the specified experimental arm ('persona' or 'baseline').\n",
        "    2.  Implements resumability by loading completed tasks from a checkpoint file.\n",
        "    3.  Uses a semaphore to control concurrency and respect rate limits.\n",
        "    4.  Calls a dedicated helper (`_fetch_forecast`) for each API request.\n",
        "    5.  Writes results immediately to the checkpoint file as they complete.\n",
        "\n",
        "    Args:\n",
        "        prompt_generator: The generator yielding all prompt objects for all arms.\n",
        "        arm_name: The name of the experimental arm to run ('persona' or 'baseline').\n",
        "        config: The study configuration dictionary.\n",
        "        checkpoint_path: Path to the JSONL file for saving/resuming raw results.\n",
        "\n",
        "    Returns:\n",
        "        A list of all raw result dictionaries (both resumed and newly fetched).\n",
        "    \"\"\"\n",
        "    # --- Configuration and Initialization ---\n",
        "    # Extract the API configuration for the forecasting LLM.\n",
        "    api_config = config['phase_2_parameters']['forecasting_llm']\n",
        "    # Extract the concurrency limit from the orchestration config.\n",
        "    concurrency = config['phase_1_parameters']['orchestration']['max_concurrency']\n",
        "\n",
        "    # Filter the main prompt generator to get only the prompts for the specified arm.\n",
        "    prompts_for_arm = [p for p in prompt_generator if p[\"metadata\"][\"arm\"] == arm_name]\n",
        "\n",
        "    # --- Resumability Logic ---\n",
        "    # Initialize a list to hold results from the checkpoint file.\n",
        "    completed_results: List[Dict[str, Any]] = []\n",
        "    # Initialize a set to track the unique identifiers of completed tasks.\n",
        "    completed_tasks: set[tuple] = set()\n",
        "    try:\n",
        "        # Open and read the checkpoint file line by line.\n",
        "        with open(checkpoint_path, 'r') as f:\n",
        "            # Each line is a JSON object representing a completed result.\n",
        "            for line in f:\n",
        "                # Parse the JSON line.\n",
        "                res = json.loads(line)\n",
        "                # Add the result to our list.\n",
        "                completed_results.append(res)\n",
        "                # Add the task's unique ID to the set of completed tasks.\n",
        "                completed_tasks.add((str(res['metadata']['id']), res['metadata']['round']))\n",
        "        # Log how many results were successfully resumed.\n",
        "        logger.info(f\"Resuming from checkpoint '{checkpoint_path}'. Found {len(completed_tasks)} completed forecasts for arm '{arm_name}'.\")\n",
        "    except FileNotFoundError:\n",
        "        # If the file doesn't exist, simply log that we are starting fresh.\n",
        "        logger.info(f\"No checkpoint file found at '{checkpoint_path}'. Starting from scratch for arm '{arm_name}'.\")\n",
        "\n",
        "    # --- Asynchronous Task Preparation ---\n",
        "    # Initialize the asynchronous OpenAI client.\n",
        "    client = AsyncOpenAI()\n",
        "    # Initialize a semaphore to control the maximum number of concurrent API calls.\n",
        "    semaphore = asyncio.Semaphore(concurrency)\n",
        "\n",
        "    # Create a list of coroutine tasks for the prompts that have not yet been completed.\n",
        "    tasks_to_run: List[Coroutine] = []\n",
        "    # Iterate through all prompts designated for this arm.\n",
        "    for prompt in prompts_for_arm:\n",
        "        # Extract the unique identifier for the current prompt.\n",
        "        metadata = prompt[\"metadata\"]\n",
        "        task_id = (str(metadata[\"id\"]), metadata[\"round\"])\n",
        "        # If this task's ID is not in the set of completed tasks, create a new task for it.\n",
        "        if task_id not in completed_tasks:\n",
        "            # Create the coroutine by calling the dedicated `_fetch_forecast` helper.\n",
        "            tasks_to_run.append(\n",
        "                _fetch_forecast(client, prompt, api_config, semaphore)\n",
        "            )\n",
        "\n",
        "    # --- Asynchronous Task Execution ---\n",
        "    # Proceed only if there are new tasks to execute.\n",
        "    if tasks_to_run:\n",
        "        # Log the number of new API calls that will be made.\n",
        "        logger.info(f\"Executing {len(tasks_to_run)} new forecast API calls for arm '{arm_name}'...\")\n",
        "        # Open the checkpoint file in append mode to add new results.\n",
        "        with open(checkpoint_path, 'a') as f:\n",
        "            # Use tqdm_asyncio.as_completed to process tasks as they finish and show a progress bar.\n",
        "            for result_coro in tqdm_asyncio.as_completed(tasks_to_run, total=len(tasks_to_run), desc=f\"Forecasting ({arm_name} arm)\"):\n",
        "                # Await the result of the next completed coroutine.\n",
        "                result = await result_coro\n",
        "                # Append the new result to our in-memory list.\n",
        "                completed_results.append(result)\n",
        "                # Immediately write the JSON result as a new line in the checkpoint file for persistence.\n",
        "                f.write(json.dumps(result) + '\\n')\n",
        "    else:\n",
        "        # If all tasks were already completed, log this and do nothing.\n",
        "        logger.info(f\"All forecasting tasks for arm '{arm_name}' were already complete.\")\n",
        "\n",
        "    # Return the complete list of results, including those loaded from the checkpoint and newly fetched ones.\n",
        "    return completed_results\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 11, Step 3 Helper: Response persistence and initial quality control\n",
        "# ------------------------------------------------------------------------------\n",
        "def _parse_and_persist_forecasts(\n",
        "    raw_results: List[Dict[str, Any]],\n",
        "    id_column_name: str\n",
        ") -> Tuple[pd.DataFrame, Dict[str, int]]:\n",
        "    \"\"\"\n",
        "    Parses raw forecast results, validates them, and creates a tidy DataFrame.\n",
        "\n",
        "    This generic function is designed to process the raw output from the API\n",
        "    execution stage for either the 'persona' or 'baseline' arm. It performs:\n",
        "    1.  Robust JSON parsing of the raw LLM response.\n",
        "    2.  Strict schema validation (must contain 16 forecast items).\n",
        "    3.  Data type conversion and validation for the forecast 'value'.\n",
        "\n",
        "    Args:\n",
        "        raw_results: A list of raw result dictionaries from the API calls.\n",
        "        id_column_name: The name for the identifier column in the output\n",
        "                        DataFrame ('persona_id' or 'baseline_id').\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "        - A pandas DataFrame of the clean, parsed forecasts.\n",
        "        - A dictionary reporting parsing statistics (successes and failures).\n",
        "    \"\"\"\n",
        "    # Initialize a list to store successfully parsed forecast records.\n",
        "    parsed_records: List[Dict[str, Any]] = []\n",
        "    # Initialize a counter for failed or unparsable API calls.\n",
        "    failure_count = 0\n",
        "\n",
        "    # Log the start of the parsing process.\n",
        "    logger.info(f\"Parsing {len(raw_results)} raw results for arm '{id_column_name.split('_')[0]}'.\")\n",
        "\n",
        "    # Iterate through each raw result dictionary from the API execution stage.\n",
        "    for result in raw_results:\n",
        "        # Check the status of the API call. If it failed, increment the counter and skip.\n",
        "        if result.get(\"status\") == \"FAILURE\":\n",
        "            # Increment the failure count.\n",
        "            failure_count += 1\n",
        "            # Continue to the next result.\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            # Extract the metadata and the raw JSON response string.\n",
        "            metadata = result[\"metadata\"]\n",
        "            raw_response_str = result[\"raw_response\"]\n",
        "\n",
        "            # Ensure the raw response is not null or empty before parsing.\n",
        "            if not raw_response_str:\n",
        "                raise ValueError(\"Raw response content is empty.\")\n",
        "\n",
        "            # Parse the JSON string into a Python dictionary.\n",
        "            response_data = json.loads(raw_response_str)\n",
        "            # Extract the list of forecasts from the parsed data.\n",
        "            forecast_list = response_data[\"forecasts\"]\n",
        "\n",
        "            # Validate that the response contains exactly 16 forecast items as per the prompt schema.\n",
        "            if len(forecast_list) != 16:\n",
        "                raise ValueError(f\"Expected 16 forecasts, but found {len(forecast_list)}\")\n",
        "\n",
        "            # Process each individual forecast item in the list.\n",
        "            for item in forecast_list:\n",
        "                # Append a structured record to our list for DataFrame creation.\n",
        "                parsed_records.append({\n",
        "                    id_column_name: metadata[\"id\"],\n",
        "                    \"round\": metadata[\"round\"],\n",
        "                    \"variable\": item[\"variable\"].upper(), # Standardize to uppercase\n",
        "                    \"horizon\": item[\"horizon\"],\n",
        "                    \"value\": float(item[\"value\"]), # Convert value to float, will raise error if not possible\n",
        "                })\n",
        "        # Catch any exceptions during parsing or validation.\n",
        "        except (json.JSONDecodeError, KeyError, ValueError, TypeError) as e:\n",
        "            # Log a warning with specific details about the failure.\n",
        "            metadata = result.get(\"metadata\", {})\n",
        "            logger.warning(\n",
        "                f\"Failed to parse response for ID {metadata.get('id')} \"\n",
        "                f\"in round {metadata.get('round')}: {e}\"\n",
        "            )\n",
        "            # Increment the failure count.\n",
        "            failure_count += 1\n",
        "\n",
        "    # Create the final pandas DataFrame from the list of successfully parsed records.\n",
        "    forecasts_df = pd.DataFrame(parsed_records)\n",
        "\n",
        "    # Compile statistics for the final report.\n",
        "    parsing_stats = {\n",
        "        \"total_api_calls\": len(raw_results),\n",
        "        \"successful_parses\": len(raw_results) - failure_count,\n",
        "        \"failed_parses\": failure_count,\n",
        "    }\n",
        "\n",
        "    # Return the clean DataFrame and the parsing statistics.\n",
        "    return forecasts_df, parsing_stats\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Orchestrator for Task 11\n",
        "# ------------------------------------------------------------------------------\n",
        "def generate_persona_forecasts(\n",
        "    prompt_generator: Generator[Dict[str, Any], None, None],\n",
        "    config: Dict[str, Any],\n",
        "    checkpoint_path: str\n",
        ") -> Tuple[pd.DataFrame, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Orchestrates the generation and parsing of forecasts for the persona arm.\n",
        "\n",
        "    This function manages the end-to-end workflow for the persona arm:\n",
        "    1.  It calls a robust, asynchronous helper (`_run_forecast_generation`) to\n",
        "        execute all required API calls, handling concurrency, rate limiting,\n",
        "        and resumability.\n",
        "    2.  It then passes the raw results to a generic parsing and validation\n",
        "        function (`_parse_and_persist_forecasts`) to produce a clean,\n",
        "        analysis-ready DataFrame.\n",
        "\n",
        "    Args:\n",
        "        prompt_generator: The generator yielding all prompt objects for all arms.\n",
        "        config: The study configuration dictionary.\n",
        "        checkpoint_path: Path to the JSONL file for saving/resuming raw results\n",
        "                         for the persona arm.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "        - persona_forecasts_df (pd.DataFrame): The final DataFrame of persona forecasts.\n",
        "        - report (Dict[str, Any]): A dictionary summarizing the process.\n",
        "    \"\"\"\n",
        "    # Log the start of the persona arm forecast generation process.\n",
        "    logger.info(\"Starting Task 11: Generate forecasts for the persona arm.\")\n",
        "\n",
        "    # --- Step 1: Asynchronous API Call Execution ---\n",
        "    # Run the main asynchronous generation function for the 'persona' arm.\n",
        "    # This function handles all the complexity of concurrency, retries, and checkpointing.\n",
        "    raw_results = asyncio.run(_run_forecast_generation(\n",
        "        prompt_generator=prompt_generator,\n",
        "        arm_name='persona',\n",
        "        config=config,\n",
        "        checkpoint_path=checkpoint_path\n",
        "    ))\n",
        "\n",
        "    # --- Step 2: Parse, Validate, and Persist Results ---\n",
        "    # Call the generic parsing helper to process the raw results into a clean DataFrame.\n",
        "    # Specify 'persona_id' as the name for the identifier column.\n",
        "    persona_forecasts_df, parsing_stats = _parse_and_persist_forecasts(\n",
        "        raw_results=raw_results,\n",
        "        id_column_name='persona_id'\n",
        "    )\n",
        "\n",
        "    # --- Report Generation ---\n",
        "    # Compile the final report with statistics from both stages.\n",
        "    report = {\n",
        "        \"arm\": \"persona\",\n",
        "        \"total_api_calls_processed\": parsing_stats[\"total_api_calls\"],\n",
        "        \"successful_forecast_sets\": parsing_stats[\"successful_parses\"],\n",
        "        \"failed_or_unparsable_sets\": parsing_stats[\"failed_parses\"],\n",
        "        \"total_forecast_points_generated\": len(persona_forecasts_df),\n",
        "    }\n",
        "\n",
        "    # Log the final summary of the process.\n",
        "    logger.info(f\"Persona arm forecast generation complete. \"\n",
        "                f\"Successfully parsed {report['successful_forecast_sets']} forecast sets, \"\n",
        "                f\"yielding {report['total_forecast_points_generated']} data points.\")\n",
        "\n",
        "    # Return the final DataFrame and the summary report.\n",
        "    return persona_forecasts_df, report\n"
      ],
      "metadata": {
        "id": "BfhQti_6-m_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 12 — Generate forecasts (no-persona baseline arm)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 12: Generate forecasts (no-persona baseline arm)\n",
        "# =============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 12, Step 3 Helper: Validate baseline variation\n",
        "# ------------------------------------------------------------------------------\n",
        "def _validate_baseline_variation(\n",
        "    baseline_df: pd.DataFrame,\n",
        "    zero_std_threshold: float = 1e-9\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Validates the stochastic variation in the baseline forecasts.\n",
        "\n",
        "    This function checks if the `temperature=1.0` setting produced diverse\n",
        "    outputs across the 100 baseline runs for each forecast group. It calculates\n",
        "    the standard deviation for each group and reports the proportion of groups\n",
        "    that exhibit near-zero variation.\n",
        "\n",
        "    Args:\n",
        "        baseline_df: The clean DataFrame of baseline forecasts.\n",
        "        zero_std_threshold: The numerical tolerance for defining \"zero\"\n",
        "                            standard deviation.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing the validation statistics and an overall assessment.\n",
        "    \"\"\"\n",
        "    # Log the start of the validation check.\n",
        "    logger.info(\"Validating stochastic variation in baseline forecasts...\")\n",
        "\n",
        "    # Return an empty report if the input DataFrame is empty.\n",
        "    if baseline_df.empty:\n",
        "        logger.warning(\"Baseline DataFrame is empty. Skipping variation check.\")\n",
        "        return {\"status\": \"SKIPPED_NO_DATA\"}\n",
        "\n",
        "    # Group the forecasts by round, variable, and horizon to analyze variation across runs.\n",
        "    # The `std()` aggregation computes the standard deviation of the 'value' for each group.\n",
        "    variation_stats = baseline_df.groupby([\"round\", \"variable\", \"horizon\"])[\"value\"].std()\n",
        "\n",
        "    # Count the total number of distinct forecast groups.\n",
        "    total_groups = len(variation_stats)\n",
        "\n",
        "    # If there are no groups, there's nothing to validate.\n",
        "    if total_groups == 0:\n",
        "        return {\"status\": \"SKIPPED_NO_GROUPS\"}\n",
        "\n",
        "    # Count the number of groups where the standard deviation is below the threshold (effectively zero).\n",
        "    zero_variation_groups = (variation_stats < zero_std_threshold).sum()\n",
        "\n",
        "    # Calculate the ratio of groups that showed no meaningful variation.\n",
        "    zero_variation_ratio = zero_variation_groups / total_groups\n",
        "\n",
        "    # Determine the overall status of the check based on a predefined tolerance (e.g., 10%).\n",
        "    # A high ratio might indicate a problem with the LLM's stochastic sampling.\n",
        "    status = \"SUCCESS\" if zero_variation_ratio <= 0.10 else \"WARNING\"\n",
        "\n",
        "    # Log a warning if the ratio of zero-variation groups is high.\n",
        "    if status == \"WARNING\":\n",
        "        logger.warning(\n",
        "            f\"High ratio of zero-variation groups found: {zero_variation_ratio:.2%}. \"\n",
        "            \"This may indicate an issue with model stochasticity.\"\n",
        "        )\n",
        "    else:\n",
        "        logger.info(f\"Stochastic variation check passed. Zero-variation ratio: {zero_variation_ratio:.2%}.\")\n",
        "\n",
        "    # Return a structured dictionary with the detailed validation results.\n",
        "    return {\n",
        "        \"status\": status,\n",
        "        \"total_groups_analyzed\": total_groups,\n",
        "        \"zero_variation_groups\": int(zero_variation_groups),\n",
        "        \"zero_variation_ratio\": zero_variation_ratio,\n",
        "    }\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 12, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "def generate_baseline_forecasts(\n",
        "    prompt_generator: Generator[Dict[str, Any], None, None],\n",
        "    config: Dict[str, Any],\n",
        "    checkpoint_path: str\n",
        ") -> Tuple[pd.DataFrame, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Orchestrates the generation and validation of forecasts for the baseline arm.\n",
        "\n",
        "    This function manages the end-to-end workflow for the control group:\n",
        "    1.  It calls the generic, robust asynchronous helper (`_run_forecast_generation`)\n",
        "        to execute all API calls for the 'baseline' arm.\n",
        "    2.  It passes the raw results to the generic parsing function to produce a\n",
        "        clean, analysis-ready DataFrame.\n",
        "    3.  It performs a baseline-specific validation to ensure that the model's\n",
        "        stochasticity produced sufficient variation across the runs.\n",
        "\n",
        "    Args:\n",
        "        prompt_generator: The generator yielding all prompt objects for all arms.\n",
        "        config: The study configuration dictionary.\n",
        "        checkpoint_path: Path to the JSONL file for saving/resuming raw results\n",
        "                         for the baseline arm.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "        - baseline_forecasts_df (pd.DataFrame): The final DataFrame of baseline forecasts.\n",
        "        - report (Dict[str, Any]): A dictionary summarizing the process.\n",
        "    \"\"\"\n",
        "    # Log the start of the baseline arm forecast generation process.\n",
        "    logger.info(\"Starting Task 12: Generate forecasts for the baseline arm.\")\n",
        "\n",
        "    # --- Step 1: Asynchronous API Call Execution ---\n",
        "    # Run the main asynchronous generation function, specifying the 'baseline' arm.\n",
        "    # This function handles concurrency, retries, and checkpointing.\n",
        "    raw_results = asyncio.run(_run_forecast_generation(\n",
        "        prompt_generator=prompt_generator,\n",
        "        arm_name='baseline',\n",
        "        config=config,\n",
        "        checkpoint_path=checkpoint_path\n",
        "    ))\n",
        "\n",
        "    # --- Step 2: Parse, Validate, and Persist Results ---\n",
        "    # Call the generic parsing helper to process the raw results.\n",
        "    # Specify 'baseline_id' as the name for the identifier column.\n",
        "    baseline_forecasts_df, parsing_stats = _parse_and_persist_forecasts(\n",
        "        raw_results=raw_results,\n",
        "        id_column_name='baseline_id'\n",
        "    )\n",
        "\n",
        "    # --- Step 3: Validate Stochastic Variation ---\n",
        "    # Perform the baseline-specific check for forecast diversity.\n",
        "    variation_report = _validate_baseline_variation(baseline_forecasts_df)\n",
        "\n",
        "    # --- Report Generation ---\n",
        "    # Compile the final, comprehensive report for the baseline arm.\n",
        "    report = {\n",
        "        \"arm\": \"baseline\",\n",
        "        \"total_api_calls_processed\": parsing_stats[\"total_api_calls\"],\n",
        "        \"successful_forecast_sets\": parsing_stats[\"successful_parses\"],\n",
        "        \"failed_or_unparsable_sets\": parsing_stats[\"failed_parses\"],\n",
        "        \"total_forecast_points_generated\": len(baseline_forecasts_df),\n",
        "        \"variation_validation_report\": variation_report,\n",
        "    }\n",
        "\n",
        "    # Log the final summary of the process.\n",
        "    logger.info(f\"Baseline arm forecast generation complete. \"\n",
        "                f\"Successfully parsed {report['successful_forecast_sets']} forecast sets. \"\n",
        "                f\"Variation check status: {variation_report.get('status', 'N/A')}.\")\n",
        "\n",
        "    # Return the final DataFrame and the summary report.\n",
        "    return baseline_forecasts_df, report\n"
      ],
      "metadata": {
        "id": "i1sEqXiS_7Fw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 13 — Parse and QC all LLM outputs\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 13: Parse and QC all LLM outputs\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 13, Step 1 Helper: Consolidate persona and baseline forecasts\n",
        "# ------------------------------------------------------------------------------\n",
        "def _consolidate_forecasts(\n",
        "    persona_forecasts_df: pd.DataFrame,\n",
        "    baseline_forecasts_df: pd.DataFrame\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Consolidates persona and baseline forecast DataFrames into a single,\n",
        "    unified DataFrame with a common schema.\n",
        "\n",
        "    Args:\n",
        "        persona_forecasts_df: DataFrame of forecasts from the persona arm.\n",
        "        baseline_forecasts_df: DataFrame of forecasts from the baseline arm.\n",
        "\n",
        "    Returns:\n",
        "        A single DataFrame containing all forecasts from both arms.\n",
        "    \"\"\"\n",
        "    # Log the start of the consolidation process.\n",
        "    logger.info(\"Consolidating persona and baseline forecast DataFrames...\")\n",
        "\n",
        "    # --- Input Validation ---\n",
        "    # Ensure input DataFrames are not empty.\n",
        "    if persona_forecasts_df.empty or baseline_forecasts_df.empty:\n",
        "        raise ValueError(\"Input forecast DataFrames cannot be empty for consolidation.\")\n",
        "\n",
        "    # --- Schema Unification ---\n",
        "    # Create a copy of the persona DataFrame to avoid modifying the original.\n",
        "    df_persona = persona_forecasts_df.copy()\n",
        "    # Add a 'source_type' column to identify the experimental arm.\n",
        "    df_persona['source_type'] = 'persona'\n",
        "    # Rename the 'persona_id' column to the generic 'source_id'.\n",
        "    df_persona.rename(columns={'persona_id': 'source_id'}, inplace=True)\n",
        "\n",
        "    # Create a copy of the baseline DataFrame.\n",
        "    df_baseline = baseline_forecasts_df.copy()\n",
        "    # Add the 'source_type' column.\n",
        "    df_baseline['source_type'] = 'baseline'\n",
        "    # Rename the 'baseline_id' column to the generic 'source_id'.\n",
        "    df_baseline.rename(columns={'baseline_id': 'source_id'}, inplace=True)\n",
        "\n",
        "    # --- Concatenation ---\n",
        "    # Concatenate the two prepared DataFrames vertically.\n",
        "    unified_df = pd.concat([df_persona, df_baseline], ignore_index=True)\n",
        "\n",
        "    # Define and enforce a canonical column order for consistency.\n",
        "    final_columns = [\n",
        "        'source_type', 'source_id', 'round', 'variable', 'horizon', 'value'\n",
        "    ]\n",
        "    # Reorder the columns.\n",
        "    unified_df = unified_df[final_columns]\n",
        "\n",
        "    # Log the result of the consolidation.\n",
        "    logger.info(f\"Successfully consolidated {len(unified_df)} total forecast points.\")\n",
        "\n",
        "    # Return the unified DataFrame.\n",
        "    return unified_df\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 13, Step 2 Helper: Enforce value constraints and units\n",
        "# ------------------------------------------------------------------------------\n",
        "def _apply_final_qc_and_flagging(\n",
        "    unified_df: pd.DataFrame\n",
        ") -> Tuple[pd.DataFrame, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Applies final validation checks, flags outliers, and generates a QC report.\n",
        "\n",
        "    Args:\n",
        "        unified_df: The consolidated DataFrame of all forecasts.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "        - The final, quality-controlled DataFrame.\n",
        "        - A dictionary summarizing the QC checks.\n",
        "    \"\"\"\n",
        "    # Log the start of the final QC process.\n",
        "    logger.info(\"Applying final quality control checks and flagging outliers...\")\n",
        "    # Create a copy to work on.\n",
        "    df = unified_df.copy()\n",
        "\n",
        "    # --- Final Numeric Validation ---\n",
        "    # Check for any null values in the 'value' column, which indicate parsing failures.\n",
        "    initial_rows = len(df)\n",
        "    null_values_mask = df['value'].isna()\n",
        "    num_nulls = null_values_mask.sum()\n",
        "\n",
        "    # Define the final QC pass status. A row passes if its 'value' is not null.\n",
        "    df['qc_pass'] = ~null_values_mask\n",
        "\n",
        "    # --- Sanity Bounds Check (Flagging Outliers) ---\n",
        "    # Define the plausible ranges for each macroeconomic variable.\n",
        "    sanity_bounds = {\n",
        "        'HICP': (-5.0, 25.0),\n",
        "        'HICPX': (-5.0, 25.0),\n",
        "        'RGDP': (-15.0, 15.0),\n",
        "        'UNR': (0.0, 30.0),\n",
        "    }\n",
        "\n",
        "    # Initialize the 'is_outlier' column to False.\n",
        "    df['is_outlier'] = False\n",
        "\n",
        "    # Iterate through each variable to apply its specific bounds.\n",
        "    for var, (min_val, max_val) in sanity_bounds.items():\n",
        "        # Create a mask for rows corresponding to the current variable.\n",
        "        var_mask = df['variable'] == var\n",
        "        # Create a mask for values that are outside the defined bounds.\n",
        "        outlier_mask = ~df['value'].between(min_val, max_val)\n",
        "        # Apply the outlier flag only to the relevant rows.\n",
        "        df.loc[var_mask & outlier_mask, 'is_outlier'] = True\n",
        "\n",
        "    # --- Report Generation ---\n",
        "    # Compile a summary report of the QC process.\n",
        "    qc_report = {\n",
        "        'total_rows_processed': initial_rows,\n",
        "        'rows_passing_qc': int(df['qc_pass'].sum()),\n",
        "        'rows_failing_qc (dropped)': int(num_nulls),\n",
        "        'outliers_flagged': {\n",
        "            var: int(df.loc[df['variable'] == var, 'is_outlier'].sum())\n",
        "            for var in sanity_bounds.keys()\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # --- Final Filtering ---\n",
        "    # Drop any rows that failed the basic QC check (i.e., had a null value).\n",
        "    final_df = df[df['qc_pass']].copy()\n",
        "\n",
        "    # Log a summary of the QC results.\n",
        "    logger.info(f\"QC complete. Passed: {qc_report['rows_passing_qc']}, Failed/Dropped: {qc_report['rows_failing_qc (dropped)']}.\")\n",
        "\n",
        "    # Return the clean DataFrame and the QC report.\n",
        "    return final_df, qc_report\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 13, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "def parse_and_qc_all_forecasts(\n",
        "    persona_forecasts_df: pd.DataFrame,\n",
        "    baseline_forecasts_df: pd.DataFrame\n",
        ") -> Tuple[pd.DataFrame, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Consolidates and performs final quality control on all generated forecasts.\n",
        "\n",
        "    This function serves as the final gateway before the analysis phase. It\n",
        "    takes the separate forecast DataFrames from the persona and baseline arms,\n",
        "    and performs the following steps:\n",
        "    1.  Consolidates them into a single, unified DataFrame with a common schema.\n",
        "    2.  Applies a final, rigorous set of quality control checks, including\n",
        "        validating numeric types and flagging outliers based on plausible\n",
        "        macroeconomic ranges.\n",
        "    3.  Drops any fundamentally invalid records (e.g., with null values).\n",
        "    4.  Returns the final, clean DataFrame ready for analysis, along with a\n",
        "        comprehensive QC report.\n",
        "\n",
        "    Args:\n",
        "        persona_forecasts_df: The DataFrame of forecasts from the persona arm.\n",
        "        baseline_forecasts_df: The DataFrame of forecasts from the baseline arm.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "        - unified_forecasts_df (pd.DataFrame): The final, clean, consolidated DataFrame.\n",
        "        - report (Dict[str, Any]): A dictionary summarizing the consolidation and QC process.\n",
        "    \"\"\"\n",
        "    # Log the start of the task.\n",
        "    logger.info(\"Starting Task 13: Parse and QC all LLM outputs.\")\n",
        "\n",
        "    # --- Step 1: Consolidate Forecasts ---\n",
        "    # Call the helper to merge the two DataFrames into one.\n",
        "    unified_df = _consolidate_forecasts(persona_forecasts_df, baseline_forecasts_df)\n",
        "\n",
        "    # --- Step 2 & 3: Apply Final QC, Flagging, and Filtering ---\n",
        "    # Call the helper to perform the final validation and outlier flagging.\n",
        "    final_df, qc_report = _apply_final_qc_and_flagging(unified_df)\n",
        "\n",
        "    # --- Final Report ---\n",
        "    # The main report is the qc_report generated by the helper.\n",
        "    logger.info(\"Consolidation and QC of all forecasts is complete.\")\n",
        "\n",
        "    # Return the final clean DataFrame and the summary report.\n",
        "    return final_df, qc_report\n"
      ],
      "metadata": {
        "id": "aQeFC4-WAnGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 14 — Compute AI panel medians (cross-sectional aggregation)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 14: Compute AI panel medians (cross-sectional aggregation)\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 14, Step 1 & 2 Helper: Compute medians for a specific arm\n",
        "# ------------------------------------------------------------------------------\n",
        "def _compute_medians_for_arm(\n",
        "    forecasts_df: pd.DataFrame,\n",
        "    arm_name: str\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Computes the cross-sectional median forecast for a specific experimental arm.\n",
        "\n",
        "    This function filters the unified forecast DataFrame for a given arm\n",
        "    ('persona' or 'baseline'), then groups by each forecast combination\n",
        "    (round, variable, horizon) and computes the median. It also counts the\n",
        "    number of individual forecasts contributing to each median for quality control.\n",
        "\n",
        "    Args:\n",
        "        forecasts_df: The unified DataFrame containing all individual forecasts.\n",
        "        arm_name: The name of the arm to process ('persona' or 'baseline').\n",
        "\n",
        "    Returns:\n",
        "        A DataFrame containing the median forecasts and sample sizes for the arm,\n",
        "        indexed by ['round', 'variable', 'horizon'].\n",
        "    \"\"\"\n",
        "    # Log the start of the aggregation for the specified arm.\n",
        "    logger.info(f\"Computing panel medians for '{arm_name}' arm...\")\n",
        "\n",
        "    # --- Input Validation ---\n",
        "    # Ensure the arm_name is valid.\n",
        "    if arm_name not in ['persona', 'baseline']:\n",
        "        raise ValueError(\"arm_name must be either 'persona' or 'baseline'.\")\n",
        "\n",
        "    # --- Filtering and Aggregation ---\n",
        "    # Filter the DataFrame to include only records from the specified arm.\n",
        "    arm_df = forecasts_df[forecasts_df['source_type'] == arm_name]\n",
        "\n",
        "    # Define the columns to group by for aggregation.\n",
        "    grouping_cols = ['round', 'variable', 'horizon']\n",
        "\n",
        "    # Perform the groupby and aggregation in one step for efficiency.\n",
        "    # The .agg() method allows computing multiple statistics simultaneously.\n",
        "    medians_df = arm_df.groupby(grouping_cols).agg(\n",
        "        # Compute the median of the 'value' column for each group.\n",
        "        # Equation: median(ŷ_i) for i in group\n",
        "        median_forecast=('value', 'median'),\n",
        "        # Count the number of forecasts in each group.\n",
        "        sample_size=('value', 'size')\n",
        "    ).reset_index() # Convert the grouped output back to a DataFrame.\n",
        "\n",
        "    # Rename the 'median_forecast' column to be specific to the arm.\n",
        "    medians_df.rename(columns={'median_forecast': f'ai_median_{arm_name}'}, inplace=True)\n",
        "\n",
        "    # Log a summary of the aggregation.\n",
        "    logger.info(f\"Computed {len(medians_df)} median forecasts for '{arm_name}' arm.\")\n",
        "\n",
        "    # Return the resulting DataFrame.\n",
        "    return medians_df\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 14, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "def compute_ai_panel_medians(\n",
        "    unified_forecasts_df: pd.DataFrame\n",
        ") -> Tuple[pd.DataFrame, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Orchestrates the computation of AI panel medians for both experimental arms.\n",
        "\n",
        "    This function takes the unified DataFrame of all individual forecasts and\n",
        "    calculates the consensus forecast (median) for each experimental arm\n",
        "    (persona and baseline) for every forecast instance (round, variable, horizon).\n",
        "\n",
        "    Args:\n",
        "        unified_forecasts_df: The clean, consolidated DataFrame of all forecasts\n",
        "                              from Task 13.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "        - ai_panel_medians_df (pd.DataFrame): A DataFrame with median forecasts\n",
        "          for both arms, ready for comparison.\n",
        "        - report (Dict[str, Any]): A dictionary summarizing the aggregation process.\n",
        "    \"\"\"\n",
        "    # Log the start of the task.\n",
        "    logger.info(\"Starting Task 14: Compute AI panel medians.\")\n",
        "\n",
        "    # --- Input Validation ---\n",
        "    # Ensure the input DataFrame is not empty and has the required columns.\n",
        "    required_cols = {'source_type', 'round', 'variable', 'horizon', 'value'}\n",
        "    if not required_cols.issubset(unified_forecasts_df.columns):\n",
        "        raise ValueError(f\"Input DataFrame is missing required columns. Expected: {required_cols}\")\n",
        "\n",
        "    # --- Step 1: Compute Persona Medians ---\n",
        "    # Call the reusable helper to compute medians for the 'persona' arm.\n",
        "    persona_medians_df = _compute_medians_for_arm(unified_forecasts_df, 'persona')\n",
        "\n",
        "    # --- Step 2: Compute Baseline Medians ---\n",
        "    # Call the reusable helper again to compute medians for the 'baseline' arm.\n",
        "    baseline_medians_df = _compute_medians_for_arm(unified_forecasts_df, 'baseline')\n",
        "\n",
        "    # --- Step 3: Combine Median DataFrames ---\n",
        "    # Log the merging step.\n",
        "    logger.info(\"Merging persona and baseline median forecasts...\")\n",
        "    # Perform an outer merge on the grouping keys. This is a robust way to combine\n",
        "    # the two datasets and will reveal any inconsistencies in coverage (i.e., if a\n",
        "    # group exists in one arm but not the other).\n",
        "    ai_panel_medians_df = pd.merge(\n",
        "        persona_medians_df,\n",
        "        baseline_medians_df,\n",
        "        on=['round', 'variable', 'horizon'],\n",
        "        how='outer',\n",
        "        suffixes=('_persona', '_baseline'),\n",
        "        # 'validate' ensures the merge keys are unique, acting as an integrity check.\n",
        "        validate='one_to_one'\n",
        "    )\n",
        "\n",
        "    # --- Report Generation ---\n",
        "    # Check for any nulls created by the outer merge, which would indicate a problem.\n",
        "    merge_issues = ai_panel_medians_df.isnull().sum().sum()\n",
        "    if merge_issues > 0:\n",
        "        logger.warning(f\"Found {merge_issues} null values after merging. \"\n",
        "                       \"This may indicate inconsistent coverage between arms.\")\n",
        "\n",
        "    # Compile the final report.\n",
        "    report = {\n",
        "        \"total_median_forecasts\": len(ai_panel_medians_df),\n",
        "        \"persona_arm_stats\": {\n",
        "            \"min_sample_size\": int(ai_panel_medians_df['sample_size_persona'].min()),\n",
        "            \"max_sample_size\": int(ai_panel_medians_df['sample_size_persona'].max()),\n",
        "            \"mean_sample_size\": ai_panel_medians_df['sample_size_persona'].mean(),\n",
        "        },\n",
        "        \"baseline_arm_stats\": {\n",
        "            \"min_sample_size\": int(ai_panel_medians_df['sample_size_baseline'].min()),\n",
        "            \"max_sample_size\": int(ai_panel_medians_df['sample_size_baseline'].max()),\n",
        "            \"mean_sample_size\": ai_panel_medians_df['sample_size_baseline'].mean(),\n",
        "        },\n",
        "        \"merge_consistency_check\": \"SUCCESS\" if merge_issues == 0 else \"WARNING\",\n",
        "    }\n",
        "\n",
        "    # Log the completion of the task.\n",
        "    logger.info(\"Successfully computed and merged AI panel medians.\")\n",
        "\n",
        "    # Return the final DataFrame and the summary report.\n",
        "    return ai_panel_medians_df, report\n"
      ],
      "metadata": {
        "id": "HZHmLRUrR_tb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 15 — Align AI medians to human medians and realized outcomes\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 15: Align AI medians to human medians and realized outcomes\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 15, Step 1 Helper: Map horizons to target years\n",
        "# ------------------------------------------------------------------------------\n",
        "def _add_target_year(\n",
        "    forecasts_df: pd.DataFrame,\n",
        "    contextual_data_df: pd.DataFrame\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Computes and adds a canonical 'target_year' column to a forecast DataFrame.\n",
        "\n",
        "    This function translates the forecast horizon (e.g., 'CY', 'CY+1', 'LT')\n",
        "    into a specific calendar year, which is the essential key for joining\n",
        "    forecasts with realized outcomes.\n",
        "\n",
        "    Args:\n",
        "        forecasts_df: A DataFrame containing forecasts with 'round' and 'horizon' columns.\n",
        "        contextual_data_df: The DataFrame with contextual data, needed for the\n",
        "                            'LT' (long-term) horizon mapping.\n",
        "\n",
        "    Returns:\n",
        "        The input DataFrame with a new 'target_year' column added.\n",
        "    \"\"\"\n",
        "    # Log the start of the target year computation.\n",
        "    logger.info(\"Computing 'target_year' from 'round' and 'horizon'...\")\n",
        "    # Create a copy to avoid modifying the original DataFrame.\n",
        "    df = forecasts_df.copy()\n",
        "\n",
        "    # --- Input Validation ---\n",
        "    # Ensure required columns are present.\n",
        "    if not {'round', 'horizon'}.issubset(df.columns):\n",
        "        raise ValueError(\"Input DataFrame must contain 'round' and 'horizon' columns.\")\n",
        "    if not {'survey_round', 'lt_year'}.issubset(contextual_data_df.columns):\n",
        "        raise ValueError(\"Contextual DataFrame must contain 'survey_round' and 'lt_year' columns.\")\n",
        "\n",
        "    # --- Vectorized Computation for Standard Horizons ---\n",
        "    # Extract the base year (as an integer) from the 'round' string (e.g., '2013Q1' -> 2013).\n",
        "    df['base_year'] = df['round'].str[:4].astype(int)\n",
        "    # Define the integer offset for each standard horizon.\n",
        "    horizon_offset_map = {'CY': 0, 'CY+1': 1, 'CY+2': 2}\n",
        "    # Apply the offset to the base year to get the target year. 'LT' will result in NaN for now.\n",
        "    df['target_year'] = df['base_year'] + df['horizon'].map(horizon_offset_map)\n",
        "\n",
        "    # --- Mapping for 'LT' Horizon ---\n",
        "    # Create an efficient mapping dictionary from survey round to its long-term target year.\n",
        "    lt_year_map = contextual_data_df.set_index('survey_round')['lt_year']\n",
        "    # Create a boolean mask to identify all rows with the 'LT' horizon.\n",
        "    lt_mask = df['horizon'] == 'LT'\n",
        "    # Use the map to fill in the 'target_year' for only the 'LT' rows.\n",
        "    df.loc[lt_mask, 'target_year'] = df.loc[lt_mask, 'round'].map(lt_year_map)\n",
        "\n",
        "    # --- Finalization ---\n",
        "    # Drop the temporary helper column.\n",
        "    df = df.drop(columns=['base_year'])\n",
        "    # Convert the final 'target_year' to a nullable integer type for safety.\n",
        "    df['target_year'] = df['target_year'].astype('Int64')\n",
        "\n",
        "    # Validate that no nulls remain in the 'target_year' column.\n",
        "    if df['target_year'].isnull().any():\n",
        "        raise ValueError(\"Failed to compute 'target_year' for all rows. Null values remain.\")\n",
        "\n",
        "    # Log completion.\n",
        "    logger.info(\"'target_year' column successfully added.\")\n",
        "    # Return the modified DataFrame.\n",
        "    return df\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 15, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "def align_forecasts_for_scoring(\n",
        "    ai_panel_medians_df: pd.DataFrame,\n",
        "    human_benchmark_df: pd.DataFrame,\n",
        "    realized_outcomes_df: pd.DataFrame,\n",
        "    contextual_data_df: pd.DataFrame,\n",
        "    config: Dict[str, Any]\n",
        ") -> Tuple[pd.DataFrame, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Creates the master analysis DataFrame by aligning AI forecasts, human\n",
        "    benchmarks, and realized outcomes.\n",
        "\n",
        "    This function executes a critical data preparation pipeline:\n",
        "    1.  Computes the `target_year` for all AI forecasts.\n",
        "    2.  Joins the AI forecasts with the human expert panel medians.\n",
        "    3.  Filters out forecasts for which no human benchmark is available (e.g., HICPX\n",
        "        before 2016Q4).\n",
        "    4.  Joins the aligned forecasts with the ground-truth realized outcomes.\n",
        "    5.  Applies business logic to filter out-of-sample (OOS) forecasts that\n",
        "        are not yet scoreable.\n",
        "\n",
        "    Args:\n",
        "        ai_panel_medians_df: DataFrame of AI median forecasts from Task 14.\n",
        "        human_benchmark_df: The cleansed DataFrame of human median forecasts.\n",
        "        realized_outcomes_df: The cleansed DataFrame of realized macro data.\n",
        "        contextual_data_df: The cleansed DataFrame of contextual data.\n",
        "        config: The study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "        - aligned_df (pd.DataFrame): The final, fully aligned analysis DataFrame.\n",
        "        - report (Dict[str, Any]): A dictionary summarizing the alignment process.\n",
        "    \"\"\"\n",
        "    # Log the start of the task.\n",
        "    logger.info(\"Starting Task 15: Aligning all data sources for scoring.\")\n",
        "\n",
        "    # --- Step 1: Map Horizons to Target Years ---\n",
        "    # Add the 'target_year' column to the AI medians DataFrame.\n",
        "    df_with_target_year = _add_target_year(ai_panel_medians_df, contextual_data_df)\n",
        "\n",
        "    # --- Step 2: Join to Human Benchmarks and Filter ---\n",
        "    # Log the join operation.\n",
        "    logger.info(\"Joining AI medians with human benchmarks...\")\n",
        "    # Perform a left merge to bring in the human median for each forecast.\n",
        "    aligned_df = pd.merge(\n",
        "        df_with_target_year,\n",
        "        human_benchmark_df.rename(columns={'value': 'human_median'}),\n",
        "        on=['round', 'variable', 'horizon', 'target_year'],\n",
        "        how='left',\n",
        "        validate='one_to_one'\n",
        "    )\n",
        "\n",
        "    # Filter out rows where no human benchmark exists. This correctly handles\n",
        "    # the HICPX availability rule.\n",
        "    rows_before_filter = len(aligned_df)\n",
        "    aligned_df.dropna(subset=['human_median'], inplace=True)\n",
        "    rows_after_filter = len(aligned_df)\n",
        "    logger.info(f\"Dropped {rows_before_filter - rows_after_filter} rows with no human benchmark.\")\n",
        "\n",
        "    # --- Step 3: Join to Realized Outcomes and Apply OOS Filter ---\n",
        "    # Log the join operation.\n",
        "    logger.info(\"Joining forecasts with realized outcomes...\")\n",
        "    # Perform a left merge to bring in the ground-truth realized value for each forecast.\n",
        "    aligned_df = pd.merge(\n",
        "        aligned_df,\n",
        "        realized_outcomes_df.rename(columns={'reference_year': 'target_year', 'value': 'realized_value'}),\n",
        "        on=['target_year', 'variable'],\n",
        "        how='left',\n",
        "        validate='many_to_one' # Multiple forecasts can map to the same realized outcome.\n",
        "    )\n",
        "\n",
        "    # Create the 'period' column to distinguish in-sample from out-of-sample rounds.\n",
        "    oos_rounds = set(config['phase_2_parameters']['oos_and_availability']['oos_rounds'])\n",
        "    aligned_df['period'] = aligned_df['round'].apply(\n",
        "        lambda r: 'out-of-sample' if r in oos_rounds else 'in-sample'\n",
        "    )\n",
        "\n",
        "    # Apply the OOS horizon filter: only keep specified horizons for OOS rounds.\n",
        "    oos_horizons = set(config['phase_2_parameters']['oos_and_availability']['oos_scored_horizons'])\n",
        "    # Create a mask to identify rows that are OOS and have a disallowed horizon.\n",
        "    oos_filter_mask = (\n",
        "        (aligned_df['period'] == 'out-of-sample') &\n",
        "        (~aligned_df['horizon'].isin(oos_horizons))\n",
        "    )\n",
        "    rows_before_oos_filter = len(aligned_df)\n",
        "    # Apply the filter by keeping all rows that are NOT in the filter mask.\n",
        "    aligned_df = aligned_df[~oos_filter_mask].copy()\n",
        "    rows_after_oos_filter = len(aligned_df)\n",
        "    logger.info(f\"Dropped {rows_before_oos_filter - rows_after_oos_filter} OOS rows with unscorable horizons.\")\n",
        "\n",
        "    # --- Report Generation ---\n",
        "    # Compile the final report.\n",
        "    report = {\n",
        "        \"initial_ai_medians\": len(df_with_target_year),\n",
        "        \"rows_after_human_join\": rows_after_filter,\n",
        "        \"rows_after_realized_join\": len(aligned_df),\n",
        "        \"in_sample_rows\": (aligned_df['period'] == 'in-sample').sum(),\n",
        "        \"out_of_sample_rows\": (aligned_df['period'] == 'out-of-sample').sum(),\n",
        "        \"scoreable_rows\": aligned_df['realized_value'].notna().sum(),\n",
        "    }\n",
        "\n",
        "    # Log the completion of the task.\n",
        "    logger.info(\"Alignment complete. Master analysis DataFrame is ready.\")\n",
        "\n",
        "    # Return the final aligned DataFrame and the summary report.\n",
        "    return aligned_df, report\n"
      ],
      "metadata": {
        "id": "R4fMlrMLSs5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 16 — Compute AI persona dispersion (within-round disagreement)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 16: Compute AI persona dispersion (within-round disagreement)\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 16, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "def compute_ai_persona_dispersion(\n",
        "    unified_forecasts_df: pd.DataFrame\n",
        ") -> Tuple[pd.DataFrame, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Computes and summarizes the dispersion of AI persona forecasts.\n",
        "\n",
        "    This function quantifies the level of disagreement within the AI panel.\n",
        "    It follows a three-step process:\n",
        "    1.  Filters for individual forecasts generated by the 'persona' arm.\n",
        "    2.  For each forecast instance (round, variable, horizon), it calculates\n",
        "        two measures of dispersion across all personas: the Interquartile Range (IQR)\n",
        "        and the population Standard Deviation (SD).\n",
        "    3.  It then summarizes these per-round dispersion metrics by computing their\n",
        "        median across all rounds for each (variable, horizon) pair.\n",
        "\n",
        "    This produces a summary table that directly corresponds to the AI panel\n",
        "    portion of Table 3 in the source paper.\n",
        "\n",
        "    Args:\n",
        "        unified_forecasts_df: The clean, consolidated DataFrame of all forecasts\n",
        "                              from Task 13.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "        - ai_dispersion_summary_df (pd.DataFrame): A summary DataFrame with the\n",
        "          median IQR and SD for each (variable, horizon) pair.\n",
        "        - report (Dict[str, Any]): A dictionary summarizing the process.\n",
        "    \"\"\"\n",
        "    # Log the start of the task.\n",
        "    logger.info(\"Starting Task 16: Compute AI persona forecast dispersion.\")\n",
        "\n",
        "    # --- Input Validation ---\n",
        "    # Ensure the input DataFrame is not empty and has the required columns.\n",
        "    required_cols = {'source_type', 'round', 'variable', 'horizon', 'value'}\n",
        "    if not required_cols.issubset(unified_forecasts_df.columns):\n",
        "        raise ValueError(f\"Input DataFrame is missing required columns. Expected: {required_cols}\")\n",
        "\n",
        "    # --- Step 1: Persona Forecast Extraction ---\n",
        "    # Filter the unified DataFrame to select only the forecasts from the 'persona' arm.\n",
        "    persona_forecasts = unified_forecasts_df[unified_forecasts_df['source_type'] == 'persona'].copy()\n",
        "\n",
        "    # Check if there is any data to process.\n",
        "    if persona_forecasts.empty:\n",
        "        logger.warning(\"No 'persona' forecasts found. Cannot compute dispersion.\")\n",
        "        # Return an empty DataFrame and a corresponding report.\n",
        "        return pd.DataFrame(), {\"status\": \"SKIPPED_NO_DATA\"}\n",
        "\n",
        "    # --- Step 2: IQR and Standard Deviation Computation (per round) ---\n",
        "    # Define custom aggregation functions for IQR and population standard deviation.\n",
        "    # Using numpy ensures precise control over the statistical calculations.\n",
        "    def iqr_func(x: pd.Series) -> float:\n",
        "        # Equation: IQR = q_75(x) - q_25(x)\n",
        "        # Calculate the 75th and 25th percentiles.\n",
        "        # `interpolation='linear'` is used for consistency with pandas' default.\n",
        "        return np.percentile(x, 75, method='linear') - np.percentile(x, 25, method='linear')\n",
        "\n",
        "    def pop_std_func(x: pd.Series) -> float:\n",
        "        # Equation: SD = sqrt( (1/K) * Σ(y_i - ȳ)^2 )\n",
        "        # Calculate the population standard deviation.\n",
        "        # `np.std` uses ddof=0 by default, which is the correct population formula.\n",
        "        return np.std(x)\n",
        "\n",
        "    # Group by each forecast instance and apply the aggregation functions.\n",
        "    logger.info(\"Calculating per-round dispersion metrics (IQR and SD)...\")\n",
        "    per_round_dispersion = persona_forecasts.groupby(['round', 'variable', 'horizon']).agg(\n",
        "        # Apply the custom IQR function to the 'value' column of each group.\n",
        "        iqr=('value', iqr_func),\n",
        "        # Apply the custom population standard deviation function.\n",
        "        std_dev=('value', pop_std_func),\n",
        "        # Also count the number of personas in each group for quality control.\n",
        "        sample_size=('value', 'size')\n",
        "    ).reset_index()\n",
        "\n",
        "    # --- Step 3: Summarize Across Rounds (Median) ---\n",
        "    # Group the per-round statistics by variable and horizon.\n",
        "    logger.info(\"Summarizing dispersion metrics across all rounds by taking the median...\")\n",
        "    ai_dispersion_summary_df = per_round_dispersion.groupby(['variable', 'horizon']).agg(\n",
        "        # Compute the median of the IQR values across all rounds for each group.\n",
        "        # Equation: median_r { IQR_rvh }\n",
        "        median_iqr_ai=('iqr', 'median'),\n",
        "        # Compute the median of the standard deviation values.\n",
        "        # Equation: median_r { SD_rvh }\n",
        "        median_sd_ai=('std_dev', 'median')\n",
        "    ).reset_index()\n",
        "\n",
        "    # --- Final Formatting ---\n",
        "    # Round the final values to match the precision in the paper's Table 3.\n",
        "    ai_dispersion_summary_df = ai_dispersion_summary_df.round(3)\n",
        "\n",
        "    # --- Report Generation ---\n",
        "    # Compile a summary report of the process.\n",
        "    report = {\n",
        "        \"total_persona_forecasts\": len(persona_forecasts),\n",
        "        \"num_groups_analyzed\": len(per_round_dispersion),\n",
        "        \"mean_personas_per_group\": per_round_dispersion['sample_size'].mean(),\n",
        "        \"final_summary_rows\": len(ai_dispersion_summary_df),\n",
        "    }\n",
        "\n",
        "    # Log the completion of the task.\n",
        "    logger.info(\"Successfully computed AI persona dispersion summary.\")\n",
        "\n",
        "    # Return the final summary DataFrame and the report.\n",
        "    return ai_dispersion_summary_df, report\n"
      ],
      "metadata": {
        "id": "eCx6x8vHTv2n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 17 — Compute human respondent dispersion (within-round disagreement)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 17: Compute human respondent dispersion (within-round disagreement)\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Prerequisite Helpers (Reused from Task 16 for methodological consistency)\n",
        "# ------------------------------------------------------------------------------\n",
        "def _iqr_func(x: Union[pd.Series, np.ndarray]) -> float:\n",
        "    \"\"\"\n",
        "    Calculates the Interquartile Range (IQR) of a numerical series.\n",
        "\n",
        "    This function computes the difference between the 75th and 25th percentiles\n",
        "    of the input data. It is designed to be a robust aggregation function for use\n",
        "    in pandas `groupby().agg()` operations.\n",
        "\n",
        "    **Equation from LaTeX Context:**\n",
        "    This function implements the standard definition of IQR:\n",
        "    IQR = q_75(x) - q_25(x)\n",
        "\n",
        "    Args:\n",
        "        x: A pandas Series or numpy array of numerical data for which to\n",
        "           calculate the IQR.\n",
        "\n",
        "    Returns:\n",
        "        The calculated Interquartile Range as a float. Returns np.nan if the\n",
        "        input series is empty or contains only NaNs.\n",
        "\n",
        "    Raises:\n",
        "        TypeError: If the input data contains non-numeric types that cannot be\n",
        "                   processed by numpy.percentile.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    # Check if the input series is empty after dropping NaNs.\n",
        "    if x.dropna().empty:\n",
        "        # If so, return NaN as the IQR is undefined.\n",
        "        return np.nan\n",
        "\n",
        "    # --- Percentile Calculation ---\n",
        "    # Calculate the 75th percentile (third quartile) of the data.\n",
        "    # `method='linear'` is specified for linear interpolation between data points,\n",
        "    # ensuring consistency with standard statistical software like pandas.\n",
        "    q75 = np.percentile(x.dropna(), 75, method='linear')\n",
        "\n",
        "    # Calculate the 25th percentile (first quartile) of the data.\n",
        "    q25 = np.percentile(x.dropna(), 25, method='linear')\n",
        "\n",
        "    # --- IQR Computation ---\n",
        "    # Compute the final IQR by subtracting the 25th percentile from the 75th.\n",
        "    iqr = q75 - q25\n",
        "\n",
        "    # Return the result as a float.\n",
        "    return float(iqr)\n",
        "\n",
        "\n",
        "def _pop_std_func(x: Union[pd.Series, np.ndarray]) -> float:\n",
        "    \"\"\"\n",
        "    Calculates the Population Standard Deviation of a numerical series.\n",
        "\n",
        "    This function computes the standard deviation for an entire population,\n",
        "    using N in the denominator. This is distinct from the sample standard\n",
        "    deviation, which uses N-1. This choice is made to ensure direct,\n",
        "    methodologically consistent comparison between the AI panel (a full\n",
        "    population of personas) and the human panel (treated as a population\n",
        "    for comparison purposes).\n",
        "\n",
        "    **Equation from LaTeX Context:**\n",
        "    This function implements the population standard deviation:\n",
        "    SD = sqrt( (1/N) * Σ(y_i - ȳ)^2 )\n",
        "\n",
        "    Args:\n",
        "        x: A pandas Series or numpy array of numerical data.\n",
        "\n",
        "    Returns:\n",
        "        The calculated population standard deviation as a float. Returns np.nan\n",
        "        if the input series has fewer than one element after dropping NaNs.\n",
        "\n",
        "    Raises:\n",
        "        TypeError: If the input data contains non-numeric types that cannot be\n",
        "                   processed by numpy.std.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    # Check if the input series is empty after dropping NaNs.\n",
        "    if x.dropna().empty:\n",
        "        # The standard deviation of an empty set is undefined.\n",
        "        return np.nan\n",
        "\n",
        "    # --- Population Standard Deviation Computation ---\n",
        "    # Calculate the population standard deviation using numpy's `std` function.\n",
        "    # By default, `np.std` sets the delta degrees of freedom (ddof) to 0,\n",
        "    # which correctly calculates the population standard deviation by dividing by N.\n",
        "    std_dev = np.std(x.dropna())\n",
        "\n",
        "    # Return the result as a float.\n",
        "    return float(std_dev)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 17, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "def compute_human_dispersion_and_compare(\n",
        "    human_micro_df: pd.DataFrame,\n",
        "    ai_dispersion_summary_df: pd.DataFrame\n",
        ") -> Tuple[pd.DataFrame, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Computes human panel dispersion and creates a comparison table against AI dispersion.\n",
        "\n",
        "    This function calculates the same dispersion metrics (IQR and population SD)\n",
        "    as in Task 16, but for the human expert panel using the micro-data. It then\n",
        "    merges the summarized results with the AI dispersion summary to produce a\n",
        "    final comparison table that replicates the structure of Table 3 in the paper.\n",
        "\n",
        "    Args:\n",
        "        human_micro_df: The cleansed DataFrame of individual human forecasts.\n",
        "        ai_dispersion_summary_df: The summary DataFrame of AI persona dispersion\n",
        "                                  from Task 16.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "        - dispersion_comparison_df (pd.DataFrame): The final comparison table.\n",
        "        - report (Dict[str, Any]): A dictionary summarizing the process.\n",
        "    \"\"\"\n",
        "    # Log the start of the task.\n",
        "    logger.info(\"Starting Task 17: Compute human respondent forecast dispersion.\")\n",
        "\n",
        "    # --- Input Validation ---\n",
        "    # Ensure input DataFrames are not empty and have required columns.\n",
        "    required_micro_cols = {'round', 'variable', 'horizon', 'forecaster_id', 'value'}\n",
        "    if not required_micro_cols.issubset(human_micro_df.columns):\n",
        "        raise ValueError(f\"Human micro DataFrame is missing required columns. Expected: {required_micro_cols}\")\n",
        "    if ai_dispersion_summary_df.empty:\n",
        "        raise ValueError(\"AI dispersion summary DataFrame cannot be empty.\")\n",
        "\n",
        "    # --- Step 1: Human Micro-Data Extraction and Validation ---\n",
        "    # Create a clean copy, dropping any rows with null forecast values.\n",
        "    clean_micro_df = human_micro_df.dropna(subset=['value']).copy()\n",
        "\n",
        "    # Validate that there are no duplicate forecasts from the same forecaster for the same item.\n",
        "    key_cols = ['round', 'variable', 'horizon', 'forecaster_id']\n",
        "    if clean_micro_df.duplicated(subset=key_cols).any():\n",
        "        raise ValueError(\"Found duplicate entries in human micro data.\")\n",
        "\n",
        "    # Check if there is any data to process.\n",
        "    if clean_micro_df.empty:\n",
        "        logger.warning(\"No valid human micro-data found. Cannot compute dispersion.\")\n",
        "        return pd.DataFrame(), {\"status\": \"SKIPPED_NO_DATA\"}\n",
        "\n",
        "    # --- Step 2: Human Panel IQR and SD Computation (per round) ---\n",
        "    # Group by each forecast instance and apply the identical aggregation functions from Task 16.\n",
        "    logger.info(\"Calculating per-round dispersion metrics (IQR and SD) for human panel...\")\n",
        "    per_round_human_dispersion = clean_micro_df.groupby(['round', 'variable', 'horizon']).agg(\n",
        "        # Apply the custom IQR function.\n",
        "        iqr=('value', _iqr_func),\n",
        "        # Apply the custom population standard deviation function for consistency.\n",
        "        std_dev=('value', _pop_std_func),\n",
        "        # Count the number of human respondents in each group.\n",
        "        sample_size=('value', 'size')\n",
        "    ).reset_index()\n",
        "\n",
        "    # --- Step 3: Human Dispersion Summarization ---\n",
        "    # Group the per-round statistics by variable and horizon and compute the median.\n",
        "    logger.info(\"Summarizing human dispersion metrics across all rounds by taking the median...\")\n",
        "    human_dispersion_summary_df = per_round_human_dispersion.groupby(['variable', 'horizon']).agg(\n",
        "        # Compute the median of the IQR values across all rounds.\n",
        "        median_iqr_human=('iqr', 'median'),\n",
        "        # Compute the median of the standard deviation values.\n",
        "        median_sd_human=('std_dev', 'median')\n",
        "    ).reset_index()\n",
        "\n",
        "    # --- Final Step: Create Comparison Table ---\n",
        "    # Merge the human summary with the AI summary to create the final table.\n",
        "    logger.info(\"Merging AI and human dispersion summaries into a final comparison table.\")\n",
        "    dispersion_comparison_df = pd.merge(\n",
        "        ai_dispersion_summary_df,\n",
        "        human_dispersion_summary_df,\n",
        "        on=['variable', 'horizon'],\n",
        "        how='left' # Left join to preserve all AI results, even if human data is missing.\n",
        "    )\n",
        "\n",
        "    # --- Final Formatting and Report Generation ---\n",
        "    # Round the final values to match the precision in the paper's Table 3.\n",
        "    dispersion_comparison_df = dispersion_comparison_df.round(3)\n",
        "\n",
        "    # Compile a summary report.\n",
        "    report = {\n",
        "        \"total_human_forecasts\": len(clean_micro_df),\n",
        "        \"num_human_groups_analyzed\": len(per_round_human_dispersion),\n",
        "        \"mean_respondents_per_group\": per_round_human_dispersion['sample_size'].mean(),\n",
        "        \"final_comparison_rows\": len(dispersion_comparison_df),\n",
        "    }\n",
        "\n",
        "    # Log the completion of the task.\n",
        "    logger.info(\"Successfully computed human dispersion and created comparison table.\")\n",
        "\n",
        "    # Return the final comparison DataFrame and the report.\n",
        "    return dispersion_comparison_df, report\n"
      ],
      "metadata": {
        "id": "ie13aV92UU1Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 18 — Compute absolute errors (AI and Human vs. realized)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 18: Compute absolute errors (AI and Human vs. realized)\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 18, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "def compute_absolute_errors(\n",
        "    aligned_forecasts_df: pd.DataFrame\n",
        ") -> Tuple[pd.DataFrame, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Computes the absolute forecast errors for both AI and human panels.\n",
        "\n",
        "    This function takes the fully aligned DataFrame of forecasts and realized\n",
        "    outcomes and calculates the absolute error for each forecast instance. It\n",
        "    filters the data to include only \"scoreable\" forecasts (i.e., those for\n",
        "    which a realized outcome is available) and computes the error for both the\n",
        "    AI panel median and the human panel median.\n",
        "\n",
        "    Args:\n",
        "        aligned_forecasts_df: The master analysis DataFrame from Task 15,\n",
        "                              containing AI medians, human medians, and\n",
        "                              realized outcomes.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "        - scored_forecasts_df (pd.DataFrame): A DataFrame containing only the\n",
        "          scoreable forecasts with the new absolute error columns.\n",
        "        - report (Dict[str, Any]): A dictionary summarizing the process.\n",
        "    \"\"\"\n",
        "    # Log the start of the task.\n",
        "    logger.info(\"Starting Task 18: Compute absolute forecast errors.\")\n",
        "\n",
        "    # --- Input Validation ---\n",
        "    # Define the set of columns required for this operation.\n",
        "    required_cols = {\n",
        "        'ai_median_persona', 'human_median', 'realized_value'\n",
        "    }\n",
        "    # Check if all required columns are present in the input DataFrame.\n",
        "    if not required_cols.issubset(aligned_forecasts_df.columns):\n",
        "        # Raise a ValueError if any required columns are missing.\n",
        "        raise ValueError(f\"Input DataFrame is missing required columns. Expected: {required_cols}\")\n",
        "\n",
        "    # --- Filter to Scoreable Forecasts ---\n",
        "    # A forecast is \"scoreable\" only if a corresponding realized_value exists.\n",
        "    # Create a boolean mask to identify rows where 'realized_value' is not null.\n",
        "    scoreable_mask = aligned_forecasts_df['realized_value'].notna()\n",
        "    # Apply the mask to create a new DataFrame containing only the scoreable rows.\n",
        "    scored_df = aligned_forecasts_df[scoreable_mask].copy()\n",
        "\n",
        "    # Check if there is any data left to score.\n",
        "    if scored_df.empty:\n",
        "        # Log a warning if no scoreable rows were found.\n",
        "        logger.warning(\"No scoreable forecasts found (no realized outcomes available). Cannot compute errors.\")\n",
        "        # Return an empty DataFrame and a corresponding report.\n",
        "        return pd.DataFrame(), {\"status\": \"SKIPPED_NO_SCOREABLE_DATA\"}\n",
        "\n",
        "    # --- Step 1: Compute Absolute Error for AI Persona Median ---\n",
        "    # This is a vectorized operation for high performance.\n",
        "    # Equation: e_rvh^AI = |ŷ_rvh^AI - y_rvh|\n",
        "    scored_df['abs_error_ai'] = (scored_df['ai_median_persona'] - scored_df['realized_value']).abs()\n",
        "\n",
        "    # --- Step 2: Compute Absolute Error for Human Median ---\n",
        "    # The same operation is applied to the human median forecasts.\n",
        "    # Equation: e_rvh^H = |ŷ_rvh^SPF - y_rvh|\n",
        "    scored_df['abs_error_human'] = (scored_df['human_median'] - scored_df['realized_value']).abs()\n",
        "\n",
        "    # --- Step 3: Final Validation and DataFrame Persistence ---\n",
        "    # Verify that the new error columns do not contain any null values.\n",
        "    # This confirms the integrity of the calculation.\n",
        "    if scored_df[['abs_error_ai', 'abs_error_human']].isnull().any().any():\n",
        "        # Raise an error if any nulls are found, as this indicates a logical flaw.\n",
        "        raise RuntimeError(\"Null values found in computed error columns. This should not happen.\")\n",
        "\n",
        "    # Log the successful computation of errors.\n",
        "    logger.info(f\"Successfully computed absolute errors for {len(scored_df)} scoreable forecasts.\")\n",
        "\n",
        "    # --- Report Generation ---\n",
        "    # Compile a summary report of the process.\n",
        "    report = {\n",
        "        \"total_aligned_forecasts\": len(aligned_forecasts_df),\n",
        "        \"scoreable_forecasts\": len(scored_df),\n",
        "        \"unscoreable_forecasts (no realized value)\": len(aligned_forecasts_df) - len(scored_df),\n",
        "        \"mean_abs_error_ai\": scored_df['abs_error_ai'].mean(),\n",
        "        \"mean_abs_error_human\": scored_df['abs_error_human'].mean(),\n",
        "    }\n",
        "\n",
        "    # Log the completion of the task.\n",
        "    logger.info(\"Absolute error computation complete.\")\n",
        "\n",
        "    # Return the final DataFrame with error columns and the summary report.\n",
        "    return scored_df, report\n"
      ],
      "metadata": {
        "id": "2nQZakNGVRwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 19 — Compute MAE (by variable and horizon)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 19: Compute MAE (by variable and horizon)\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 19, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "def compute_mae_results(\n",
        "    scored_forecasts_df: pd.DataFrame\n",
        ") -> Tuple[pd.DataFrame, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Computes Mean Absolute Error (MAE) for AI and human panels.\n",
        "\n",
        "    This function calculates the primary forecast accuracy metric, MAE, for both\n",
        "    the AI and human panels. It partitions the data by period (in-sample vs.\n",
        "    out-of-sample), variable, and forecast horizon, then computes the mean of\n",
        "    the absolute errors for each group. The final output is a tidy DataFrame\n",
        "    that corresponds to the data presented in Table 4 of the source paper.\n",
        "\n",
        "    Args:\n",
        "        scored_forecasts_df: The DataFrame from Task 18, containing scoreable\n",
        "                             forecasts and their absolute errors.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "        - mae_results_df (pd.DataFrame): A tidy DataFrame with the MAE results\n",
        "          for each forecast group.\n",
        "        - report (Dict[str, Any]): A dictionary summarizing the process.\n",
        "    \"\"\"\n",
        "    # Log the start of the MAE computation task.\n",
        "    logger.info(\"Starting Task 19: Compute Mean Absolute Error (MAE) results.\")\n",
        "\n",
        "    # --- Input Validation ---\n",
        "    # Define the set of columns required for this operation.\n",
        "    required_cols = {'period', 'variable', 'horizon', 'abs_error_ai', 'abs_error_human'}\n",
        "    # Check if all required columns are present in the input DataFrame.\n",
        "    if not required_cols.issubset(scored_forecasts_df.columns):\n",
        "        # Raise a ValueError if any required columns are missing.\n",
        "        raise ValueError(f\"Input DataFrame is missing required columns. Expected: {required_cols}\")\n",
        "\n",
        "    # Check if there is any data to process.\n",
        "    if scored_forecasts_df.empty:\n",
        "        # Log a warning if the input DataFrame is empty.\n",
        "        logger.warning(\"Scored forecasts DataFrame is empty. Cannot compute MAE.\")\n",
        "        # Return an empty DataFrame and a corresponding report.\n",
        "        return pd.DataFrame(), {\"status\": \"SKIPPED_NO_DATA\"}\n",
        "\n",
        "    # --- Step 1 & 2: Grouping and MAE Computation ---\n",
        "    # Define the columns to group by for the analysis.\n",
        "    grouping_cols = ['period', 'variable', 'horizon']\n",
        "\n",
        "    # Log the start of the aggregation process.\n",
        "    logger.info(f\"Grouping by {grouping_cols} and computing MAE...\")\n",
        "\n",
        "    # Perform the groupby and aggregation in a single, efficient operation.\n",
        "    mae_results_df = scored_forecasts_df.groupby(grouping_cols).agg(\n",
        "        # Compute the MAE for the AI panel by taking the mean of its absolute errors.\n",
        "        # Equation: MAE_vh^AI = (1/n_vh) * Σ e_rvh^AI\n",
        "        mae_ai=('abs_error_ai', 'mean'),\n",
        "\n",
        "        # Compute the MAE for the human panel similarly.\n",
        "        # Equation: MAE_vh^H = (1/n_vh) * Σ e_rvh^H\n",
        "        mae_human=('abs_error_human', 'mean'),\n",
        "\n",
        "        # Count the number of observations (rounds) in each group for reference.\n",
        "        n_vh=('abs_error_ai', 'size')\n",
        "    ).reset_index() # Convert the grouped output back into a DataFrame.\n",
        "\n",
        "    # --- Step 3: Add Comparison Flags and Format ---\n",
        "    # Determine the better-performing panel for each group based on the lower MAE.\n",
        "    # This flag will be used for formatting (e.g., bolding) in the final table.\n",
        "    mae_results_df['ai_is_better'] = mae_results_df['mae_ai'] < mae_results_df['mae_human']\n",
        "\n",
        "    # Round the MAE values to two decimal places to match the paper's presentation.\n",
        "    mae_results_df['mae_ai'] = mae_results_df['mae_ai'].round(2)\n",
        "    mae_results_df['mae_human'] = mae_results_df['mae_human'].round(2)\n",
        "\n",
        "    # --- Report Generation ---\n",
        "    # Compile a summary report of the MAE computation.\n",
        "    report = {\n",
        "        \"total_groups_analyzed\": len(mae_results_df),\n",
        "        \"in_sample_groups\": (mae_results_df['period'] == 'in-sample').sum(),\n",
        "        \"out_of_sample_groups\": (mae_results_df['period'] == 'out-of-sample').sum(),\n",
        "        \"ai_wins\": int(mae_results_df['ai_is_better'].sum()),\n",
        "        \"human_wins\": int((mae_results_df['mae_human'] < mae_results_df['mae_ai']).sum()),\n",
        "        \"ties\": int((mae_results_df['mae_human'] == mae_results_df['mae_ai']).sum()),\n",
        "    }\n",
        "\n",
        "    # Log the completion of the task.\n",
        "    logger.info(\"Successfully computed MAE results.\")\n",
        "\n",
        "    # Return the final tidy DataFrame of MAE results and the summary report.\n",
        "    return mae_results_df, report\n"
      ],
      "metadata": {
        "id": "JIe-ViQRWper"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 20 — Construct win-share statistics\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 20: Construct win-share statistics\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 20, Step 1 Helper: Define win indicator per match\n",
        "# ------------------------------------------------------------------------------\n",
        "def _add_win_indicators(\n",
        "    scored_forecasts_df: pd.DataFrame,\n",
        "    tolerance: float = 1e-9\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Adds win, loss, and tie indicators to the scored forecasts DataFrame.\n",
        "\n",
        "    This function compares the absolute errors of the AI and human panels for\n",
        "    each forecast match and adds boolean columns to indicate the outcome.\n",
        "    It uses a numerical tolerance for equality checks to robustly handle\n",
        "    floating-point precision.\n",
        "\n",
        "    Args:\n",
        "        scored_forecasts_df: The DataFrame containing absolute errors for both panels.\n",
        "        tolerance: The tolerance for considering two errors as a tie.\n",
        "\n",
        "    Returns:\n",
        "        The input DataFrame with three new boolean columns: 'ai_wins',\n",
        "        'human_wins', and 'is_tie'.\n",
        "    \"\"\"\n",
        "    # Create a copy to avoid modifying the original DataFrame.\n",
        "    df = scored_forecasts_df.copy()\n",
        "\n",
        "    # --- Input Validation ---\n",
        "    # Ensure the required error columns are present.\n",
        "    if not {'abs_error_ai', 'abs_error_human'}.issubset(df.columns):\n",
        "        raise ValueError(\"Input DataFrame must contain 'abs_error_ai' and 'abs_error_human' columns.\")\n",
        "\n",
        "    # --- Win/Loss/Tie Calculation ---\n",
        "    # Determine if the AI's error is strictly less than the human's error.\n",
        "    # Equation: win_rvh = 1{e_rvh^AI < e_rvh^H}\n",
        "    df['ai_wins'] = df['abs_error_ai'] < df['abs_error_human']\n",
        "\n",
        "    # Determine if the human's error is strictly less than the AI's error.\n",
        "    df['human_wins'] = df['abs_error_human'] < df['abs_error_ai']\n",
        "\n",
        "    # Determine if the errors are a tie, using a numerical tolerance for robust comparison.\n",
        "    # This is more robust than a direct `==` comparison with floating-point numbers.\n",
        "    df['is_tie'] = np.isclose(df['abs_error_ai'], df['abs_error_human'], atol=tolerance)\n",
        "\n",
        "    # --- Integrity Check ---\n",
        "    # The three outcomes (AI win, human win, tie) must be mutually exclusive and exhaustive.\n",
        "    # We adjust the win/loss flags to ensure strict inequality, respecting the tie definition.\n",
        "    df.loc[df['is_tie'], ['ai_wins', 'human_wins']] = False\n",
        "\n",
        "    # Final validation: for each row, exactly one of the three flags must be True.\n",
        "    outcome_sum = df[['ai_wins', 'human_wins', 'is_tie']].sum(axis=1)\n",
        "    if not (outcome_sum == 1).all():\n",
        "        raise RuntimeError(\"Win/loss/tie indicators are not mutually exclusive and exhaustive.\")\n",
        "\n",
        "    # Return the DataFrame with the new indicator columns.\n",
        "    return df\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 20, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "def construct_win_share_statistics(\n",
        "    scored_forecasts_df: pd.DataFrame\n",
        ") -> Tuple[pd.DataFrame, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Constructs win-share statistics by aggregating individual match outcomes.\n",
        "\n",
        "    This function first determines the winner for each forecast match (AI win,\n",
        "    human win, or tie) and then aggregates these outcomes to compute the\n",
        "    overall win-share for the AI panel in each forecast category (period,\n",
        "    variable, horizon). The win-share is calculated excluding ties, as per\n",
        "    the paper's methodology.\n",
        "\n",
        "    Args:\n",
        "        scored_forecasts_df: The DataFrame from Task 18, containing scoreable\n",
        "                             forecasts and their absolute errors.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "        - win_share_df (pd.DataFrame): A tidy DataFrame with the win-share\n",
        "          statistics for each forecast group, ready for hypothesis testing.\n",
        "        - report (Dict[str, Any]): A dictionary summarizing the process.\n",
        "    \"\"\"\n",
        "    # Log the start of the task.\n",
        "    logger.info(\"Starting Task 20: Construct win-share statistics.\")\n",
        "\n",
        "    # --- Input Validation ---\n",
        "    # Check if the input DataFrame is empty.\n",
        "    if scored_forecasts_df.empty:\n",
        "        logger.warning(\"Scored forecasts DataFrame is empty. Cannot construct win-share stats.\")\n",
        "        return pd.DataFrame(), {\"status\": \"SKIPPED_NO_DATA\"}\n",
        "\n",
        "    # --- Step 1: Define Win Indicator Per Match ---\n",
        "    # Call the helper to add the 'ai_wins', 'human_wins', and 'is_tie' columns.\n",
        "    df_with_indicators = _add_win_indicators(scored_forecasts_df)\n",
        "\n",
        "    # --- Step 2: Aggregate Wins and Compute Win-Share ---\n",
        "    # Define the columns to group by for the analysis.\n",
        "    grouping_cols = ['period', 'variable', 'horizon']\n",
        "\n",
        "    # Log the aggregation step.\n",
        "    logger.info(f\"Grouping by {grouping_cols} and aggregating win/loss/tie counts...\")\n",
        "\n",
        "    # Perform the groupby and aggregation.\n",
        "    win_share_df = df_with_indicators.groupby(grouping_cols).agg(\n",
        "        # Count the number of AI wins in each group by summing the boolean column.\n",
        "        W_vh=('ai_wins', 'sum'),\n",
        "        # Count the number of human wins.\n",
        "        human_wins=('human_wins', 'sum'),\n",
        "        # Count the number of ties.\n",
        "        ties=('is_tie', 'sum'),\n",
        "        # Get the total number of matches in the group, including ties.\n",
        "        total_matches=('round', 'size')\n",
        "    ).reset_index()\n",
        "\n",
        "    # Calculate the denominator for the win-share calculation (total non-tie matches).\n",
        "    # Equation: n_vh = (number of AI wins) + (number of human wins)\n",
        "    win_share_df['n_vh'] = win_share_df['W_vh'] + win_share_df['human_wins']\n",
        "\n",
        "    # Calculate the AI win-share.\n",
        "    # Equation: w_vh = W_vh / n_vh\n",
        "    # Use a where clause to handle the edge case of division by zero.\n",
        "    win_share_df['win_share'] = (\n",
        "        win_share_df['W_vh'] / win_share_df['n_vh']\n",
        "    ).where(win_share_df['n_vh'] > 0, np.nan)\n",
        "\n",
        "    # --- Report Generation ---\n",
        "    # Compile a summary report.\n",
        "    total_ties = int(win_share_df['ties'].sum())\n",
        "    total_matches = int(win_share_df['total_matches'].sum())\n",
        "    report = {\n",
        "        \"total_matches_analyzed\": total_matches,\n",
        "        \"total_ties\": total_ties,\n",
        "        \"tie_ratio\": total_ties / total_matches if total_matches > 0 else 0,\n",
        "        \"total_groups\": len(win_share_df),\n",
        "    }\n",
        "\n",
        "    # Log the completion of the task.\n",
        "    logger.info(f\"Successfully constructed win-share statistics. Found {total_ties} ties ({report['tie_ratio']:.2%}).\")\n",
        "\n",
        "    # Return the final DataFrame of win-share statistics and the summary report.\n",
        "    return win_share_df, report\n"
      ],
      "metadata": {
        "id": "6MI4QPPPXWSs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 21 — In-sample Monte Carlo hypothesis tests\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 21: In-sample Monte Carlo hypothesis tests\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 21, Step 1 & 2 Helper: Run a single Monte Carlo test\n",
        "# ------------------------------------------------------------------------------\n",
        "def _run_single_mc_test(\n",
        "    observed_wins: int,\n",
        "    num_trials: int,\n",
        "    num_simulations: int\n",
        ") -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Runs a single Monte Carlo simulation to calculate p-values for a win-share result.\n",
        "\n",
        "    This function generates a null distribution based on the binomial distribution\n",
        "    (B(n, 0.5)) and then calculates the one-tailed and two-tailed empirical\n",
        "    p-values by comparing the observed number of wins to this simulated distribution.\n",
        "\n",
        "    Args:\n",
        "        observed_wins: The actual number of wins observed (W_vh).\n",
        "        num_trials: The total number of non-tie matches (n_vh).\n",
        "        num_simulations: The number of Monte Carlo simulations to run (N).\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing the one-tailed and two-tailed p-values.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    # Ensure the inputs are valid for a binomial simulation.\n",
        "    if not (isinstance(observed_wins, (int, np.integer)) and isinstance(num_trials, (int, np.integer))):\n",
        "        raise TypeError(\"observed_wins and num_trials must be integers.\")\n",
        "    if observed_wins > num_trials or observed_wins < 0:\n",
        "        raise ValueError(\"observed_wins must be between 0 and num_trials.\")\n",
        "\n",
        "    # --- Step 1: Generate Null Distribution ---\n",
        "    # Generate the null distribution by drawing samples from a binomial distribution.\n",
        "    # H0: W_vh ~ Binom(n_vh, 0.5)\n",
        "    # This simulates the number of wins we would expect to see by chance if both\n",
        "    # panels had an equal probability of winning each match.\n",
        "    null_distribution = np.random.binomial(n=num_trials, p=0.5, size=num_simulations)\n",
        "\n",
        "    # --- Step 2: Compute P-Values ---\n",
        "    # Calculate the one-tailed p-value.\n",
        "    # This is the proportion of simulated outcomes that are as extreme or more\n",
        "    # extreme (>=) than the observed number of wins.\n",
        "    # Equation: p_vh^(1) = (1/N) * Σ 1{W_j^* >= W_vh}\n",
        "    p_one_tailed = np.mean(null_distribution >= observed_wins)\n",
        "\n",
        "    # Calculate the two-tailed p-value.\n",
        "    # This requires calculating the probability in both tails.\n",
        "    # Equation: p_vh^(2) = 2 * min( P(W* >= W_vh), P(W* <= W_vh) )\n",
        "    p_lower_tail = np.mean(null_distribution <= observed_wins)\n",
        "    # The two-tailed p-value is twice the smaller of the two tail probabilities.\n",
        "    p_two_tailed = 2 * min(p_one_tailed, p_lower_tail)\n",
        "\n",
        "    # Return the computed p-values.\n",
        "    return {\n",
        "        \"p_one_tailed\": p_one_tailed,\n",
        "        \"p_two_tailed\": p_two_tailed,\n",
        "    }\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 21, Step 3 Helper: Classify significance\n",
        "# ------------------------------------------------------------------------------\n",
        "def _classify_significance(p_value: float) -> str:\n",
        "    \"\"\"\n",
        "    Applies significance star notation based on a p-value.\n",
        "\n",
        "    Args:\n",
        "        p_value: The one-tailed p-value to classify.\n",
        "\n",
        "    Returns:\n",
        "        A string with the corresponding significance stars ('***', '**', '*', or '').\n",
        "    \"\"\"\n",
        "    # Check for NaN or invalid p-values.\n",
        "    if pd.isna(p_value):\n",
        "        return \"\"\n",
        "    # Apply the classification rules from the paper.\n",
        "    if p_value <= 0.01:\n",
        "        return '***'\n",
        "    elif p_value <= 0.05: # Note: paper says p < 0.05, but standard is <=\n",
        "        return '**'\n",
        "    elif p_value <= 0.10:\n",
        "        return '*'\n",
        "    else:\n",
        "        return ''\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 21, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "def run_in_sample_mc_tests(\n",
        "    win_share_df: pd.DataFrame,\n",
        "    config: Dict[str, Any]\n",
        ") -> Tuple[pd.DataFrame, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Performs Monte Carlo hypothesis tests for in-sample win-share statistics.\n",
        "\n",
        "    This function isolates the in-sample results, then for each forecast category,\n",
        "    it runs a Monte Carlo simulation to generate a null distribution and calculate\n",
        "    empirical p-values. It then adds these p-values and corresponding significance\n",
        "    stars back to the main win-share DataFrame.\n",
        "\n",
        "    Args:\n",
        "        win_share_df: The DataFrame of win-share statistics from Task 20.\n",
        "        config: The study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "        - The updated win_share_df with new columns for p-values and significance.\n",
        "        - A dictionary summarizing the testing process.\n",
        "    \"\"\"\n",
        "    # Log the start of the task.\n",
        "    logger.info(\"Starting Task 21: In-sample Monte Carlo hypothesis tests.\")\n",
        "\n",
        "    # --- Configuration and Initialization ---\n",
        "    # Extract simulation parameters from the config.\n",
        "    mc_config = config['phase_2_parameters']['scoring_and_inference']\n",
        "    num_simulations = mc_config['monte_carlo_simulations']\n",
        "    seed = mc_config['monte_carlo_seed']\n",
        "\n",
        "    # Set the random seed for reproducibility of the entire simulation process.\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    # Filter the DataFrame to include only the in-sample results.\n",
        "    in_sample_df = win_share_df[win_share_df['period'] == 'in-sample'].copy()\n",
        "\n",
        "    # Check if there is any data to process.\n",
        "    if in_sample_df.empty:\n",
        "        logger.warning(\"No in-sample data found. Skipping Monte Carlo tests.\")\n",
        "        return win_share_df, {\"status\": \"SKIPPED_NO_IN_SAMPLE_DATA\"}\n",
        "\n",
        "    # --- Run Simulations for Each Group ---\n",
        "    # Initialize a list to store the results of each test.\n",
        "    results = []\n",
        "    # Log the start of the simulations.\n",
        "    logger.info(f\"Running {num_simulations} simulations for {len(in_sample_df)} in-sample groups...\")\n",
        "\n",
        "    # Iterate through each row (each forecast group) in the in-sample data.\n",
        "    for _, row in in_sample_df.iterrows():\n",
        "        # Run the Monte Carlo test for the current group.\n",
        "        p_values = _run_single_mc_test(\n",
        "            observed_wins=int(row['W_vh']),\n",
        "            num_trials=int(row['n_vh']),\n",
        "            num_simulations=num_simulations\n",
        "        )\n",
        "        # Store the results along with the identifying keys.\n",
        "        results.append({\n",
        "            'period': row['period'],\n",
        "            'variable': row['variable'],\n",
        "            'horizon': row['horizon'],\n",
        "            **p_values\n",
        "        })\n",
        "\n",
        "    # Convert the list of results into a DataFrame.\n",
        "    p_values_df = pd.DataFrame(results)\n",
        "\n",
        "    # --- Step 3: Classify Significance and Merge Results ---\n",
        "    # Apply the significance classification to the one-tailed p-values.\n",
        "    p_values_df['significance'] = p_values_df['p_one_tailed'].apply(_classify_significance)\n",
        "\n",
        "    # Merge the p-values and significance stars back into the original win_share_df.\n",
        "    # A left merge ensures that all original rows are kept.\n",
        "    updated_win_share_df = pd.merge(\n",
        "        win_share_df,\n",
        "        p_values_df,\n",
        "        on=['period', 'variable', 'horizon'],\n",
        "        how='left'\n",
        "    )\n",
        "\n",
        "    # --- Report Generation ---\n",
        "    # Compile a summary report.\n",
        "    report = {\n",
        "        \"groups_tested\": len(p_values_df),\n",
        "        \"num_simulations_per_group\": num_simulations,\n",
        "        \"random_seed_used\": seed,\n",
        "    }\n",
        "\n",
        "    # Log the completion of the task.\n",
        "    logger.info(\"In-sample Monte Carlo hypothesis tests complete.\")\n",
        "\n",
        "    # Return the updated DataFrame and the summary report.\n",
        "    return updated_win_share_df, report\n"
      ],
      "metadata": {
        "id": "s71OlFYMY2Q4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 22 — Out-of-sample exact Binomial hypothesis tests\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 22: Out-of-sample exact Binomial hypothesis tests\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 22, Step 2 Helper: Run a single exact Binomial test\n",
        "# ------------------------------------------------------------------------------\n",
        "def _run_single_exact_binomial_test(\n",
        "    observed_wins: int,\n",
        "    num_trials: int\n",
        ") -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Runs a single exact binomial test to calculate p-values for a win-share result.\n",
        "\n",
        "    This function is used for small sample sizes where the binomial distribution\n",
        "    can be calculated directly, avoiding the need for simulation.\n",
        "\n",
        "    Args:\n",
        "        observed_wins: The actual number of wins observed (W_vh).\n",
        "        num_trials: The total number of non-tie matches (n_vh).\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing the one-tailed and two-tailed p-values.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    # If there are no trials, the test is undefined.\n",
        "    if num_trials == 0:\n",
        "        return {\"p_one_tailed\": np.nan, \"p_two_tailed\": np.nan}\n",
        "    if not (isinstance(observed_wins, (int, np.integer)) and isinstance(num_trials, (int, np.integer))):\n",
        "        raise TypeError(\"observed_wins and num_trials must be integers.\")\n",
        "    if observed_wins > num_trials or observed_wins < 0:\n",
        "        raise ValueError(\"observed_wins must be between 0 and num_trials.\")\n",
        "\n",
        "    # --- P-Value Computation ---\n",
        "    # Define the null hypothesis: W ~ Binom(n, p=0.5).\n",
        "    # Calculate the one-tailed p-value: P(W >= observed_wins).\n",
        "    # This is computed using the survival function (sf), which is 1 - CDF(k-1).\n",
        "    # Equation: p_vh^(1) = Pr{W >= W_vh}\n",
        "    p_one_tailed = binom.sf(k=observed_wins - 1, n=num_trials, p=0.5)\n",
        "\n",
        "    # Calculate the two-tailed p-value.\n",
        "    # Equation: p_vh^(2) = 2 * min( Pr{W >= W_vh}, Pr{W <= W_vh} )\n",
        "    # The lower-tail probability, P(W <= observed_wins), is the CDF.\n",
        "    p_lower_tail = binom.cdf(k=observed_wins, n=num_trials, p=0.5)\n",
        "    # The two-tailed p-value is twice the smaller of the two tail probabilities.\n",
        "    p_two_tailed = 2 * min(p_one_tailed, p_lower_tail)\n",
        "    # For discrete distributions, it's possible for 2*min > 1, so we cap it at 1.0.\n",
        "    p_two_tailed = min(p_two_tailed, 1.0)\n",
        "\n",
        "    # Return the computed p-values.\n",
        "    return {\n",
        "        \"p_one_tailed\": p_one_tailed,\n",
        "        \"p_two_tailed\": p_two_tailed,\n",
        "    }\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 22, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "def run_out_of_sample_exact_tests(\n",
        "    win_share_df: pd.DataFrame\n",
        ") -> Tuple[pd.DataFrame, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Performs exact binomial hypothesis tests for out-of-sample win-share statistics.\n",
        "\n",
        "    This function isolates the out-of-sample (OOS) results, which have small\n",
        "    sample sizes, and applies the statistically appropriate exact binomial test\n",
        "    to calculate p-values. It then merges these results back into the main\n",
        "    win-share DataFrame.\n",
        "\n",
        "    Args:\n",
        "        win_share_df: The DataFrame of win-share statistics, potentially already\n",
        "                      containing p-values for the in-sample period.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "        - The updated win_share_df with OOS p-values and significance populated.\n",
        "        - A dictionary summarizing the testing process.\n",
        "    \"\"\"\n",
        "    # Log the start of the task.\n",
        "    logger.info(\"Starting Task 22: Out-of-sample exact binomial hypothesis tests.\")\n",
        "\n",
        "    # --- Step 1: Isolate Out-of-Sample Data ---\n",
        "    # Filter the DataFrame to include only the out-of-sample results.\n",
        "    oos_df = win_share_df[win_share_df['period'] == 'out-of-sample'].copy()\n",
        "\n",
        "    # Check if there is any data to process.\n",
        "    if oos_df.empty:\n",
        "        logger.warning(\"No out-of-sample data found. Skipping exact binomial tests.\")\n",
        "        return win_share_df, {\"status\": \"SKIPPED_NO_OOS_DATA\"}\n",
        "\n",
        "    # --- Step 2: Run Exact Tests for Each Group ---\n",
        "    # Initialize a list to store the results of each test.\n",
        "    results = []\n",
        "    # Log the start of the tests.\n",
        "    logger.info(f\"Running exact binomial tests for {len(oos_df)} out-of-sample groups...\")\n",
        "\n",
        "    # Iterate through each row (each forecast group) in the OOS data.\n",
        "    for _, row in oos_df.iterrows():\n",
        "        # Run the exact binomial test for the current group.\n",
        "        p_values = _run_single_exact_binomial_test(\n",
        "            observed_wins=int(row['W_vh']),\n",
        "            num_trials=int(row['n_vh'])\n",
        "        )\n",
        "        # Store the results along with the identifying keys.\n",
        "        results.append({\n",
        "            'period': row['period'],\n",
        "            'variable': row['variable'],\n",
        "            'horizon': row['horizon'],\n",
        "            **p_values\n",
        "        })\n",
        "\n",
        "    # Convert the list of results into a DataFrame.\n",
        "    p_values_df = pd.DataFrame(results)\n",
        "\n",
        "    # --- Step 3: Classify Significance and Merge Results ---\n",
        "    # Apply the significance classification to the one-tailed p-values.\n",
        "    p_values_df['significance'] = p_values_df['p_one_tailed'].apply(_classify_significance)\n",
        "\n",
        "    # Merge the OOS p-values back into the main win_share_df.\n",
        "    # We use a left merge and then combine the columns to avoid overwriting\n",
        "    # the in-sample results that may already be present.\n",
        "    updated_win_share_df = pd.merge(\n",
        "        win_share_df,\n",
        "        p_values_df,\n",
        "        on=['period', 'variable', 'horizon'],\n",
        "        how='left',\n",
        "        suffixes=('', '_oos') # Add suffix to new columns to avoid name clashes\n",
        "    )\n",
        "\n",
        "    # Coalesce the results from the in-sample (MC) and out-of-sample (exact) tests\n",
        "    # into the final p-value and significance columns.\n",
        "    for col in ['p_one_tailed', 'p_two_tailed', 'significance']:\n",
        "        # Use the value from the OOS test if the original is null, otherwise keep original.\n",
        "        updated_win_share_df[col] = updated_win_share_df[col].fillna(updated_win_share_df[f'{col}_oos'])\n",
        "        # Drop the temporary OOS column.\n",
        "        updated_win_share_df = updated_win_share_df.drop(columns=[f'{col}_oos'])\n",
        "\n",
        "    # --- Report Generation ---\n",
        "    # Compile a summary report.\n",
        "    report = {\n",
        "        \"groups_tested\": len(p_values_df),\n",
        "        \"test_type\": \"Exact Binomial Test\",\n",
        "    }\n",
        "\n",
        "    # Log the completion of the task.\n",
        "    logger.info(\"Out-of-sample exact binomial hypothesis tests complete.\")\n",
        "\n",
        "    # Return the fully updated DataFrame and the summary report.\n",
        "    return updated_win_share_df, report\n"
      ],
      "metadata": {
        "id": "iGzWmFMIeQpN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 23 — Create orchestrator for end-to-end pipeline execution\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 23: Create orchestrator for end-to-end pipeline execution\n",
        "# ==============================================================================\n",
        "\n",
        "def run_full_replication_pipeline(\n",
        "    data_paths: Dict[str, str],\n",
        "    config: Dict[str, Any],\n",
        "    output_dir: str,\n",
        "    total_persona_rows: int,\n",
        "    run_kappa_validation: bool = True\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrates the full end-to-end replication of the research pipeline.\n",
        "\n",
        "    This master function executes the entire sequence of 22 tasks described in\n",
        "    the research paper, from initial raw data validation to final statistical\n",
        "    hypothesis testing. It is designed for robustness, auditability, and\n",
        "    reproducibility.\n",
        "\n",
        "    **Pipeline Workflow:**\n",
        "    1.  **Setup**: Creates a structured directory for all outputs and configures\n",
        "        file-based logging for a persistent audit trail.\n",
        "    2.  **Data Loading**: Loads all raw analytical datasets into memory.\n",
        "    3.  **Phase I (Validation & Cleansing)**: Executes Tasks 1-4 to validate\n",
        "        and cleanse all raw input data, creating analysis-ready files.\n",
        "    4.  **Phase II (Persona Filtering)**: Executes Tasks 5-9, a sequential\n",
        "        filtering pipeline to derive the final set of 2,368 personas. This\n",
        "        includes an optional but critical kappa validation step.\n",
        "    5.  **Phase III (Forecast Generation)**: Executes Tasks 10-12 to assemble\n",
        "        all prompts and run the high-volume, asynchronous forecast generation\n",
        "        for both the persona and baseline arms.\n",
        "    6.  **Phase IV (Analysis & Scoring)**: Executes Tasks 13-22, a linear\n",
        "        sequence of data analysis steps including aggregation, error calculation,\n",
        "        and hypothesis testing.\n",
        "    7.  **Artifact Persistence**: Saves all key final result tables (e.g., MAE,\n",
        "        win-share) and a comprehensive JSON report of the entire run.\n",
        "\n",
        "    Args:\n",
        "        data_paths: A dictionary mapping logical data names (e.g., 'persona_hub',\n",
        "                    'contextual_data') to their raw file paths.\n",
        "        config: The main study configuration dictionary containing all parameters.\n",
        "        output_dir: The root directory where all outputs (processed data,\n",
        "                    results, checkpoints, logs) will be saved.\n",
        "        total_persona_rows: The total number of rows in the raw persona hub file,\n",
        "                            required for streaming validation.\n",
        "        run_kappa_validation: A flag to enable/disable the optional (but\n",
        "                              recommended) kappa validation step (Task 9).\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing all generated artifacts (key DataFrames) and\n",
        "        a nested dictionary of all task-specific reports.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If initial data validation fails.\n",
        "        Exception: If any subsequent pipeline step fails, the exception is logged\n",
        "                   and re-raised to halt execution.\n",
        "    \"\"\"\n",
        "    # --- Step 1: Environment Setup ---\n",
        "    # Create the main output directory Path object.\n",
        "    root_path = Path(output_dir)\n",
        "    # Define paths for all subdirectories.\n",
        "    processed_path = root_path / \"processed\"\n",
        "    results_path = root_path / \"results\"\n",
        "    checkpoints_path = root_path / \"checkpoints\"\n",
        "    # Create all directories, including parents, if they don't already exist.\n",
        "    for p in [processed_path, results_path, checkpoints_path]:\n",
        "        p.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Configure logging to write to a file for a persistent audit trail.\n",
        "    # This provides a complete record of the pipeline's execution.\n",
        "    log_file_path = root_path / \"pipeline_run.log\"\n",
        "    # Create a file handler.\n",
        "    file_handler = logging.FileHandler(log_file_path, mode='w') # Overwrite log on each run\n",
        "    # Set a clear format for the log messages.\n",
        "    file_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(module)s - %(message)s'))\n",
        "    # Add the handler to the root logger.\n",
        "    logging.getLogger().addHandler(file_handler)\n",
        "\n",
        "    # Initialize a dictionary to store all intermediate and final data artifacts.\n",
        "    artifacts: Dict[str, Any] = {}\n",
        "    # Initialize a dictionary to store the summary report from each task.\n",
        "    all_reports: Dict[str, Any] = {}\n",
        "    # Log the start of the pipeline execution.\n",
        "    logger.info(f\"Pipeline started. All artifacts will be saved in '{root_path}'.\")\n",
        "\n",
        "    try:\n",
        "        # --- Step 2: Data Loading ---\n",
        "        # Log the data loading phase.\n",
        "        logger.info(\"Loading raw analytical data files into memory...\")\n",
        "        # Load the contextual data CSV into a pandas DataFrame.\n",
        "        artifacts['raw_contextual_df'] = pd.read_csv(data_paths['contextual_data'])\n",
        "        # Load the human benchmark data.\n",
        "        artifacts['raw_human_benchmark_df'] = pd.read_csv(data_paths['human_benchmark'])\n",
        "        # Load the human micro-data.\n",
        "        artifacts['raw_human_micro_df'] = pd.read_csv(data_paths['human_micro'])\n",
        "        # Load the realized outcomes data.\n",
        "        artifacts['raw_realized_outcomes_df'] = pd.read_csv(data_paths['realized_outcomes'])\n",
        "        # Conditionally load human annotations if kappa validation is enabled.\n",
        "        if run_kappa_validation:\n",
        "            artifacts['raw_human_annotations_df'] = pd.read_csv(data_paths['human_annotations'])\n",
        "\n",
        "        # --- Phase I: Validation and Cleansing (Tasks 1-4) ---\n",
        "        # Log the beginning of Phase I.\n",
        "        logger.info(\"--- Running Phase I: Data Validation and Cleansing ---\")\n",
        "        # Execute Task 1: Validate the large, raw persona hub file via streaming.\n",
        "        all_reports['task_01_validation'] = validate_persona_hub_df(data_paths['persona_hub'], config, total_persona_rows)\n",
        "\n",
        "        # Execute Task 2: Validate all analytical inputs that were loaded into memory.\n",
        "        all_reports['task_02_validation'] = validate_analytical_inputs(\n",
        "            artifacts['raw_contextual_df'], artifacts['raw_human_benchmark_df'],\n",
        "            artifacts['raw_human_micro_df'], artifacts['raw_realized_outcomes_df'], config\n",
        "        )\n",
        "\n",
        "        # Define the path for the cleansed persona data.\n",
        "        cleansed_persona_path = processed_path / \"persona_hub_clean.parquet\"\n",
        "        # Execute Task 3: Cleanse the raw persona hub file and write to the new path.\n",
        "        all_reports['task_03_cleansing'] = cleanse_persona_hub_df(data_paths['persona_hub'], str(cleansed_persona_path), config)\n",
        "        # Store the path to the cleansed file for the next step.\n",
        "        artifacts['cleansed_persona_path'] = str(cleansed_persona_path)\n",
        "\n",
        "        # Execute Task 4: Cleanse all analytical DataFrames in memory.\n",
        "        cleansed_dfs = cleanse_analytical_inputs(\n",
        "            artifacts['raw_contextual_df'], artifacts['raw_human_benchmark_df'],\n",
        "            artifacts['raw_human_micro_df'], artifacts['raw_realized_outcomes_df'], config\n",
        "        )\n",
        "        # Unpack and store the cleansed DataFrames in the artifacts dictionary.\n",
        "        artifacts['clean_contextual_df'], artifacts['clean_human_benchmark_df'], artifacts['clean_human_micro_df'], artifacts['clean_realized_outcomes_df'] = cleansed_dfs\n",
        "\n",
        "        # --- Phase II: Persona Filtering (Tasks 5-9) ---\n",
        "        # Log the beginning of Phase II.\n",
        "        logger.info(\"--- Running Phase II: Persona Filtering Pipeline ---\")\n",
        "        # Define the path for the output of the first filtering step.\n",
        "        persona_step1_path = processed_path / \"persona_step1.parquet\"\n",
        "        # Execute Task 5: Apply the keyword and domain filter.\n",
        "        all_reports['task_05_filter1'] = apply_keyword_domain_filter(artifacts['cleansed_persona_path'], str(persona_step1_path), config)\n",
        "\n",
        "        # Load the result of the first filter into memory (it's now small enough).\n",
        "        artifacts['persona_step1_df'] = pd.read_parquet(persona_step1_path)\n",
        "        # Execute Task 6: Apply the NER filter to the in-memory DataFrame.\n",
        "        artifacts['persona_step2_df'], all_reports['task_06_filter2'] = apply_ner_person_filter(artifacts['persona_step1_df'], config)\n",
        "        # Execute Task 7: Apply embedding-based deduplication.\n",
        "        artifacts['persona_step3_df'], all_reports['task_07_filter3'] = apply_embedding_deduplication(artifacts['persona_step2_df'], config)\n",
        "\n",
        "        # Define the path for the LLM judge checkpoint file.\n",
        "        judge_checkpoint_path = checkpoints_path / \"judge_results.jsonl\"\n",
        "        # Execute Task 8: Run the asynchronous LLM-as-judge filter.\n",
        "        artifacts['persona_final_df'], all_reports['task_08_filter4'] = asyncio.run(apply_llm_judge_filter(artifacts['persona_step3_df'], config, str(judge_checkpoint_path)))\n",
        "\n",
        "        # Conditionally execute Task 9: Validate the reliability of the LLM judge.\n",
        "        if run_kappa_validation:\n",
        "            # Load the full set of judgments from the checkpoint file.\n",
        "            all_judgments_df = pd.read_json(judge_checkpoint_path, lines=True)\n",
        "            # Run the kappa validation.\n",
        "            kappa_report_df, summary_t9 = validate_llm_judge_reliability(artifacts['persona_step3_df'], all_judgments_df, artifacts['raw_human_annotations_df'], config)\n",
        "            # Store the results.\n",
        "            all_reports['task_09_kappa'] = summary_t9\n",
        "            artifacts['kappa_report_df'] = kappa_report_df\n",
        "\n",
        "        # --- Phase III: Forecast Generation (Tasks 10-12) ---\n",
        "        # Log the beginning of Phase III.\n",
        "        logger.info(\"--- Running Phase III: Forecast Generation ---\")\n",
        "        # Execute Task 10: Assemble all prompts for both arms. Convert generator to list to allow multiple passes.\n",
        "        all_prompts = list(assemble_forecasting_prompts(artifacts['persona_final_df'], artifacts['clean_contextual_df'], config))\n",
        "\n",
        "        # Define the checkpoint path for the persona arm forecasts.\n",
        "        persona_checkpoint_path = checkpoints_path / \"persona_forecasts.jsonl\"\n",
        "        # Execute Task 11: Generate forecasts for the persona arm.\n",
        "        artifacts['persona_forecasts_df'], all_reports['task_11_gen_persona'] = generate_persona_forecasts((p for p in all_prompts), config, str(persona_checkpoint_path))\n",
        "\n",
        "        # Define the checkpoint path for the baseline arm forecasts.\n",
        "        baseline_checkpoint_path = checkpoints_path / \"baseline_forecasts.jsonl\"\n",
        "        # Execute Task 12: Generate forecasts for the baseline arm.\n",
        "        artifacts['baseline_forecasts_df'], all_reports['task_12_gen_baseline'] = asyncio.run(generate_baseline_forecasts((p for p in all_prompts), config, str(baseline_checkpoint_path)))\n",
        "\n",
        "        # --- Phase IV: Analysis and Scoring (Tasks 13-22) ---\n",
        "        # Log the beginning of Phase IV.\n",
        "        logger.info(\"--- Running Phase IV: Analysis and Scoring ---\")\n",
        "        # Execute Task 13: Consolidate and QC all generated forecasts.\n",
        "        artifacts['unified_forecasts_df'], all_reports['task_13_qc'] = parse_and_qc_all_forecasts(artifacts['persona_forecasts_df'], artifacts['baseline_forecasts_df'])\n",
        "        # Execute Task 14: Compute the AI panel medians for both arms.\n",
        "        artifacts['ai_panel_medians_df'], all_reports['task_14_aggregation'] = compute_ai_panel_medians(artifacts['unified_forecasts_df'])\n",
        "        # Execute Task 15: Create the master analysis DataFrame by aligning all data sources.\n",
        "        artifacts['aligned_df'], all_reports['task_15_alignment'] = align_forecasts_for_scoring(artifacts['ai_panel_medians_df'],\n",
        "                                                                                                artifacts['clean_human_benchmark_df'],\n",
        "                                                                                                artifacts['clean_realized_outcomes_df'],\n",
        "                                                                                                artifacts['clean_contextual_df'], config)\n",
        "\n",
        "        # Execute Tasks 16 & 17: Compute and compare forecast dispersion.\n",
        "        ai_dispersion_df, report_t16 = compute_ai_persona_dispersion(artifacts['unified_forecasts_df'])\n",
        "        artifacts['dispersion_comparison_df'], report_t17 = compute_human_dispersion_and_compare(artifacts['clean_human_micro_df'], ai_dispersion_df)\n",
        "        all_reports['task_16_17_dispersion'] = {'ai': report_t16, 'human': report_t17}\n",
        "\n",
        "        # Execute Task 18: Compute absolute errors for all scoreable forecasts.\n",
        "        artifacts['scored_forecasts_df'], all_reports['task_18_errors'] = compute_absolute_errors(artifacts['aligned_df'])\n",
        "        # Execute Task 19: Compute Mean Absolute Error (MAE) results.\n",
        "        artifacts['mae_results_df'], all_reports['task_19_mae'] = compute_mae_results(artifacts['scored_forecasts_df'])\n",
        "\n",
        "        # Execute Task 20: Construct win-share statistics.\n",
        "        win_share_df, report_t20 = construct_win_share_statistics(artifacts['scored_forecasts_df'])\n",
        "        all_reports['task_20_win_share'] = report_t20\n",
        "\n",
        "        # Execute Tasks 21 & 22: Run hypothesis tests for in-sample and out-of-sample periods.\n",
        "        win_share_with_insample_tests, report_t21 = run_in_sample_mc_tests(win_share_df, config)\n",
        "        artifacts['final_win_share_df'], report_t22 = run_out_of_sample_exact_tests(win_share_with_insample_tests)\n",
        "        all_reports['task_21_22_hyp_tests'] = {'insample': report_t21, 'oos': report_t22}\n",
        "\n",
        "        # Log the successful completion of all tasks.\n",
        "        logger.info(\"--- All pipeline tasks completed successfully. ---\")\n",
        "\n",
        "    # Catch any exception from any task in the pipeline.\n",
        "    except Exception as e:\n",
        "        # Log the critical error, including the stack trace.\n",
        "        logger.critical(f\"Pipeline execution failed with a critical error: {e}\", exc_info=True)\n",
        "        # Add failure information to the report.\n",
        "        all_reports['pipeline_status'] = 'FAILURE'\n",
        "        all_reports['failure_reason'] = str(e)\n",
        "        # Save the partial report for debugging purposes.\n",
        "        report_path = results_path / \"FAILED_pipeline_report.json\"\n",
        "        # Use a robust JSON dump that can handle non-serializable types like Path objects.\n",
        "        with open(report_path, 'w') as f:\n",
        "            json.dump(all_reports, f, indent=2, default=str)\n",
        "        # Re-raise the exception to halt execution and signal failure.\n",
        "        raise\n",
        "\n",
        "    # --- Final Artifact Persistence ---\n",
        "    # Log the final step of saving all result tables.\n",
        "    logger.info(\"--- Saving final result artifacts to CSV ---\")\n",
        "    # Save the dispersion comparison table.\n",
        "    artifacts['dispersion_comparison_df'].to_csv(results_path / \"dispersion_comparison.csv\")\n",
        "    # Save the MAE results table.\n",
        "    artifacts['mae_results_df'].to_csv(results_path / \"mae_results.csv\")\n",
        "    # Save the final win-share results table with hypothesis tests.\n",
        "    artifacts['final_win_share_df'].to_csv(results_path / \"win_share_results.csv\", index=False)\n",
        "    # Conditionally save the kappa validation report.\n",
        "    if 'kappa_report_df' in artifacts:\n",
        "        artifacts['kappa_report_df'].to_csv(results_path / \"kappa_validation_report.csv\", index=False)\n",
        "\n",
        "    # --- Final Reporting ---\n",
        "    # Set the final status to SUCCESS.\n",
        "    all_reports['pipeline_status'] = 'SUCCESS'\n",
        "    # Define the path for the final, comprehensive JSON report.\n",
        "    report_path = results_path / \"full_pipeline_report.json\"\n",
        "    # Write the report to a file.\n",
        "    with open(report_path, 'w') as f:\n",
        "        json.dump(all_reports, f, indent=2, default=str)\n",
        "\n",
        "    # Log the location of the final report.\n",
        "    logger.info(f\"Full pipeline report saved to '{report_path}'.\")\n",
        "\n",
        "    # Add the final collection of reports to the artifacts dictionary to be returned.\n",
        "    artifacts['all_reports'] = all_reports\n",
        "\n",
        "    # Return the dictionary containing all key in-memory artifacts and the final report.\n",
        "    return artifacts\n"
      ],
      "metadata": {
        "id": "zaTgHF5Jf1C6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 24 — Ablation: paired t-test (persona vs. no-persona)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 24: Ablation: paired t-test (persona vs. no-persona)\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 24, Step 1 Helper: Construct paired differences\n",
        "# ------------------------------------------------------------------------------\n",
        "def _prepare_paired_ablation_data(\n",
        "    aligned_df: pd.DataFrame\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Prepares the data for a paired t-test by calculating error differences.\n",
        "\n",
        "    This function filters the aligned data to find all forecast matches where\n",
        "    results from both the persona and no-persona arms are available, along with\n",
        "    a realized outcome. It then calculates the absolute error for each arm and\n",
        "    the paired difference between these errors.\n",
        "\n",
        "    Args:\n",
        "        aligned_df: The master analysis DataFrame from Task 15.\n",
        "\n",
        "    Returns:\n",
        "        A DataFrame containing the paired absolute errors and their differences,\n",
        "        ready for statistical testing.\n",
        "    \"\"\"\n",
        "    # Log the start of the data preparation.\n",
        "    logger.info(\"Preparing paired data for ablation t-test...\")\n",
        "\n",
        "    # --- Input Validation ---\n",
        "    # Define the columns required for this paired comparison.\n",
        "    required_cols = {\n",
        "        'ai_median_persona', 'ai_median_no_persona', 'realized_value'\n",
        "    }\n",
        "    # Check if all required columns are present.\n",
        "    if not required_cols.issubset(aligned_df.columns):\n",
        "        raise ValueError(f\"Input DataFrame is missing required columns for paired test: {required_cols}\")\n",
        "\n",
        "    # --- Filter for Complete Paired Cases ---\n",
        "    # A case is complete if we have a forecast from both arms and a realized value.\n",
        "    # Drop all rows where any of these three key values are missing.\n",
        "    paired_data = aligned_df.dropna(subset=list(required_cols)).copy()\n",
        "\n",
        "    # Check if any data remains after filtering.\n",
        "    if paired_data.empty:\n",
        "        logger.warning(\"No complete paired cases found for ablation test. Cannot proceed.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # --- Compute Absolute Errors for Each Arm ---\n",
        "    # Equation: e_j^(AI,P) = |ŷ_j^(AI,P) - y_j|\n",
        "    paired_data['abs_error_persona'] = (\n",
        "        paired_data['ai_median_persona'] - paired_data['realized_value']\n",
        "    ).abs()\n",
        "\n",
        "    # Equation: e_j^(AI,NP) = |ŷ_j^(AI,NP) - y_j|\n",
        "    paired_data['abs_error_no_persona'] = (\n",
        "        paired_data['ai_median_no_persona'] - paired_data['realized_value']\n",
        "    ).abs()\n",
        "\n",
        "    # --- Compute Paired Difference ---\n",
        "    # The difference is defined as the error of the persona arm minus the error of the no-persona arm.\n",
        "    # A negative value means the persona arm had a lower error (was better).\n",
        "    # Equation: d_j = e_j^(AI,P) - e_j^(AI,NP)\n",
        "    paired_data['error_difference'] = (\n",
        "        paired_data['abs_error_persona'] - paired_data['abs_error_no_persona']\n",
        "    )\n",
        "\n",
        "    # Log the number of pairs prepared for the test.\n",
        "    logger.info(f\"Prepared {len(paired_data)} paired observations for the t-test.\")\n",
        "\n",
        "    # Return the prepared DataFrame.\n",
        "    return paired_data\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 24, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "def run_ablation_paired_ttest(\n",
        "    aligned_df: pd.DataFrame\n",
        ") -> Tuple[pd.DataFrame, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Performs a paired t-test to compare the forecast accuracy of the persona\n",
        "    and no-persona arms.\n",
        "\n",
        "    This function executes the first statistical test of the ablation study. It\n",
        "    compares the mean absolute error of the persona-prompted forecasts against\n",
        "    the no-persona baseline forecasts to determine if there is a statistically\n",
        "    significant difference in accuracy.\n",
        "\n",
        "    Args:\n",
        "        aligned_df: The master analysis DataFrame containing forecasts from both\n",
        "                    arms and the realized outcomes.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "        - A single-row DataFrame summarizing the t-test results.\n",
        "        - A dictionary report of the process.\n",
        "    \"\"\"\n",
        "    # Log the start of the task.\n",
        "    logger.info(\"Starting Task 24: Ablation paired t-test.\")\n",
        "\n",
        "    # --- Step 1: Construct Paired Differences ---\n",
        "    # Call the helper to prepare the paired dataset.\n",
        "    paired_data = _prepare_paired_ablation_data(aligned_df)\n",
        "\n",
        "    # If no paired data could be created, exit gracefully.\n",
        "    if paired_data.empty:\n",
        "        return pd.DataFrame(), {\"status\": \"SKIPPED_NO_PAIRED_DATA\"}\n",
        "\n",
        "    # --- Step 2: Compute Paired T-Test Statistics ---\n",
        "    # Log the execution of the statistical test.\n",
        "    logger.info(\"Performing paired t-test on error differences...\")\n",
        "\n",
        "    # Use scipy's ttest_rel for a paired t-test. This is equivalent to a\n",
        "    # one-sample t-test on the `error_difference` column.\n",
        "    # H0: The true mean difference between the paired samples is zero.\n",
        "    t_statistic, p_value = ttest_rel(\n",
        "        a=paired_data['abs_error_persona'],\n",
        "        b=paired_data['abs_error_no_persona']\n",
        "    )\n",
        "\n",
        "    # --- Step 3: Report and Interpret Results ---\n",
        "    # Calculate descriptive statistics for the report.\n",
        "    mean_diff = paired_data['error_difference'].mean()\n",
        "    std_diff = paired_data['error_difference'].std()\n",
        "    n_pairs = len(paired_data)\n",
        "    degrees_freedom = n_pairs - 1\n",
        "\n",
        "    # Compare the results to the values reported in the paper for validation.\n",
        "    expected_t = -1.02\n",
        "    expected_p = 0.31\n",
        "    t_match = np.isclose(t_statistic, expected_t, atol=0.01)\n",
        "    p_match = np.isclose(p_value, expected_p, atol=0.01)\n",
        "    replication_status = \"SUCCESS\" if t_match and p_match else \"FAILURE\"\n",
        "\n",
        "    # Create a structured dictionary of the final results.\n",
        "    results = {\n",
        "        \"test_name\": \"Paired t-test (Persona vs. No-Persona)\",\n",
        "        \"n_pairs\": n_pairs,\n",
        "        \"mean_difference\": mean_diff,\n",
        "        \"std_dev_of_difference\": std_diff,\n",
        "        \"t_statistic\": t_statistic,\n",
        "        \"degrees_of_freedom\": degrees_freedom,\n",
        "        \"p_value_two_sided\": p_value,\n",
        "        \"conclusion\": \"Fail to reject H0\" if p_value >= 0.05 else \"Reject H0\",\n",
        "        \"interpretation\": \"No statistically significant difference in mean absolute error.\",\n",
        "        \"replication_status\": replication_status\n",
        "    }\n",
        "\n",
        "    # Convert the results dictionary to a single-row DataFrame for consistent output format.\n",
        "    results_df = pd.DataFrame([results])\n",
        "\n",
        "    # Log the conclusion.\n",
        "    logger.info(f\"Paired t-test complete. p-value={p_value:.4f}. Conclusion: {results['interpretation']}\")\n",
        "    if replication_status == \"FAILURE\":\n",
        "        logger.warning(\"T-test results do not match the values reported in the paper.\")\n",
        "\n",
        "    # Return the results DataFrame and a simple report.\n",
        "    return results_df, {\"status\": \"SUCCESS\", \"pairs_analyzed\": n_pairs}\n"
      ],
      "metadata": {
        "id": "S2sgMkspiJPV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 25 — Ablation: Kolmogorov–Smirnov test (distributional equivalence)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 25: Ablation: Kolmogorov–Smirnov test (distributional equivalence)\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 25, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "def run_ablation_ks_test(\n",
        "    aligned_df: pd.DataFrame\n",
        ") -> Tuple[pd.DataFrame, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Performs a two-sample Kolmogorov-Smirnov (KS) test to compare the error\n",
        "    distributions of the persona and no-persona arms.\n",
        "\n",
        "    This function executes the second statistical test of the ablation study.\n",
        "    While the t-test compares the means, the KS test compares the entire shape\n",
        "    of the error distributions. A non-significant result provides strong evidence\n",
        "    that the two distributions are statistically indistinguishable.\n",
        "\n",
        "    Args:\n",
        "        aligned_df: The master analysis DataFrame containing forecasts from both\n",
        "                    arms and the realized outcomes.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "        - A single-row DataFrame summarizing the KS-test results.\n",
        "        - A dictionary report of the process.\n",
        "    \"\"\"\n",
        "    # Log the start of the task.\n",
        "    logger.info(\"Starting Task 25: Ablation Kolmogorov-Smirnov (KS) test.\")\n",
        "\n",
        "    # --- Step 1: Construct Error Distribution Samples ---\n",
        "    # Use the same helper from the t-test task to get the paired error data.\n",
        "    # This ensures we are testing on the exact same set of matches.\n",
        "    paired_data = _prepare_paired_ablation_data(aligned_df)\n",
        "\n",
        "    # If no paired data could be created, exit gracefully.\n",
        "    if paired_data.empty:\n",
        "        # Log a warning and return empty results.\n",
        "        logger.warning(\"No complete paired cases found for ablation KS test. Cannot proceed.\")\n",
        "        return pd.DataFrame(), {\"status\": \"SKIPPED_NO_PAIRED_DATA\"}\n",
        "\n",
        "    # Extract the two error distributions as numpy arrays.\n",
        "    error_dist_persona = paired_data['abs_error_persona'].values\n",
        "    error_dist_no_persona = paired_data['abs_error_no_persona'].values\n",
        "\n",
        "    # --- Input Validation ---\n",
        "    # The two-sample KS test requires two independent samples. While these are\n",
        "    # paired, the test is still valid for testing distributional equivalence.\n",
        "    # We must ensure they are of equal size.\n",
        "    if len(error_dist_persona) != len(error_dist_no_persona):\n",
        "        raise ValueError(\"Error distribution samples must have the same size for this test.\")\n",
        "\n",
        "    # --- Step 2: Compute Two-Sample KS Test ---\n",
        "    # Log the execution of the statistical test.\n",
        "    logger.info(\"Performing two-sample KS test on error distributions...\")\n",
        "\n",
        "    # Use scipy's ks_2samp function to perform the test.\n",
        "    # H0: The two samples are drawn from the same distribution.\n",
        "    # The function returns the D statistic and the two-sided p-value.\n",
        "    # Equation: D = sup_x |F_P(x) - F_NP(x)|\n",
        "    d_statistic, p_value = ks_2samp(\n",
        "        data1=error_dist_persona,\n",
        "        data2=error_dist_no_persona\n",
        "    )\n",
        "\n",
        "    # --- Step 3: Report and Interpret Results ---\n",
        "    # Get the sample size for the report.\n",
        "    n_samples = len(error_dist_persona)\n",
        "\n",
        "    # Compare the results to the values reported in the paper for validation.\n",
        "    expected_d = 0.05\n",
        "    expected_p = 0.28\n",
        "    d_match = np.isclose(d_statistic, expected_d, atol=0.01)\n",
        "    p_match = np.isclose(p_value, expected_p, atol=0.01)\n",
        "    replication_status = \"SUCCESS\" if d_match and p_match else \"FAILURE\"\n",
        "\n",
        "    # Create a structured dictionary of the final results.\n",
        "    results = {\n",
        "        \"test_name\": \"Two-Sample Kolmogorov-Smirnov Test (Persona vs. No-Persona)\",\n",
        "        \"n_samples_per_dist\": n_samples,\n",
        "        \"d_statistic\": d_statistic,\n",
        "        \"p_value\": p_value,\n",
        "        \"conclusion\": \"Fail to reject H0\" if p_value >= 0.05 else \"Reject H0\",\n",
        "        \"interpretation\": \"The error distributions are statistically indistinguishable.\",\n",
        "        \"replication_status\": replication_status\n",
        "    }\n",
        "\n",
        "    # Convert the results dictionary to a single-row DataFrame.\n",
        "    results_df = pd.DataFrame([results])\n",
        "\n",
        "    # Log the conclusion.\n",
        "    logger.info(f\"KS test complete. p-value={p_value:.4f}. Conclusion: {results['interpretation']}\")\n",
        "    if replication_status == \"FAILURE\":\n",
        "        logger.warning(\"KS-test results do not match the values reported in the paper.\")\n",
        "\n",
        "    # Return the results DataFrame and a simple report.\n",
        "    return results_df, {\"status\": \"SUCCESS\", \"samples_analyzed\": n_samples}\n"
      ],
      "metadata": {
        "id": "8mQ1tm6JsGYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Top-Level Orchestrator\n",
        "\n",
        "# ==============================================================================\n",
        "# Top-Level Orchestrator for the Entire Study\n",
        "# ==============================================================================\n",
        "\n",
        "def run_synthetic_economist_study(\n",
        "    data_paths: Dict[str, str],\n",
        "    config: Dict[str, Any],\n",
        "    output_dir: str,\n",
        "    total_persona_rows: int,\n",
        "    run_kappa_validation: bool = True\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Executes the complete research study, including the main replication pipeline\n",
        "    and the final ablation study statistical tests.\n",
        "\n",
        "    This top-level function serves as the single entry point for the entire\n",
        "    project. It orchestrates the execution of all 25 tasks in the correct\n",
        "    sequence, ensuring that the outputs of the main pipeline (Tasks 1-22) are\n",
        "    correctly passed as inputs to the final ablation tests (Tasks 24-25).\n",
        "\n",
        "    **Workflow:**\n",
        "    1.  Calls `run_full_replication_pipeline` to execute the main data\n",
        "        processing, forecast generation, and analysis pipeline.\n",
        "    2.  Extracts the key `aligned_df` artifact from the pipeline's output.\n",
        "    3.  Calls `run_ablation_paired_ttest` on the `aligned_df` to compare the\n",
        "        mean absolute errors of the two experimental arms.\n",
        "    4.  Calls `run_ablation_ks_test` on the `aligned_df` to compare the\n",
        "        error distributions of the two arms.\n",
        "    5.  Collects and returns all results in a single, comprehensive dictionary.\n",
        "\n",
        "    Args:\n",
        "        data_paths: A dictionary mapping logical data names to their raw file paths.\n",
        "        config: The main study configuration dictionary.\n",
        "        output_dir: The root directory where all outputs will be saved.\n",
        "        total_persona_rows: The total number of rows in the raw persona hub file.\n",
        "        run_kappa_validation: A flag to enable/disable the kappa validation step.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing all major artifacts and reports from the entire study.\n",
        "    \"\"\"\n",
        "    # Log the start of the top-level orchestration.\n",
        "    logger.info(\">>> Starting the top-level orchestrator for the Synthetic Economist study. <<<\")\n",
        "\n",
        "    # Initialize a dictionary to hold the final, comprehensive results.\n",
        "    final_results: Dict[str, Any] = {}\n",
        "\n",
        "    try:\n",
        "        # --- Step 1: Run the main replication pipeline (Tasks 1-22) ---\n",
        "        # This function is the master orchestrator for the main body of the paper.\n",
        "        # It handles all data processing, filtering, forecast generation, and initial analysis.\n",
        "        # It returns a dictionary of all key artifacts produced during its run.\n",
        "        replication_artifacts = run_full_replication_pipeline(\n",
        "            data_paths=data_paths,\n",
        "            config=config,\n",
        "            output_dir=output_dir,\n",
        "            total_persona_rows=total_persona_rows,\n",
        "            run_kappa_validation=run_kappa_validation\n",
        "        )\n",
        "        # Store the entire collection of artifacts from the main pipeline.\n",
        "        final_results['replication_pipeline_artifacts'] = replication_artifacts\n",
        "\n",
        "        # --- Step 2: Unpack results and prepare for ablation tests ---\n",
        "        # Log the transition to the ablation study phase.\n",
        "        logger.info(\"--- Main replication pipeline complete. Proceeding to final ablation tests. ---\")\n",
        "\n",
        "        # Extract the critical `aligned_df` artifact, which is required for both ablation tests.\n",
        "        # This DataFrame contains the aligned forecasts from both arms and the realized outcomes.\n",
        "        aligned_df = replication_artifacts.get('aligned_df')\n",
        "\n",
        "        # --- Input Validation for Ablation Tests ---\n",
        "        # Perform a rigorous check to ensure the required DataFrame exists and is valid.\n",
        "        if aligned_df is None or not isinstance(aligned_df, pd.DataFrame) or aligned_df.empty:\n",
        "            # If the required data is missing, the ablation tests cannot run. Raise a critical error.\n",
        "            error_msg = \"The 'aligned_df' artifact was not found or is empty. Cannot run ablation tests.\"\n",
        "            logger.critical(error_msg)\n",
        "            raise RuntimeError(error_msg)\n",
        "\n",
        "        # --- Step 3: Run the paired t-test (Task 24) ---\n",
        "        # Execute the paired t-test to compare the mean absolute errors.\n",
        "        ttest_df, ttest_report = run_ablation_paired_ttest(aligned_df=aligned_df)\n",
        "        # Store the results of the t-test.\n",
        "        final_results['ablation_ttest_report'] = ttest_df\n",
        "        # Save the t-test report to a file for easy access.\n",
        "        ttest_df.to_csv(Path(output_dir) / \"results\" / \"ablation_ttest_report.csv\", index=False)\n",
        "\n",
        "        # --- Step 4: Run the Kolmogorov-Smirnov test (Task 25) ---\n",
        "        # Execute the KS test to compare the error distributions.\n",
        "        ks_test_df, ks_test_report = run_ablation_ks_test(aligned_df=aligned_df)\n",
        "        # Store the results of the KS test.\n",
        "        final_results['ablation_ks_test_report'] = ks_test_df\n",
        "        # Save the KS test report to a file.\n",
        "        ks_test_df.to_csv(Path(output_dir) / \"results\" / \"ablation_ks_test_report.csv\", index=False)\n",
        "\n",
        "        # Log the successful completion of the entire study.\n",
        "        logger.info(\">>> Top-level orchestrator completed all tasks successfully. <<<\")\n",
        "\n",
        "    # Catch any exception from any part of the entire process.\n",
        "    except Exception as e:\n",
        "        # Log the critical failure.\n",
        "        logger.critical(f\"The top-level orchestrator failed with a critical error: {e}\", exc_info=True)\n",
        "        # Add failure information to the final results dictionary.\n",
        "        final_results['pipeline_status'] = 'FAILURE'\n",
        "        final_results['failure_reason'] = str(e)\n",
        "        # Re-raise the exception to halt execution and signal failure to the caller.\n",
        "        raise\n",
        "\n",
        "    # If successful, set the final status.\n",
        "    final_results['pipeline_status'] = 'SUCCESS'\n",
        "\n",
        "    # Return the final, comprehensive dictionary of all results.\n",
        "    return final_results\n"
      ],
      "metadata": {
        "id": "Jw5zSd9DuEUI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}